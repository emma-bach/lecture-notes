\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}
%\usepackage{bbm}
%\usepackage{biblatex}
%\addbibresource{bib.bib}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

\newcommand{\tbf}{\textbf}
\newcommand{\argmin}[1] {
    \begin{array}{c}
        \vphantom{#1}\\ %vphantom makes sure text is centered vertically in the end
        \text{argmin}\\
        #1\\
        \end{array}
    }
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}

\renewcommand*\contentsname{Table of contents}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\yhat}{\hat{y}}


\begin{document}
\include{title}
\tableofcontents
\thispagestyle{fancy}
\chapter{Introduction}
Given a set $\{(x_1,y_1), \hdots, (x_n,y_n)\}$, i.e. a set of features (inputs) $x = \{x_1, \hdots, x_n\}$ and a target (desired output) $y = \{y_1, \hdots, y_n\}$, Machine Learning is the process of algorithmically finding the best possible function $f(x)$ such that
\begin{equation*}
 \forall x_i[f(x_i) \approx y_i].
\end{equation*}
$y$ is also known as the \tbf{ground truth}.
In \tbf{unsupervised learning}, $y$ is not explicitly given to the algorithm (and may not even exist). The focus of the course, and thus these notes, was \tbf{supervised learning}, where $y$ is explicitly given.
\newpar
In order to find an optimal $f(x)$, we first rephrase the function in terms of \tbf{learnable parameters $\theta$} to get a function $f(x,\theta)$. We often write $f(x_i, \theta) = \hat{y}_i$, where the hat $\hat{}$ is supposed to show that $\hat{y}_i$ is an approximation of $y_i$. The full image $\hat{y}$ is often called the \tbf{prediction model}.
\newpar
We now need to define a seperate function that we can use to judge how good our values of $\theta$ are. Such a function is known as a \tbf{loss function}, which we write as $\mathcal{L}(y, \hat{y})$. The problem of finding the best possible $f$ then comes down to a minimization problem of the form
\begin{equation*}
\argmin{\theta} \sum_{n=1}^N \mathcal{L}(y_n, \hat{y}_n)
\end{equation*}
This function that we want to minimize is also called the \tbf{empirical risk}.
\newpar
\section{Basic Prediction Models}
Common basic prediction models include:
\begin{itemize}
 \item Linear Models:
 \begin{equation*}
 \hat{y_n} = \theta_0 + \sum_{m=1}^M \theta_m x_{n,m} 
 \end{equation*}
 \item Polynomial Regression Models:
 \begin{align*}
 \hat{y_n} = \theta_0 &+ \sum_{m=1}^M \theta_m x_{n,m}\\
           &+ \sum_{m=1}^M \sum_{m'=1}^M \theta_{m,m'} x_{n,m} x_{n,m'}\\
           &+ \sum_{m=1}^M \sum_{m'=1}^M \sum_{m''=1}^M \theta_{m,m',m''} x_{n,m} x_{n,m'} x_{n,m''}\\
           &+ \hdots\\
 \end{align*}
 \item Decision Trees
 \item Neural Networks
\end{itemize}
%
\subsection{Decision Trees}
In a Decision Tree, the model gives its answer by starting at the root node of a binary tree and then moving from each node to one of its children by answering a yes/no question at each leaf. A decision tree can be thought of as approximating an arbitrary function through a piecewise combination of constant functions.
%
\subsection{Neural Networks}
In a Neural Network, a given neuron labeled $i$ consists of a non-linear function $f_i(x, \theta_i)$, such that if the output of a neuron $i$ is connected to a neuron $j$, then the output $\hat{y_{n,j}}$ of the neuron $j$ is
\begin{align*}
 \hat{y_{n,j}} = f_j(f_i(x, \theta_i), \theta_j)
\end{align*}
%
\section{Common Loss Functions}
A \tbf{Regression Problem} is a problem where the output is an arbitrary real number. Common loss functions for regression include:
\begin{itemize}
 \item Least Squares:
 \begin{align*}
  \loss(y_n, \yhat_n) = (y_n - \yhat_n)^2
 \end{align*}
 Least Square Loss is an approximation of the maximum likelihood estimator of a linear model with normally distributed error (with a mean of 0).
 \item L1 Loss:
 \begin{align*}
  \loss(y_n, \yhat_n) = |y_n - \yhat_n|
 \end{align*}
\end{itemize}
For \tbf{Binary Classification Problems}, where $y_n$ can only take on two possible values, more specialized loss functions are used:
\begin{itemize}
 \item Logistic Loss, $y_n \in \{0,1\}$:
 \begin{align*}
  \loss(y_n, \yhat_n) = -y_n \log(\yhat_n) - (1-y) \log (1-\yhat_n)
 \end{align*}
 \item Hinge Loss, $y_n \in \{-1,1\}$:
 \begin{align*}
  \loss(y_n, \yhat_n) = \max(0,1-y_n\yhat_n)
 \end{align*}
\end{itemize}
A \tbf{Softmax} is a type of function used to express \tbf{Non-Binary Classification Problems}, where targets $y_n \in {1, \hdots, C}$, as a set of binary classification targets:
\begin{align*}
 y_{n,k} = 
\begin{cases}
1 & y_n = k\\
0 & y_n \neq k\\
\end{cases}
\end{align*}
The final result is then obtained by re-expressing the results as probabilities among classes as follows:
\begin{align*}
 \yhat_{n,k} = \frac{e^{f(x_n, \theta_k)}}{\sum_{i=1}^{C}e^{f(x_n, \theta_i)}}
\end{align*}
The exponential function is used to avoid negative probabilities. A common loss function for Softmax problems is \tbf{Logloss}:
\begin{align*}
 \loss(y_n, \yhat_n) = -\sum_{i=i}^{C} y_{n,c} \log(\yhat_{n,c})
\end{align*}
%
\section{Overfitting and Underfitting}
Two common issues every model needs to deal with are \tbf{Underfitting}, where the bias of a model is too high, leading to a model that is unable to capture the data's complexity, and \tbf{Overfitting}, where the variance of a model is too high, meaning that the model has captured noise in the training data and is unable to generalize to pieces of data not included in the training data. Very simple models like linear models tend to be prone to underfitting, while most modern models that are more complex tend to be more prone to overfitting. Decision Trees in particular are very prone to overfitting.
%
\subsection{Regularization}
Regularization is the process of fighting overfitting by reducing the the size of the parameters of a model. This is expressed by adding a \tbf{Regularization Function} as a penalty term to the underlying optimization problem of minimizing the empirical risk:
\begin{equation*}
\argmin{\theta} \sum_{n=1}^N \mathcal{L}(y_n, \hat{y}_n) + \alpha \Omega(\theta)
\end{equation*}
Just as there are many different possible loss functions, there are also many different possible regularization functions. Using a regularization always comes with the tradeoff of increasing model bias (but is done in the vast majority of models, since with complex modern models, overfitting tends to be a more common issue than underfitting).
%
\section{ML Design Cycle}
\begin{enumerate}
 \item Pre-processing
 \item Feature extraction / Feature encoding
 \item Feature selection
 \item Machine learning
 \item Evaluation / Model Selection
 \item Post-processing
\end{enumerate}
%
\section{Non-Parametric Models}
\subsection{Nearest Neighbors}
Predict target $y_i$ by averaging over the targets of the $k$ nearest points $x_1, \hdots, x_k$ to the point $x_i$.
For a continuus target the aggregation is simply the mean:
\begin{align}
 f(x_i) = \frac{1}{k} \sum_{y_j \in Y_{near}}^{k} y_j
\end{align}
For classification, we get our probability of the target being a certain class by taking the number of points belonging to each class and dividing by the total number of points we sampled:
\begin{align}
 f_c(x) = \frac{|y_j y_j \in Y_{near} \wedge y_j = c|}{k}
\end{align}
%
\subsection{Naive Bayes}
Bayes Rule:
\begin{align}
 P(y|x) = P(y) \frac{P(x|y)}{P(x)}
\end{align}
Using the law of total probability this can be extended to:
\begin{align}
 P(y|x) = P(y) \frac{P(x|y)}{\sum_{y' \in Y}P(x|y')P(y')}
\end{align}
Note that all of this only formally works if all features are conditionally independent.
%
\chapter{Linear Methods}
\section{Linear Regression}
The following problem is also known as the regression problem:
\begin{itemize}
 \item Dataset of $N$ instances $(x_i, y_i)$ ($x_i$ is generally a vector) 
 \item Find a model $\yhat$ that minimizes a loss function $\loss$:
 \begin{align*}
  \argmin{w \in W} \sum_{i=1}^{N} \loss(y_i f(x_i; w))
 \end{align*}
 \item Very simple and basic model: \tbf{Linear Regression}
 \begin{align*}
  f(x_i;w) &= w_0 + x_i w^{T}
 \end{align*}
 $w_0$ is a fixed offset and thus often called the \tbf{bias}, while $w$ is the \tbf{weight vector} or \tbf{parameter vector}.
\end{itemize}
Linear regression is more powerful than it looks - for example, $f(x) = (x+1)^2$ can be expressed as a linear function by increasing the dimensionality and writing it as \[f(a,b) = a +2b + 1\] (such that $b = x$ and $a = x^2$). Formally, we can take any set of arbitrary \tbf{basis functions} $\phi_j(x_i)$ and arrive at a linear model
\begin{align*}
 f(x_i;w) = w_0 + \sum_{j=1}^{M} w_j \phi_j(x_i)
\end{align*}
Linear models are therefore linear in the weights $w_i$, but they don't have to be linear in the inputs $x_i$.
\newpar
The \tbf{residual error} of a linear model is given by:
\begin{align*}
 y_i - f(x_i;w)
\end{align*}
For the loss function of a linear model, we just sum the losses of the individual predictions. Typical choices for the loss function are L2 loss $(y-\yhat)^2$ and L1 loss $|y - \yhat|$
\newpar
A closed-form solution for linear regression using L2 loss exists under the following assumptions:
\begin{itemize}
 \item The expected value of the residuals is 0: $\forall i: E(\epsilon_i) = 0$
 \item Residuals are uncorrelated and share the same variance: $\forall i: Var(\epsilon_i) = \sigma_2$
 \item Residuals follow a normal distribution: $\forall i: \epsilon_i \sim \mathcal{N}(0,\sigma^2)$
\end{itemize}
If these hold, then the closed-form solution is:
\begin{align}
 w = (X^T X)^{-1} X^T y
\end{align}
Where $X$ is the input matrix, i.e.
\begin{align*}
X = 
\begin{pmatrix}
x_1\\
\vdots\\
x_n\\
\end{pmatrix}
\end{align*}
%
\section{Linear Classification}
\tbf{Logistic regression} is a way of using linear regression to solve classification problems, by bounding the output of a linear model to $0 \leq h_w(x) \leq 1$ by taking the sigmoid of our output. The output is then interpreted as the probability that $y = 1$ (formally, $h_w(x) = P(y = 1|x;w)$). It's kind of fucked up that it's called ``logistic regression'' instead of ``logistic classification''.
\begin{align}
 h_w(x) = \sigma(w^T x)
\end{align}

\newpar
Alternative idea: predict $y = 1$ if $w x^T \geq 0$ and $y = 0$ otherwise (see support vector machines in a later chapter.)
\newpar
We use \tbf{Logistic Loss}, as already seen in the introduction:
\begin{align}
 \loss(y_i,h_w(x_i)) =
 \begin{cases}
    -log(1-h_w(x_i)) & y_i = 0\\
    -log(h_w(x_i)) & y_i = 1
 \end{cases}
\end{align}
Note: I wrote $h_w(x_i)$ here because that's what the lecture does, but there's no real reason not to just write $\yhat(x_i)$ instead.
%
\subsection{Gradient Descent}:
\begin{itemize}
 \item For linear regression using L2 loss:
 \begin{align}
  \loss(w) = \sum_{i=1}^N (y_i - f(x_i; w))^2 
 \end{align}
 \begin{align}
  \implies \pd{J(w)}{w} = \sum_{i=1}^N -2(y_i - f(x_i;w))x_i
 \end{align}
 \item For logistic regression using logistic loss:
 \begin{align}
  \loss(w) = \sum_{i=1}^N -y_i log(h_w(x_i))-(1-y_i)log(1-h_w(x_i))
 \end{align}
 \begin{align}
  \implies \pd{J(w)}{w}  = \sum_{i=1}^N -(y_i - h_w(x_i))x_i
 \end{align}
\end{itemize}
%
\chapter{Principles of Regularization}
Reminder: for $x \in \mathbb{R}$, the polynomial prediction model of degree $M$ is simply:
\begin{align}
 \yhat = f(x;\theta) = \sum_{j=0}^M \theta_j x^j
\end{align}
Optimal values $\theta1^*$ are learned by minimizing the empirical risk (which is just the L2 loss? Why is a new term needed here?)
\begin{align}
 \theta^* := \argmin{\theta} \sum_{i=1}^N (f(x_i;\theta)-y_i)^2
\end{align}
Small $M$ leads to underfitting, large $M$ leads to overfitting.
%
\section{Bias-Variance Tradeoff}
The expected target of $y$ given $x$ is written 
\begin{align}
 \overline{y}(x) := E_{y|x}(x) = \int_y y p(y|x)  
\end{align}
The expected prediction model $\overline{f}(x)$ over sample datasets is the model we expect to obtain given that we train randomly sampled data $D$ that was sampled following a distribution $\mathcal{P}$:
\begin{align}
 \overline{f}(x) := E_{D \sim P^N}[\hat{f}(x;D)]
\end{align}
The expected value of the loss function is also called the \tbf{expected test error}:
\begin{align}
 E_{(x,y)\sim P}[(\hat{f}(x;D)-y)^2]
\end{align}
It turns out that the expected test error can be directly decomposed into a simple function of the variance, the bias squared, and the expected noise:
\begin{align}
 E_{x,y,D}[(\hat{f}(x;D)-y)^2] &= E_{x,D}[(\hat{f}(x;D)-\overline{f}(x;D))^2]\\ &+ E_{x,D}[(\overline{f}(x;D) - \overline{y}(x))^2]\\ &+ E_{x,y}[(\overline{y}(x) - y)^2]
\end{align}
Recall the definitions of noise, bias and variance:
\begin{itemize}
 \item The \tbf{variance} of a random variable $X$ is defined as:
 \begin{align}
  \sigma = E[(X - E[X])^2] = E[(f - E(f))^2] = E[f - \overline{f}]
 \end{align}
 \item The \tbf{bias} of a predictor $X$ that predicts $y$ is defined as:
 \begin{align}
  b = E[E[X] - y] = E[\overline{f} - \overline{y}]
 \end{align}
 (Note: Why $\overline{y}$ instead of $y$? I assume both work because its wrapped up in an expected value anyways?)
 \item The \tbf{noise} is the difference in our data from the expected value of the data, i.e:
 \begin{align}
  n = (\overline{y} - y)
 \end{align}
\end{itemize}
%
\section{Regularization}
An increase in model complexity will always lead to an increase in variance, but a decrease in bias. In practice, this means that any method of decreasing a models variance will increase the bias and vice versa. Since sensible models are more likely to overfit, Regularization is very commonly used, since it decreases variance. Common regularization functions are:
\begin{align}
 L1: \Omega(\theta) = \frac{1}{|\theta|} \sum_{k=1}^{|\theta|} |\theta_k|
\end{align}
\begin{align}
 L2: \Omega(\theta) = \frac{1}{|\theta|} \sum_{k=1}^{|\theta|} \theta_k^2
\end{align}
%
\chapter{Support Vector Machines}
Motivation: Classify data by seperating it using a linear decision boundary. We want to find the boundary that maximizes the margin between the line and the current data.
\newpar
Seperation is generalized to arbitrary dimensions by hyperplanes:
\begin{align}
 H(x): w^T x + w_0 = 0
\end{align}
Classification of $x_i$ then comes down to simply taking $sign(H(x_i)$.
\newpar
As a loss function, we can take a variant of the hinge loss:
\begin{align}
 \sum_{i=1,f(x_i;w) = -1}^N -y_i(w^Tx_i + w_0)
\end{align}
Since this will be negative if $y_i$ and $sign(w^Tx_i + w_0)$ don't agree.
The size of the margin scales inversely with the size of the weights.
\newpar
If the data cant be linearly seperate, we introduce slack margins, i.e. we enforce a margin greater than $1 - \xi_i$ and then minimize $\xi_i$.
\newpar
Maximizing the margin is equivalent to minimizing the squared weights $w^T w$. (I'm skipping a significant amount of stuff on the lagrange dual form of the SVM problem  at this point due to time constraints)
\newpar
The set of points whose removal would allow the creation of a different hyperplane with a larger margin is known as the set of \tbf{support vectors}. The dual formulation allows us to do inference on a new point by calculating its dot product with the support vectors, instead of having to evaluate the entire hyperplane. To be honest, I'm not entirely sure how this is more efficient.
\newpar
We can still use SVMs on ``strongly'' non-linearly seperable data, where slack margins aren't enough, by applying a nonlinear mapping $\phi(x)$ to the data to make it linearly seperable in a higher dimensional space. We can efficiently do the necessary calculations using the \tbf{Kernel Trick}, where we replace instances of $\phi(x_i)^T \phi(x_j)$ in relevant equations with a kernel function $K(x_i,x_j) = \phi(x_i)^T \phi(x_j)$ that doesn't need to explicitly calculate the mapping $\phi$.
%
\chapter{Decision Trees}
\begin{itemize}
 \item Equivalent to step function with steps in the middle between data points
 \item Depth $D$ is a property of individual nodes, the maximum depth of any node is known as the \tbf{height} of the tree.
 \item Trees partition feature space into regions $R_i$ and then assign a constant prediction to each region.
 \item The constant is obtained by aggregating the data points in that region, i.e. taking the mean:
 \begin{align}
  \yhat_{R_i}(x) = \frac{1}{|R_i|} \sum_{(x,y) \in R_i} y
 \end{align}

 \item In this lecture, we only use \tbf{binary splits}.
\end{itemize}
%
\section{Choosing Splits}
We assign to each set $D$ an \tbf{Impurity} $H(D)$.
We then define the \tbf{information gain} $I(x \leq v)$ of the split $x \leq v$ as the difference in impurity before and after the split:
\begin{align}
 I(x \leq v) = |D| \cdot H(Y) - |D_L| \cdot H(Y_L) - |D_R| \cdot H(Y_R)
\end{align}
Where:
\begin{itemize}
 \item $D$ is the original set of data
 \item $D_L = \{(x,y) \in D \mid y \leq v\}$
 \item $D_R = \{(x,y) \in D \mid y > v\}$
\end{itemize}
Choosing correct splits then naturally comes down to maximizing the information gain.
%
\subsection{For regression}
In regression, we want to minimize the variance:
\begin{align}
 |D| \cdot H(D) = \sum_{(x,y) \in D} (y - \yhat_L(x))^2
\end{align}
%
\subsection{For classification}
In classification, where $y \in \{1, \hdots, k\}$, we want to minimize the \tbf{entropy} of the data in the chosen split:
\begin{align}
 H(D) = -\sum_{k=1}^K p(y = k) log_2(p(y=k))
\end{align}
%
\section{Pros and Cons of Decision Trees}
\begin{multicols}{2}
Pros:
\begin{itemize}
 \item Flexible with many tunable hyperparameters (splitting criterion, leaf model, split type, depth etc.)
 \item Easily interpretable
 \item Easily deals with different types of inputs
 \item Handles features of different priorities well
 \item Easily scalable for large datasets
\end{itemize}
Cons:
\begin{itemize}
 \item Tends to overfit
 \item Fully deterministic, making them bad at bagging (see later section).
 \item[\vspace{-1em}]
 \item[\vspace{-1em}]
 \item[\vspace{-1em}]
\end{itemize}
\end{multicols}
%
\chapter{Neural Networks}
Common activation functions and their gradients:
\begin{itemize}
 \item ReLu:
 \begin{align}
  \text{ReLu}(x) &= x \cdot \mathbf{1}(x \geq 0)\\
  \pd{ReLu}{x} &= \mathbf{1}(x \geq 0)\\
 \end{align}
 \item Sigmoid:
 \begin{align}
  \sigma(x) &= \frac{1}{1+e^{-x}}\\
  \pd{\sigma}{x} &= \sigma(x)(1-\sigma(x))
 \end{align}
 \item Hyperbolic Tan:
 \begin{align}
  \tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}}\\
  \pd{\tanh}{x} &= 1 - \tanh(x)^2
 \end{align}
 \item Softmax:
 \begin{align}
  \sigma_{SM}(x_i) &= \frac{e^{x_i}}{\sum_{n=1}^{N} e^{x_n}}\\
  \pd{\sigma_{SM}}{x} &= oof,\ not\ doing\ this\ one
 \end{align}

\end{itemize}
%
\chapter{Ensembles}
\section{Bagging}
Idea: If we have many models with low biases but a high variance, and they have independent outputs, then by the law of large numbers we can get closer to the correct value by averaging the prediction of these different models.
\newpar
Since we only have one big dataset $D$, our models aren't actually independent in practice. We try to get close by creating many different smaller datasets $D_i$ from $D$ by \tbf{sampling with replacement}, i.e. we allow a dataset to draw the same element multiple times, and then training our models on those datasets. This is known as \tbf{Bootstrap Aggregation}.
\newpar
The output of our bagged model is then simply the mean of the ouputs of the individual models:
\begin{align}
 \hat{f}(x) = \frac{1}{K} \sum_{k=1}^K \hat{f}^k(x)
\end{align}
\newpar
The models still have to be \tbf{uncorrelated} to each other, i.e. their covariances have to be $0$, otherwise this process doesn't work. To be precise, the expected squared error of an ensemble of $K$ models with a shared variance $v$ and a covariamce $c$ is:
\begin{align}
 \epsilon = E\left[\left(\frac{1}{K}\sum_{k=1}^k \epsilon_k\right)^2\right] = \frac{1}{K^2}E\left[\left(\sum_{k=1}^k \epsilon_k^2 + \sum_{k \neq l} \epsilon_k \epsilon_l\right)\right] = \frac{1}{K}v + \frac{K-1}{K}c
\end{align}
\tbf{Trees} are low bias, but often end up being correlated. We can decrease their correlation by randomizing which feature we decide to split on at each point. Bagged Trees that split on random features are known as \tbf{Random Forests}. Generally, random forests don't split on entirely random features, instead they pick $m$ features randomly out of the full set of $M$ features and then pick the best split among the chosen features.
\newpar
Bagged ensembles provide an estimate of the test error via the out-of-bag training error, i.e. the output we get from the bagged model if we only take the output $f_k(x_i)$ of each submodel $k$ if $x_i$ wasn't part of its training data $D_k$:
\begin{align}
 \epsilon_{OOB}& = \frac{1}{|D|} \sum_{i=1}^{|D|} \loss\left(y_i, \frac{1}{S(x_i)} \sum_{k \in S(x_i)}f_k(x_i)\right)\\
 S(X) &= \{k \in \{1, \hdots, K\} \mid (x,y) \notin D_{k}\}
\end{align}
The variance of an ensemble is:
\begin{align}
 \sigma(x) = \sqrt{\frac{1}{K-1}\sum_{k=1}^K \left(\hat{f}_k(x) -\hat{f}(x)\right)^2}
\end{align}
%
\section{Boosting}
Boosting creates a low-bias model out of an esemble of high-bias models, by training each model to correct the error of the last model. The final ensemble $F_K$ is a weighted sum of a set of models $f_k$:
\begin{align}
 F_K(x) = \sum_{k=1}^K \alpha f_k(x)
\end{align}
We train each model sequentially while keeping the parameters of past models fixed:
\begin{align}
 \theta_{k+1} = \argmin{\theta}\sum_{n=1}^N \loss(y_n, F_k(x_n; \theta_{fixed}) + \alpha f(x_n; \theta))
\end{align}
\subsection{Gradient Boosting}
\begin{enumerate}
 \item Initialize model with a constant value:
 \begin{align}
  F_0(x) = \argmin{\alpha_0} \sum_{i=1}^n \loss(y_i,\alpha_0)
 \end{align}
 In the lecture, this calculation was skipped and 0 was always used instead.
 \item Let $M$ be an arbitrary number of maximum iterations. For each $m = 1, \hdots, M$:
 \begin{enumerate}
 \item Compute \textit{pseudo-residuals}, i.e. the gradient of the loss function by the ensemble output:
 \begin{align}
  z_m = \pd{\loss(y_n, F_k(x_n))}{F_k (x_n)}
 \end{align}
 \item Train model $f_m$ on the set $\{(x_i,r_i)\}^n_{i=1}$
 \item Compute the best possible $\alpha_n$, i.e. $\alpha_m = \argmin{\alpha} \sum_{i=1}^n \loss(y_i, F_{m-1} + \alpha f_m(x_i))$
  \item Update the ensemble model: $F_k (x) = F_{m-1}(x) + \alpha_m f_m(x)$
 \end{enumerate}
\item Output the finished ensemble model $F_M$.
\end{enumerate}
\subsection{AdaBoost}
AdaBoost is a special case of Gradient Boosting with a particular loss function that results in
weighting points misclassified by previous models higher than correctly classified ones.
\newpar
Specifically, AdaBoost uses an approximation of \tbf{instance-penalty loss}:
\begin{align}
 \loss(y_i, f(x_i)) &= e^{-y_i f(x_i)}\\
 &\approx \frac{\sum_{i=1}^N w^{(k)}_i \cdot 1(y_i \neq f(x_i))}{\sum_{i=1}^N w^{(k)}_i}\\ &:= \epsilon
\end{align}
In natural language, this just means dividing the sum of the weights of incorrectly classified data points by the total sum of the weights.
\newpar
The model weights are then given by:
\begin{align}
 \alpha_k = ln\left(\frac{1 - \epsilon_k}{\epsilon_k}\right)
\end{align}
The feature weights $w_i$ are initialized as $\frac{1}{N}$, i.e. all features are weighted equally, and then updated according to the following rule:
\begin{align}
w_i^{(k+1)} = w_i^{(k)} e^{(\alpha_k \cdot 1(y_i \neq f(x_i))}
\end{align}
This means that the weights of correctly classified features stay the same, while the weights of incorrectly classified features increase by a factor of $\frac{1 - \epsilon_k}{\epsilon_k}$.
%
\section{Gradient-Boosted Decision Trees}
Tree overfitting happens if there are too many leaves (coresponding to too many steps in the final function) or if the leaves' output values are too high (correspnding to large jumps in the final function). A common regularization function for trees is thus:
\begin{align}
 \Omega(f) = \gamma T + \frac{\lambda}{2}\sum_{j=1}^T w_j^2
\end{align}
Where $T$ denotes the number of leaves.
\newpar
For Gradient Boosted Trees, the function we want to minimize at each step is:
\begin{align}
 J(y_i, f^{(k)}) = \left(\sum_{n=1}^N \loss_n^{k-1} G_n f^{(k)}(x_n) + \frac{1}{2} H_n (f^{(k)}(x_n))^2\right) + \Omega(f^{(k)})
\end{align}
i.e. we approximate $\loss_n^{(k)}$ via a Taylor expansion:
\begin{align}
 \loss_n^{(k)} \approx \loss_n^{(k-1)} +  G_n^{(k)}f^{k}(x_n) + \frac{1}{2}H_n(f^{(k)}(x_n))^2
\end{align}
Minimizing this loss function corresponds to minimizing the following Objective Function $O_j$ for each Leaf $j$:
\begin{align}
 O_j = - \frac{1}{2}\frac{\left(\sum_{n=1}^N G_n\right)^2}{\left(\lambda + \sum_{n=1}^N H_n\right)} + \gamma
\end{align}
Where $G_n$ is the gradient of the loss function over the model output and $H_n$ is the Hessian matrix, i.e. the multidimensional equivalent of the second derivative.
\newpar
If we let $I_J$ denotes the indices of all data points in the region $R_J$ belonging to leaf $J$, the optimal leaf weights are then given by:
\begin{align}
 w^* = - \frac{\sum_{n=1}^N G_n}{\lambda + \sum_{n=1}^N H_n}
\end{align}

\newpar
The gain of splitting a leaf is simply:
\begin{align}
 Gain_j = O_j - O_L - O_R
\end{align}
just like the information gain of regular trees.
\newpar
We can decide to stop splitting further if $Gain_j \leq 0$. This means that $\gamma$ directly controls the minimum gain needed for the model to perform a split.
\newpar
The most common loss function for classification is logistic loss, so here are the gradient and hessian pre-calculated:
\begin{align}
 \loss_n &= -y_n \ln(\sigma(\yhat_n)) - (1-y_n)\ln(1-\sigma(\yhat_n))\\
 G_n &= \pd{\loss_n}{\yhat_n} = \sigma(\yhat_n) - y_n\\
 H_n &= \pd{^2 \loss_n}{^2\yhat_n} = \sigma(\yhat_n)(1 - \sigma(\yhat_n))
\end{align}
Remember that $\yhat_n^{(k)} = \yhat_n^{(k-1)} + f^{(k)}(x_n)$. (Or $\yhat_n^{(k)} = \yhat_n^{(k-1)} + \alpha f^{(k)}(x_n)$ if we want to do a weighted sum). In this lecture, we assume $\yhat^{(0)} = 0$.
\newpar
GBDTs have very low biases while also retaining a relatively low variance. They are also very fast to train, requiring a runtime of $O(kMN \log(T))$, where $k$ is the number of trees in the ensemble, $M$ is the number of features and $N$ is the number of data points.
%
\chapter{Hyperparameter Optimization}
A function $f(\lambda, D)$ is called a \tbf{response function} if its output is the loss of a given model that was trained on the dataset $D$ using the hyperparameters $\lambda$. It can also be a function of the hyperparameters and dataset that we want to maximize, such as classification accuracy.
%
\section{Bayesian Optimization}
Straightforwardly optimizing $f$ is costly enough to generally be infeasible, since it would involve retraining the entire model for every new set of hyperparameters. Instead, we trade a surrogate model $\Psi$ to predict the output of $f$ based on known values. We then obtain new hyperparameters by picking the ones that maximize/minimize $\Psi$, training a model using these new hyperparameters, and retraining $\Psi$ on the augmented dataset that includes the performance of the model using the new hyperparameters.
\newpar
$\Psi$ is often called a \tbf{Gaussian Process Regressor}, and the process of optimizing hyperparameters by getting them from the surrogate model is also known as \tbf{Bayesian Optimization} or \tbf{Sequantial Model Based Optimization (SMBO)}.
\newpar
If we know the mean $\mu$ and the variance  $\sigma$ of the surrogate model, we can also pick our hyperparameters based on the \tbf{lower confidence bound}:
\begin{align}
 LCB(\lambda) = -\mu + \beta \sigma
\end{align}
or, for classification, where we want to maximize $\mu$, based on the \tbf{upper confidence bound}:
\begin{align}
 UCB(\lambda) = \mu + \beta \sigma
\end{align}
\section{Gray-Box Search}
Bayesian Optimization is an example of black box optimization, where we don't know the structure of the problem. \tbf{Gray-box optimization} does have knowledge about the structure of the problem. This means that we can:
\begin{itemize}
 \item We can do \tbf{$k$-fold cross-validation} to discard bad hyperparameters early:
\begin{itemize}
 \item Split data $D$ into $k$ subsets
 \item For every subset $D_i$, use $D \setminus D_i$ to train a model and $D_i$ as the test data
 \item Retain evaluation score of each model
\end{itemize}
\item If we use iterative ML algorithms, we can simply stop poor runs early.
\begin{itemize}
 \item Hyperband: Start with a lot of models with random hyperparameters, then discard low-performing half / two thirds / a different fractions of the models each time a certain number of learning epochs is up.
\end{itemize}
\end{itemize}
Hyperband is faster than Bayesian Optimization, but Bayesian Optimization generally produces better results if given enough time (but ``enough time'' may be a factor on the order of $10^2$)
%
\chapter{Advanced Regularization Or Whatever}
\section{Dropout}
During training of a neural network, at each step, train as if a random set of node outputs were actually $0$. We then take the output of multiple versions of a neural network with different dropped weights, making dropout a form of bagging.
%
\section{Residual Networks}
A \tbf{residual network} is a type of neural network whose architecture includes connections that ``skip'' a certain number of layers, i.e. a neuron $h_n$ may be connected to a neuron $h_{n+c}$ for $c \geq 2$.
\newpar
Residual networks were the key advancement needed to start training ever deeper models, since they fix issues deep neural networks run into, like vanishing/exploding gradients during back propagation.
\newpar
Shake-Shake is a method of regularization for residual networks that randomly assigns weights to the output of each ``branch'' during training (?)
%
\section{Data Augmentation}
\tbf{Data Augmentation} is the process of artificially creating additional data before training. It is primarily used in Image Processing Application, where it is easy to create realistic, sensible data through changes to images such as rotation, translation etc. Additional methods of data augmentation for images include:
\begin{itemize}
 \item Cutout, where a random patch of the image is replaced with black pixels
 \item Mixup, where two images are linearly interpolated (i.e. made transparent and then layered over each other)
 \item Cutmix, where a random patch of an image is replaced with a patch from a different image.
\end{itemize}
These can help a model deal with images in unusual circumstances, such as recognizing an object that is partially covered by something else.
%
\section{Cocktails}
A combination of multiple regularizers is known as a \tbf{Cocktail}. Using cocktails is generally better than using an individual regularizer. Finding the best cocktail for a given problem is an active area of research.
%
\chapter{Error Measures}
If a loss function we wish to use for a given problem is non-differentiable, we can use another arbitrary function as a \tbf{Proxy Loss} as long as we know that the minima of the proxy loss are at the same points as the minima of the original loss.
%
\section{Classification}
Missclassification Rate:
\begin{align}
 MCR(y,\yhat) = \frac{FP + FN}{TP+TN+FP+FN}
\end{align}
An effective proxy loss of $MCR$ is the cross-entropy (presented here in the general form used for multiclass-classification, where $y_{n,c}$ is $1$ if $y_n = c$ and $0$ otherwise:
\begin{align}
 - \sum_{n=1}^N \sum_{c=1}^C y_{n,c} \log(\yhat_c(x_n;\theta))
\end{align}

\newpar
True Positive Rate or Recall:
\begin{align}
 TPR(y,\yhat) = \frac{TP}{TP+FN}
\end{align}
False Positive Rate (``Ratio of false alarms'')
\begin{align}
 FPR(y,\yhat) = \frac{FP}{TN+FP}
\end{align}
The curve we get by plotting $TPR$ on the $y$-axis against $FPR$ on the $x$-axis for different activation thresholds is known as the $ROC$ curve (``Receiver Operating Characteristic''). The Area under the $ROC$ curve (``$AUC$'') tells us the likelihood that, for a given pair of $y_i = 0$ and $y_j = 1$, our model predicts a higher value to $\yhat_j$ than $\yhat_i$.
\newpar
Precision (Percentage of predicted positives that are correct):
\begin{align}
 PRC = \frac{TP}{TP+FP}
\end{align}
\newpar
If we plot precision and recall against each other as we did for the $ROC$-curve, the result is known as a $PR$(precision-recall)-curve. Using a $PR$ curve makes sense in imbalanced datasets where we care more about the positive class.
\newpar
F1 (Harmonic mean between Precision and Recall):
\begin{align}
 F1 = 2\frac{PRC \cdot REC}{PRC + REC}
\end{align}
F1 is hard to optimize for because it is non-differentiable and non-decomposable. We have to train models using variants of regular classification loss functions (logistic loss, hinge loss etc.) as proxies. It also helps to oversample the positive or negative class, such that the training minibatches have approximately 50\% positive and 50\% negative instances.
\section{Regression}
Scale-dependent errors:
\begin{itemize}
 \item Mean Absolute Error:
 \begin{align}
  MAE = \frac{1}{N}\sum_{n=1}^N |y_n - \yhat_n|
 \end{align}
 \item Root Mean Square Error:
 \begin{align}
  RMSE = \sqrt{\frac{1}{N}\sum_{n=1}^N (y_n - \yhat_n)^2}
 \end{align}
\end{itemize}
Scale independent errors: Percentage errors, such as Mean Absolute Percentage Error:
\begin{align}
 MAPE = \frac{1}{n}\sum_{n=1}^N \left|\frac{y_n - \yhat_n}{y_n}\right|
\end{align}
An issue with percentage errors is that they are undefined when $y_n = 0$ and produce extreme values when $y_n \approx 0$. One fix is to use scaled errors such as \tbf{Mean Absolute Scaled Error}:
\begin{align}
 MASE = \frac{1}{N} \sum_{n=1}^N \left|\frac{y_n - \yhat_n}{\frac{1}{N-1}\sum_{n'=2}^N |y_{n'} - y_{n'-1}|}\right|
\end{align}
\section{Ranking}
$DCG$ (\tbf{Discounted Cumulative Gain}):
\begin{align}
 DCG = \sum_{i=1}^K \frac{2^{l_i} - 1}{\log(i+1)}
\end{align}
Where $l_i$ is the ``ground truth relevance'' of the data point that our model assigns $i$-th most relevant. The $DCG$ of the ground truth ranking is known as the $IDCG$, and a good error measure for our ranking model is then given by the normalized $DCG$:
\begin{align}
NDCG = \frac{DCG}{IDCG}
\end{align}
\newpar
Pairwise Ranking Loss (Logistic Loss adapted for ranking problems):
\begin{align}
 \loss_{i,j} = -P_{i,j} \log(\hat{P}_{i,j}) - (1 - P_{i,j}) \log(1 - \hat{P}_{i,j})
\end{align}
Where $P_{ij}$ is given by:
\begin{align}
 P_{ij} = \begin{cases}
           1 & l_i > l_j\\
           0.5 & l_i = l_j\\
           0 & l_i < l_j
          \end{cases}
\end{align}
And $\hat{P}_{i,j}$ is an estimation of the probability that the pair $i,j$ is correctly ranked:
\begin{align}
 \hat{P}_{i,j} = \sigma(s_i-s_j)
\end{align}
Where $s_i$ are the predicted relevances.
\newpar
Alternatively, we may reexpress $P_{ij}$ in terms of the following term $S_{ij}$:
\begin{align}
 S_{ij} = \begin{cases}
           1 & l_i > l_j\\
           0 & l_i = l_j\\
           -1 & l_i < l_j
          \end{cases}
\end{align}
Sadly, Pairwise Loss is provably non-optimal for maximizing $NDCG$.
\end{document}

