\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{bbm}
%\usepackage{biblatex}
%\addbibresource{bib.bib}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

\newcommand{\tbf}{\textbf}
\newcommand{\argmin}[1] {
    \begin{array}{c}
        \vphantom{#1}\\ %vphantom makes sure text is centered vertically in the end
        \text{argmin}\\
        #1\\
        \end{array}
    }
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}

\renewcommand*\contentsname{Table of contents}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\yhat}{\hat{y}}


\begin{document}
\include{title}
\tableofcontents
\thispagestyle{fancy}
\chapter{Introduction}
Given a set $\{(x_1,y_1), \hdots, (x_n,y_n)\}$, i.e. a set of features (inputs) $x = \{x_1, \hdots, x_n\}$ and a target (desired output) $y = \{y_1, \hdots, y_n\}$, Machine Learning is the process of algorithmically finding the best possible function $f(x)$ such that
\begin{equation*}
 \forall x_i[f(x_i) \approx y_i].
\end{equation*}
$y$ is also known as the \tbf{ground truth}.
In \tbf{unsupervised learning}, $y$ is not explicitly given to the algorithm (and may not even exist). The focus of the course, and thus these notes, was \tbf{supervised learning}, where $y$ is explicitly given.
\newpar
In order to find an optimal $f(x)$, we first rephrase the function in terms of \tbf{learnable parameters $\theta$} to get a function $f(x,\theta)$. We often write $f(x_i, \theta) = \hat{y}_i$, where the hat $\hat{}$ is supposed to show that $\hat{y}_i$ is an approximation of $y_i$. The full image $\hat{y}$ is often called the \tbf{prediction model}.
\newpar
We now need to define a seperate function that we can use to judge how good our values of $\theta$ are. Such a function is known as a \tbf{loss function}, which we write as $\mathcal{L}(y, \hat{y})$. The problem of finding the best possible $f$ then comes down to a minimization problem of the form
\begin{equation*}
\argmin{\theta} \sum_{n=1}^N \mathcal{L}(y_n, \hat{y}_n)
\end{equation*}
This function that we want to minimize is also called the \tbf{empirical risk}.
\newpar
\section{Basic Prediction Models}
Common basic prediction models include:
\begin{itemize}
 \item Linear Models:
 \begin{equation*}
 \hat{y_n} = \theta_0 + \sum_{m=1}^M \theta_m x_{n,m} 
 \end{equation*}
 \item Polynomial Regression Models:
 \begin{align*}
 \hat{y_n} = \theta_0 &+ \sum_{m=1}^M \theta_m x_{n,m}\\
           &+ \sum_{m=1}^M \sum_{m'=1}^M \theta_{m,m'} x_{n,m} x_{n,m'}\\
           &+ \sum_{m=1}^M \sum_{m'=1}^M \sum_{m''=1}^M \theta_{m,m',m''} x_{n,m} x_{n,m'} x_{n,m''}\\
           &+ \hdots\\
 \end{align*}
 \item Decision Trees
 \item Neural Networks
\end{itemize}
%
\subsection{Decision Trees}
In a Decision Tree, the model gives its answer by starting at the root node of a binary tree and then moving from each node to one of its children by answering a yes/no question at each leaf. A decision tree can be thought of as approximating an arbitrary function through a piecewise combination of constant functions.
%
\subsection{Neural Networks}
In a Neural Network, a given neuron labeled $i$ consists of a non-linear function $f_i(x, \theta_i)$, such that if the output of a neuron $i$ is connected to a neuron $j$, then the output $\hat{y_{n,j}}$ of the neuron $j$ is
\begin{align*}
 \hat{y_{n,j}} = f_j(f_i(x, \theta_i), \theta_j)
\end{align*}
%
\section{Common Loss Functions}
A \tbf{Regression Problem} is a problem where the output is an arbitrary real number. Common loss functions for regression include:
\begin{itemize}
 \item Least Squares:
 \begin{align*}
  \loss(y_n, \yhat_n) = (y_n - \yhat_n)^2
 \end{align*}
 Least Square Loss is an approximation of the maximum likelihood estimator of a linear model with normally distributed error (with a mean of 0).
 \item L1 Loss:
 \begin{align*}
  \loss(y_n, \yhat_n) = |y_n - \yhat_n|
 \end{align*}
\end{itemize}
For \tbf{Binary Classification Problems}, where $y_n$ can only take on two possible values, more specialized loss functions are used:
\begin{itemize}
 \item Logistic Loss, $y_n \in \{0,1\}$:
 \begin{align*}
  \loss(y_n, \yhat_n) = -y_n \log(\yhat_n) - (1-y) \log (1-\yhat_n)
 \end{align*}
 \item Hinge Loss, $y_n \in \{-1,1\}$:
 \begin{align*}
  \loss(y_n, \yhat_n) = \max(0,1-y_n\yhat_n)
 \end{align*}
\end{itemize}
A \tbf{Softmax} is a type of function used to express \tbf{Non-Binary Classification Problems}, where targets $y_n \in {1, \hdots, C}$, as a set of binary classification targets:
\begin{align*}
 y_{n,k} = 
\begin{cases}
1 & y_n = k\\
0 & y_n \neq k\\
\end{cases}
\end{align*}
The final result is then obtained by re-expressing the results as probabilities among classes as follows:
\begin{align*}
 \yhat_{n,k} = \frac{e^{f(x_n, \theta_k)}}{\sum_{i=1}^{C}e^{f(x_n, \theta_i)}}
\end{align*}
The exponential function is used to avoid negative probabilities. A common loss function for Softmax problems is \tbf{Logloss}:
\begin{align*}
 \loss(y_n, \yhat_n) = -\sum_{i=i}^{C} y_{n,c} \log(\yhat_n,c)
\end{align*}
%
\section{Overfitting and Underfitting}
Two common issues every model needs to deal with are \tbf{Underfitting}, where the bias of a model is too high, leading to a model that is unable to capture the data's complexity, and \tbf{Overfitting}, where the variance of a model is too high, meaning that the model has captured noise in the training data and is unable to generalize to pieces of data not included in the training data. Very simple models like linear models tend to be prone to underfitting, while most modern models that are more complex tend to be more prone to overfitting. Decision Trees in particular are very prone to overfitting.
%
\subsection{Regularization}
Regularization is the process of fighting overfitting by reducing the the size of the parameters of a model. This is expressed by adding a \tbf{Regularization Function} as a penalty term to the underlying optimization problem of minimizing the empirical risk:
\begin{equation*}
\argmin{\theta} \sum_{n=1}^N \mathcal{L}(y_n, \hat{y}_n) + \alpha \Omega(\theta)
\end{equation*}
Just as there are many different possible loss functions, there are also many different possible regularization functions. Using a regularization always comes with the tradeoff of increasing model bias (but is done in the vast majority of models, since with complex modern models, overfitting tends to be a more common issue than underfitting).
%
\section{ML Design Cycle}
\begin{enumerate}
 \item Pre-processing
 \item Feature extraction / Feature encoding
 \item Feature selection
 \item Machine learning
 \item Evaluation / Model Selection
 \item Post-processing
\end{enumerate}
%
\section{Non-Parametric Models}
\subsection{Nearest Neighbors}
Predict target $y_i$ by averaging over the targets of the $k$ nearest points $x_1, \hdots, x_k$ to the point $x_i$.
For a continuus target the aggregation is simply the mean:
\begin{align}
 f(x_i) = \frac{1}{k} \sum_{y_j \in Y_{near}}^{k} y_j
\end{align}
For classification, we get our probability of the target being a certain class by taking the number of points belonging to each class and dividing by the total number of points we sampled:
\begin{align}
 f_c(x) = \frac{|y_j y_j \in Y_{near} \wedge y_j = c|}{k}
\end{align}
%
\subsection{Naive Bayes}
Bayes Rule:
\begin{align}
 P(y|x) = P(y) \frac{P(x|y)}{P(x)}
\end{align}
Using the law of total probability this can be extended to:
\begin{align}
 P(y|x) = P(y) \frac{P(x|y)}{\sum_{y' \in Y}P(x|y')P(y')}
\end{align}
Note that all of this only formally works if all features are conditionally independent.
%
\chapter{Linear Methods}
\section{Linear Regression}
The following problem is also known as the regression problem:
\begin{itemize}
 \item Dataset of $N$ instances $(x_i, y_i)$ ($x_i$ is generally a vector) 
 \item Find a model $\yhat$ that minimizes a loss function $\loss$:
 \begin{align*}
  \argmin{w \in W} \sum_{i=1}^{N} \loss(y_i f(x_i; w))
 \end{align*}
 \item Very simple and basic model: \tbf{Linear Regression}
 \begin{align*}
  f(x_i;w) &= w_0 + x_i w^{T}
 \end{align*}
 $w_0$ is a fixed offset and thus often called the \tbf{bias}, while $w$ is the \tbf{weight vector} or \tbf{parameter vector}.
\end{itemize}
Linear regression is more powerful than it looks - for example, $f(x) = (x+1)^2$ can be expressed as a linear function by increasing the dimensionality and writing it as \[f(a,b) = a +2b + 1\] (such that $b = x$ and $a = x^2$). Formally, we can take any set of arbitrary \tbf{basis functions} $\phi_j(x_i)$ and arrive at a linear model
\begin{align*}
 f(x_i;w) = w_0 + \sum_{j=1}^{M} w_j \phi_j(x_i)
\end{align*}
Linear models are therefore linear in the weights $w_i$, but they don't have to be linear in the inputs $x_i$.
\newpar
The \tbf{residual error} of a linear model is given by:
\begin{align*}
 y_i - f(x_i;w)
\end{align*}
For the loss function of a linear model, we just sum the losses of the individual predictions. Typical choices for the loss function are L2 loss $(y-\yhat)^2$ and L1 loss $|y - \yhat|$
\newpar
A closed-form solution for linear regression using L2 loss exists under the following assumptions:
\begin{itemize}
 \item The expected value of the residuals is 0: $\forall i: E(\epsilon_i) = 0$
 \item Residuals are uncorrelated and share the same variance: $\forall i: Var(\epsilon_i) = \sigma_2$
 \item Residuals follow a normal distribution: $\forall i: \epsilon_i \sim \mathcal{N}(0,\sigma^2)$
\end{itemize}
If these hold, then the closed-form solution is:
\begin{align}
 w = (X^T X)^{-1} X^T y
\end{align}
Where $X$ is the input matrix, i.e.
\begin{align*}
X = 
\begin{pmatrix}
x_1\\
\vdots\\
x_n\\
\end{pmatrix}
\end{align*}
\section{Linear Classification}
\tbf{Logistic regression} is a way of using linear regression to solve classification problems, by bounding the output of a linear model to $0 \leq h_w(x) \leq 1$ by taking the sigmoid of our output. The output is then interpreted as the probability that $y = 1$ (formally, $h_w(x) = P(y = 1|x;w)$). It's kind of fucked up that it's called ``logistic regression'' instead of ``logistic classification''.
\begin{align}
 h_w(x) = \sigma(w^T x)
\end{align}

\newpar
Alternative idea: predict $y = 1$ if $w x^T \geq 0$ and $y = 0$ otherwise (see support vector machines in a later chapter.)
\newpar
We use \tbf{Logistic Loss}, as already seen in the introduction:
\begin{align}
 \loss(y_i,h_w(x_i)) =
 \begin{cases}
    -log(1-h_w(x_i)) & y_i = 0\\
    -log(h_w(x_i)) & y_i = 1
 \end{cases}
\end{align}
Note: I wrote $h_w(x_i)$ here because that's what the lecture does, but there's no real reason not to just write $\yhat(x_i)$ instead.
\subsection{Gradient Descent}:
\begin{itemize}
 \item For linear regression using L2 loss:
 \begin{align}
  \loss(w) = \sum_{i=1}^N (y_i - f(x_i; w))^2 
 \end{align}
 \begin{align}
  \implies \pd{J(w)}{w} = \sum_{i=1}^N -2(y_i - f(x_i;w))x_i
 \end{align}
 \item For logistic regression using logistic loss:
 \begin{align}
  \loss(w) = \sum_{i=1}^N -y_i log(h_w(x_i))-(1-y_i)log(1-h_w(x_i))
 \end{align}
 \begin{align}
  \implies \pd{J(w)}{w}  = \sum_{i=1}^N -(y_i - h_w(x_i))x_i
 \end{align}
\end{itemize}
\chapter{Principles of Regularization}
Reminder: for $x \in \mathbb{R}$, the polynomial prediction model of degree $M$ is simply:
\begin{align}
 \yhat = f(x;\theta) = \sum_{j=0}^M \theta_j x^j
\end{align}
Optimal values $\theta1^*$ are learned by minimizing the empirical risk (which is just the L2 loss? Why is a new term needed here?)
\begin{align}
 \theta^* := \argmin{\theta} \sum_{i=1}^N (f(x_i;\theta)-y_i)^2
\end{align}
Small $M$ leads to underfitting, large $M$ leads to overfitting.
\section{Bias-Variance Tradeoff}
The expected target of $y$ given $x$ is written 
\begin{align}
 \overline{y}(x) := E_{y|x}(x) = \int_y y p(y|x)  
\end{align}
The expected prediction model $\overline{f}(x)$ over sample datasets is the model we expect to obtain given that we train randomly sampled data $D$ that was sampled following a distribution $\mathcal{P}$:
\begin{align}
 \overline{f}(x) := E_{D \sim P^N}[\hat{f}(x;D)]
\end{align}
The expected value of the loss function is also called the \tbf{expected test error}:
\begin{align}
 E_{(x,y)\sim P}[(\hat{f}(x;D)-y)^2]
\end{align}
It turns out that the expected test error can be directly decomposed into a simple function of the variance, the bias squared, and the expected noise:
\begin{align}
 E_{x,y,D}[(\hat{f}(x;D)-y)^2] &= E_{x,D}[(\hat{f}(x;D)-\overline{f}(x;D))^2]\\ &+ E_{x,D}[(\overline{f}(x;D) - \overline{y}(x))^2]\\ &+ E_{x,y}[(\overline{y}(x) - y)^2]
\end{align}
Recall the definitions of noise, bias and variance:
\begin{itemize}
 \item The \tbf{variance} of a random variable $X$ is defined as:
 \begin{align}
  \sigma = E[(X - E[X])^2] = E[(f - E(f))^2] = E[f - \overline{f}]
 \end{align}
 \item The \tbf{bias} of a predictor $X$ that predicts $y$ is defined as:
 \begin{align}
  b = E[E[X] - y] = E[\overline{f} - \overline{y}]
 \end{align}
 (Note: Why $\overline{y}$ instead of $y$? I assume both work because its wrapped up in an expected value anyways?)
 \item The \tbf{noise} is the difference in our data from the expected value of the data, i.e:
 \begin{align}
  n = (\overline{y} - y)
 \end{align}
\end{itemize}
\section{Regularization}
An increase in model complexity will always lead to an increase in variance, but a decrease in bias. In practice, this means that any method of decreasing a models variance will increase the bias and vice versa. Since sensible models are more likely to overfit, Regularization is very commonly used, since it decreases variance. Common regularization functions are:
\begin{align}
 L1: \Omega(\theta) = \frac{1}{|\theta|} \sum_{k=1}^{|\theta|} |\theta_k|
\end{align}
\begin{align}
 L2: \Omega(\theta) = \frac{1}{|\theta|} \sum_{k=1}^{|\theta|} \theta_k^2
\end{align}
\chapter{Decision Trees}
\begin{itemize}
 \item Equivalent to step function with steps in the middle between data points
 \item Depth $D$ is a property of individual nodes, the maximum depth of any node is known as the \tbf{height} of the tree.
 \item Trees partition feature space into regions $R_i$ and then assign a constant prediction to each region.
 \item The constant is obtained by aggregating the data points in that region, i.e. taking the mean:
 \begin{align}
  \yhat_{R_i}(x) = \frac{1}{|R_i|} \sum_{(x,y) \in R_i} y
 \end{align}

 \item In this lecture, we only use \tbf{binary splits}.
\end{itemize}
\section{Choosing Splits}
We assign to each set $D$ an \tbf{Impurity} $H(D)$.
We then define the \tbf{information gain} $I(x \leq v)$ of the split $x \leq v$ as the difference in impurity before and after the split:
\begin{align}
 I(x \leq v) = |D| \cdot H(Y) - |D_L| \cdot H(Y_L) - |D_R| \cdot H(Y_R)
\end{align}
Where:
\begin{itemize}
 \item $D$ is the original set of data
 \item $D_L = \{(x,y) \in D \mid y \leq v\}$
 \item $D_R = \{(x,y) \in D \mid y > v\}$
\end{itemize}
Choosing correct splits then naturally comes down to maximizing the information gain.
\subsection{For regression}
In regression, we want to minimize the variance:
\begin{align}
 |D| \cdot H(D) = \sum_{(x,y) \in D} (y - \yhat_L(x))^2
\end{align}

\subsection{For classification}
In classification, where $y \in \{1, \hdots, k\}$, we want to minimize the \tbf{entropy} of the data in the chosen split:
\begin{align}
 H(D) = -\sum_{k=1}^K p(y = k) log_2(p(y=k))
\end{align}
\section{Pros and Cons of Decision Trees}
\begin{multicols}{2}
Pros:
\begin{itemize}
 \item Flexible with many tunable hyperparameters (splitting criterion, leaf model, split type, depth etc.)
 \item Easily interpretable
 \item Easily deals with different types of inputs
 \item Handles features of different priorities well
 \item Easily scalable for large datasets
\end{itemize}
Cons:
\begin{itemize}
 \item Tends to overfit
 \item Fully deterministic, making them bad at bagging (see later section).
 \item[\vspace{-1em}]
 \item[\vspace{-1em}]
 \item[\vspace{-1em}]
\end{itemize}
\end{multicols}
\chapter{Ensembles}
\section{Bagging}
Idea: If we have many models with low biases but a high variance, and they have independent outputs, then by the law of large numbers we can get closer to the correct value by averaging the prediction of these different models.
\newpar
Since we only have one big dataset $D$, our models aren't actually independent in practice. We try to get close by creating many different smaller datasets $D_i$ from $D$ by \tbf{sampling with replacement}, i.e. we allow a dataset to draw the same element multiple times, and then training our models on those datasets. This is known as \tbf{Bootstrap Aggregation}.
\newpar
The output of our bagged model is then simply the mean of the ouputs of the individual models:
\begin{align}
 \hat{f}(x) = \frac{1}{K} \sum_{k=1}^K \hat{f}^k(x)
\end{align}
\newpar
The models still have to be \tbf{uncorrelated} to each other, i.e. their covariances have to be $0$, otherwise this process doesn't work. To be precise, the expected squared error of an ensemble of $K$ models with a shared variance $v$ and a covariamce $c$ is:
\begin{align}
 \epsilon = E\left[\left(\frac{1}{K}\sum_{k=1}^k \epsilon_k\right)^2\right] = \frac{1}{K^2}E\left[\left(\sum_{k=1}^k \epsilon_k^2 + \sum_{k \neq l} \epsilon_k \epsilon_l\right)\right] = \frac{1}{K}v + \frac{K-1}{K}c
\end{align}
\tbf{Trees} are low bias, but often end up being correlated. We can decrease their correlation by randomizing which feature we decide to split on at each point. Bagged Trees that split on random features are known as \tbf{Random Forests}. Generally, random forests don't split on entirely random features, instead they pick $m$ features randomly out of the full set of $M$ features and then pick the best split among the chosen features.
\newpar
Bagged ensembles provide an estimate of the test error via the out-of-bag training error, i.e. the output we get from the bagged model if we only take the output $f_k(x_i)$ of each submodel $k$ if $x_i$ wasn't part of its training data $D_k$:
\begin{align}
 \epsilon_{OOB}& = \frac{1}{|D|} \sum_{i=1}^{|D|} \loss\left(y_i, \frac{1}{S(x_i)} \sum_{k \in S(x_i)}f_k(x_i)\right)\\
 S(X) &= \{k \in \{1, \hdots, K\} \mid (x,y) \notin D_{k}\}
\end{align}
The variance of an ensemble is:
\begin{align}
 \sigma(x) = \sqrt{\frac{1}{K-1}\sum_{k=1}^K \left(\hat{f}_k(x) -\hat{f}(x)\right)^2}
\end{align}
%
\section{Boosting}
Boosting creates a low-bias model out of an esemble of high-bias models, by training each model to correct the error of the last model. The final ensemble $F_K$ is a weighted sum of a set of models $f_k$:
\begin{align}
 F_K(x) = \sum_{k=1}^K \alpha f_k(x)
\end{align}
We train each model sequentially while keeping the parameters of past models fixed:
\begin{align}
 \theta_{k+1} = \argmin{\theta}\sum_{n=1}^N \loss(y_n, F_k(x_n; \theta_{fixed}) + \alpha f(x_n; \theta))
\end{align}
\subsection{Gradient Boosting}
\begin{enumerate}
 \item Initialize model with a constant value:
 \begin{align}
  F_0(x) = \argmin{\alpha_0} \sum_{i=1}^n \loss(y_i,\alpha_0)
 \end{align}
 In the lecture, this calculation was skipped and 0 was always used instead.
 \item Let $M$ be an arbitrary number of maximum iterations. For each $m = 1, \hdots, M$:
 \begin{enumerate}
 \item Compute \textit{pseudo-residuals}, i.e. the gradient of the loss function by the ensemble output:
 \begin{align}
  z_m = \pd{\loss(y_n, F_k(x_n))}{F_k (x_n)}
 \end{align}
 \item Train model $f_m$ on the set $\{(x_i,r_i)\}^n_{i=1}$
 \item Compute the best possible $\alpha_n$, i.e. $\alpha_m = \argmin{\alpha} \sum_{i=1}^n \loss(y_i, F_{m-1} + \alpha f_m(x_i))$
  \item Update the ensemble model: $F_k (x) = F_{m-1}(x) + \alpha_m f_m(x)$
 \end{enumerate}
\item Output the finished ensemble model $F_M$.
\end{enumerate}
\subsection{AdaBoost}
AdaBoost is a special case of Gradient Boosting with a particular loss function that results in
weighting points misclassified by previous models higher than correctly classified ones.
\newpar
Specifically, AdaBoost uses an approximation of \tbf{instance-penalty loss}:
\begin{align}
 \loss(y_i, f(x_i)) &= e^{-y_i f(x_i)}\\
 &\approx \frac{\sum_{i=1}^N w^{(k)}_i \cdot \mathbbm{1}(y_i \neq f(x_i))}{\sum_{i=1}^N w^{(k)}_i}\\ &:= \epsilon
\end{align}
In natural language, this just means dividing the sum of the weights of incorrectly classified data points by the total sum of the weights.
\newpar
The model weights are then given by:
\begin{align}
 \alpha_k = ln\left(\frac{1 - \epsilon_k}{\epsilon_k}\right)
\end{align}
The feature weights $w_i$ are initialized as $\frac{1}{N}$, i.e. all features are weighted equally, and then updated according to the following rule:
\begin{align}
w_i^{(k+1)} = w_i^{(k)} e^{(\alpha_k \cdot \mathbbm{1}(y_i \neq f(x_i))}
\end{align}
This means that the weights of correctly classified features stay the same, while the weights of incorrectly classified features increase by a factor of $\frac{1 - \epsilon_k}{\epsilon_k}$.
%
\section{Gradient-Boosted Decision Trees}
Tree overfitting happens if there are too many leaves (coresponding to too many steps in the final function) or if the leaves' output values are too high (correspnding to large jumps in the final function). A common regularization function for trees is thus:
\begin{align}
 \Omega(f) = \gamma T + \frac{\lambda}{2}\sum_{j=1}^T w_j^2
\end{align}
Where $T$ denotes the number of leaves.
\newpar
For Gradient Boosted Trees, the function we want to minimize at each step is:
\begin{align}
 J(y_i, f^{(k)}) = \left(\sum_{n=1}^N \loss_n^{k-1} G_n f^{(k)}(x_n) + \frac{1}{2} H_n (f^{(k)}(x_n))^2\right) + \Omega(f^{(k)})
\end{align}
i.e. we approximate $\loss_n^{(k)}$ via a Taylor expansion:
\begin{align}
 \loss_n^{(k)} \approx \loss_n^{(k-1)} +  G_n^{(k)}f^{k}(x_n) + \frac{1}{2}H_n(f^{(k)}(x_n))^2
\end{align}
Minimizing this loss function corresponds to minimizing the following Objective Function $O_j$ for each Leaf $j$:
\begin{align}
 O_j = - \frac{1}{2}\frac{\left(\sum_{n=1}^N G_n\right)^2}{\left(\lambda + \sum_{n=1}^N H_n\right)} + \gamma
\end{align}
Where $G_n$ is the gradient of the loss function over the model output and $H_n$ is the Hessian matrix, i.e. the multidimensional equivalent of the second derivative.
\newpar
If we let $I_J$ denotes the indices of all data points in the region $R_J$ belonging to leaf $J$, the optimal leaf weights are then given by:
\begin{align}
 w^* = - \frac{\sum_{n=1}^N G_n}{\lambda + \sum_{n=1}^N H_n}
\end{align}

\newpar
The gain of splitting a leaf is simply:
\begin{align}
 Gain_j = O_j - O_L - O_R
\end{align}
just like the information gain of regular trees.
\newpar
We can decide to stop splitting further if $Gain_j \leq 0$. This means that $\gamma$ directly controls the minimum gain needed for the model to perform a split.
\newpar
The most common loss function for classification is logistic loss, so here are the gradient and hessian pre-calculated:
\begin{align}
 \loss_n &= -y_n \ln(\sigma(\yhat_n)) - (1-y_n)\ln(1-\sigma(\yhat_n))\\
 G_n &= \pd{\loss_n}{\yhat_n} = \sigma(\yhat_n) - y_n\\
 H_n &= \pd{^2 \loss_n}{^2\yhat_n} = \sigma(\yhat_n)(1 - \sigma(\yhat_n))
\end{align}
Remember that $\yhat_n^{(k)} = \yhat_n^{(k-1)} + f^{(k)}(x_n)$. (Or $\yhat_n^{(k)} = \yhat_n^{(k-1)} + \alpha f^{(k)}(x_n)$ if we want to do a weighted sum). In this lecture, we assume $\yhat^{(0)} = 0$.
\newpar
GBDTs have very low biases while also retaining a relatively low variance. They are also very fast to train, requiring a runtime of $O(kMN \log(T))$, where $k$ is the number of trees in the ensemble, $M$ is the number of features and $N$ is the number of data points.
\end{document}
