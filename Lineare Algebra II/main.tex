\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz-cd} % commutative diagrams
\usepackage{physics} % \abs and \norm macros

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\ve}{\vec{e}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{0}}

\newcommand{\Mat}[3]{\text{Mat}^{#1}_{#2}\left(#3\right)}
\newcommand{\scalar}[2]{\langle #1, #2 \rangle}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Korollar}



\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{beispiel}[lemma]{Beispiel}
\newtheorem{beobachtung}[lemma]{Beobachtung}
\newtheorem{anmerkung}[lemma]{Anmerkung}
\newtheorem{question}[lemma]{Frage}
\newtheorem{application}[lemma]{Anwendung}
\newtheorem{konsequenz}[lemma]{Konsequenz}
%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
\chapter{Wiederholung}
\section{Notation}
\begin{itemize}
 \item Die Menge aller $n \times m$ Matrizen mit Einträgen aus dem Körper $k$ schreiben wir $Mat(n \times m, k)$
 \item Die Gruppe der Invertierbaren $n \times n$-Matrizen mit Einträgen aus $k$ schreiben wir $Gl_n(k)$
 \item Den Eigenraum einer Abbildung $f$ zum Eigenwert $\lambda$ schreiben wir $Eig_f(\lambda)$
 \item Die $n \times n$-Einheitsmatrix schreiben wir $I_n$ oder $E_n$, die Identitätsabbildung des Vektorraums $V$ schreiben wir $id_V$. Vermutlich werden wir die Unterscheidung zwischen den beiden Begriffen jedoch öfters ignorieren.
 \item Das characteristische Polynom eines Endomorphismus $f$ ist
 \begin{equation}
  \chi_f := det(f - t \cdot id_V).
 \end{equation}
 Analog ist das characteristische Polynom einer $n \times n$-Matrix $A$
 \begin{equation}
  \chi_A := det(A - t \cdot I_n).
 \end{equation}
\end{itemize}
%
%
\section{Determinanten}
\subsection{Axiomatische Beschreibung}
\begin{enumerate}
 \item Die Determinante ist \textbf{multilinear}, also:
 \begin{enumerate}
  \item $det(v_1 + w, v_2, \hdots, v_n) = det(v_1, \hdots, v_n) + det(w, v_2 \hdots, v_n)$, ebenso in den anderen Spalten und für Zeilenvektoren.
  \item $det(cv_1 + w, v_2, \hdots, v_n) = c \cdot det(v_1, \hdots, v_n)$, ebenso in anderen Spalten und für Zeilenvektoren
 \end{enumerate}
 \item Die Determinante ist \textbf{alternierend}: Sind zwei Spalten oder Zeilen gleich, ist die Determinante $0$.
 \item Sie ist \textbf{normiert} durch $det(I_n) := 1$
\end{enumerate}
%
%
\subsection{Weitere Eigenschaften}
\begin{enumerate}

 \item $det(A^\top) = det(A)$
 \item $det(A^{-1}) = \frac{1}{det(A)}$
 \item Für quadratische $A,B$ gleicher Größe gilt $det(AB) = det(A)det(B)$
 \item Für Konstante $c$ und $n \times n$-Matrix $A$ gilt $det(cA) = c^n det(A)$
 \item Für Dreiecksmatrizen $A$ gilt $det(A) = a_{11}a_{22}\hdots a_{nn}$

 \item Besteht eine Spalte oder Zeile aus Nullen, ist die Determinante $0$.
 \item Vertauscht man zwei Spalten oder Zeilen, ändert die Determinante ihr Vorzeichen. Dies ist äquivalent zu ``alternierend'' für Körper ohne selbstinverse Elemente $x = -x \neq 0$.
 \item Addition eines Vielfachen einer Zeile/Spalte zu einer anderen ändert die Determinante nicht.
\end{enumerate}

%
%
\subsection{Berechnung}
Die Determinante einer $2 \times 2$-Matrix ist:
\begin{align*}
 det \begin{pmatrix}
      a & b\\
      c & d
     \end{pmatrix}
    = ad - bc.
\end{align*}
Die Determinante einer $3 \times 3$-Matrix ist:
\begin{align*}
det \begin{pmatrix}
     a & b & c\\
     d & e & f\\
     g & h & i\\
    \end{pmatrix}
    = aei + bfg + cdh - ceg - bdi - afh
\end{align*}
%
%
\section{Basiswechsel}
Zu Beginn will ich einige relevante Sätze, Definitionen und Notationsstandards aus der Vorlesung ``Lineare Algebra I'' wiederholen.
\begin{definition}
Sei $f: V \to W$ eine lineare Abbildung. Sei $A = \{a_1, \hdots, a_n\}$ eine Basis von $V$ und $B = \{b_1, \hdots, b_m\}$ eine Basis von $W$. So lässt sich jeder Vektor $w \in W$ darstellen als \textbf{endliche} Linearkombination der Basisvektoren:
\begin{equation}
    w = \sum_{i=1}^{m} \alpha_i b_i
\end{equation}
Insbesondere lassen sich die Bilder der Basisvektoren $a_j \in A$ in dieser Form darstellen:
\begin{equation}
    f(a_j) = \sum_{i=1}^{m} \alpha_{ij} b_i
\end{equation}
Die \textbf{Darstellungsmatrix} $\text{Mat}_B^A f$ ist genau durch diese Koeffizienten $\alpha_{ij}$ gegeben.
\begin{equation}
    \text{Mat}_B^A f = 
    \begin{pmatrix}
       \alpha_{11} & \hdots & \alpha_{1n}\\
       \vdots      &        & \vdots\\
       \alpha_{m1} & \hdots & \alpha_{mn}\\
    \end{pmatrix}
\end{equation}
\end{definition}
\newpar
In der Regel arbeiten wir mit der Standardbasis $E = \{e_1, \hdots, e_n\}$ und interpretieren jede Matrix $M$ als Darstellungsmatrix $M_E^E f$ einer Linearen Abbildung $f$. 
\begin{definition}
Die \textbf{Basiswechselmatrix} $T_B^A$ ist die Abbildungsmatrix der Identitätsabbildung.
\begin{equation}
 T_B^A = \text{Mat}_B^A (\text{id}_V)
\end{equation}
\end{definition}
\begin{theorem}
Für jede Basis $B$ eines beliebigen Vektorraums $V$ gilt 
\begin{align}
T_B^B = I_n = \begin{pmatrix}
                1 & \hdots & 0\\
                \vdots & \ddots & \vdots\\
                0 & \hdots & 1\\
               \end{pmatrix}
\end{align}
\end{theorem}
%
%
%
\begin{proof}
Da die Darstellung jedes Vektors durch die Basisvektoren eindeutig gegeben ist, ist die Darstellung $id_V(b_j) = b_j = \sum_{i=1}^{m} \alpha_{ij} b_i$ eines Basisvektors als Linearkombination genau gegeben durch die Linearkombination mit $\alpha_{ij} = 1$ und $\alpha_{ik} = 0$ für $k \neq j$. Dies entspricht genau dem Standardbasisvektor $e_j$. Also gilt $T_B^B = (e_1 \hdots e_n) = I_n$.
\end{proof}
%
%
%
\begin{proposition}
\label{prop:basiswechsel}
Gegeben $\text{Mat}_B^A f$ lässt sich die Abbildungsmatrix $\text{Mat}_D^C f$ von $f$ bezüglich zweier neuen Basen $C$ und $D$ durch Nutzung von Basiswechselmatrizen folgendermaßen berechnen:
\begin{equation}
 \text{Mat}_D^C(f) = T_D^B \cdot \text{Mat}_B^A f \cdot T_C^B
\end{equation}
Ein besonders relevanter Spezialfall ist:
\begin{equation}
 \text{Mat}_B^B(f) = T_B^A \cdot \text{Mat}_A^A f \cdot T_A^B
\end{equation}
\end{proposition}
%
\newpar
%
\begin{theorem}
\label{satz:basiswechsel_inverse}
Es gilt $T_B^A$ = $(T_A^B)^{-1}$
\end{theorem}
\begin{proof}
\begin{align*}
T_A^B \cdot T_B^A &= T_A^B \cdot id_V \cdot T_B^A\\
                  &=\text{Mat}_A^B (\text{id}_V) \cdot \text{Mat}_B^B (id_V) \cdot \text{Mat}_B^A (\text{id}_V)\\
                  &= \text{Mat}_A^A (\text{id}_V)\\
                  &= \text{id}_V\\
\end{align*}
\end{proof}
%
\newpar
%
\begin{anmerkung}
Betrachten wir eine beliebige Basis $B = \{b_1, \hdots, b_n\}$. Die Matrix $T_B^E$ kann man trivial finden, da jeder Vektor $b_i = (b_{i1} \hdots b_{in})^T$ bezüglich der Standardbasis trivial geschrieben ist als $b_i = \sum_{i=1}^n b_{ij} \cdot e_i$. Nach \ref{satz:basiswechsel_inverse} lässt sich die Matrix $T_E^B$ ebenfalls ohne größere Probleme durch invertierung von $T_B^E$ finden.
\end{anmerkung}
%
\begin{definition}
 Wir nennen zwei Matrizen $A$ und $B$ \textbf{ähnlich}, falls eine Matrix $S$ existiert, sodass
 \begin{align*}
  A = S^{-1} B S
 \end{align*}
\end{definition}
%
\newpar
%
Ein zentrales Ziel der Vorlesung ``Lineare Algebra II'' ist es, zu einer Matrix $A$ eine besonders simple Matrix $B$ zu finden, welche $A$ ähnlich ist. Hierbei sollte man immer \ref{prop:basiswechsel} im Kopf behalten - zwei Matrizen $A$ und $B$ sind genau dann ähnlich, wenn sie die Darstellungsmatrizen der gleichen Funktion zu verschiedenen Basen sind, also wenn Basen $B_1$ und $B_2$ und ein Endomorphismus $f$ existieren, sodass
\begin{align*}
 A = Mat_{B_1}^{B_1}(f) \text{ und } B = Mat_{B_2}^{B_2}(f)
\end{align*}
%
%
%
\chapter{Die Jordansche Normalform}
Let $V$ be a vector space. Let $f: V \to V$ be a nilpotent endomorphism.
\newpar
Define
$
V^p = \ker(f^p)
$
Where $f^p$ denotes composition.
\begin{lemma}
\begin{equation*}
 V^1 \subseteq V^2 \subseteq \hdots \subseteq V
\end{equation*}

\end{lemma}
\begin{lemma}
\begin{equation*}
 \forall p : \forall v \in V : v \in V^p \Leftrightarrow f(v) \in V^{p-1}
\end{equation*}
\end{lemma}


\begin{theorem}
The \textbf{``natural mapping''}
\begin{align*}
 \overline{f}: V^p / V^{p-1} \to V^{p-1} / V^{p-2}
\end{align*}
is injective.
\end{theorem}
\begin{proof}
We know that the restriction$f|_{V^p}$ is a map $V^p \to V^{p-1}$.
\newpar
Let $q_p$ denote the quotient map $V^p \to V^p/V^{p-1}$.
\newpar
Let $q_{p-1}$ denote the quotient map $V^{p-1} \to V^{p-1}/V^{p-2}$.
\newpar
We have the following diagram:
\begin{figure}[h!]
\centering
\begin{tikzcd}[row sep = huge, column sep = huge]
V^p
\arrow[r, "f|_{V^p}"]
\arrow[d, "q_p"]
& V^{p-1}
\arrow[d, "q_{p-1}"] \\
V^p/V^{p-1}
\arrow[r, "\overline{f}"]
&  V^{p-1}/V^{p-2}
\end{tikzcd}
\end{figure}
\newpar
By \textbf{``the universal property''} we have that a unique $\overline{f}$ exists iff. $V^{p-1} \subseteq \ker(q_{p-1} \circ f|_{V^p})$. This inclusion holds, since for $v \in V^{p-1}$ we have $f(v) \in V^{p-2}$, which is exactly the kernel of $q_{p-1} V^{p-1} \to V^{p-1}/V^{p-2}$. Therefore the diagram commutes.
\newpar
We will show injectivity of $\overline{f}$ by showing $\ker(f) = \{0\}$. Let $k \in V^p / V^{p-1}$. Pick an arbitrary representative $v \in k$. Because the diagram commutes, we have
\begin{align*}
 k \in \ker f \Leftrightarrow f | V_p (v) \in \ker(q_{p-1}) \Leftrightarrow f(v) \in V^{p-2} \Leftrightarrow v \in V^{p-1}
\end{align*}
Since $k \in V^p / V^{p-1}$, we have that $v \in V^{p-1}$ must be in the same equivalence class as $0$. Therefore the kernel of $\overline{f}$ consists only of the equivalence class of the zero vector (which *is* the zero vector of the quotient space).
\end{proof}
\newpage
We have the following diagram:
\begin{figure}[h!]
\centering
\begin{tikzcd}[row sep = huge, column sep = huge]
V
\arrow[d, "q"]
& F(V) = V^p
\arrow[r, "\eta_V = f|_{V^p}"]
\arrow[d, "F(q) = q_p"]
& G(V) = V^{p-1}
\arrow[d, "G(q) = q_{p-1}"] \\
V/ker(f)
&  F(V/ker(f)) = V^p/V^{p-1}
\arrow[r, "\eta_{V/ker(f)} = \overline{f}"]
&  G(V/ker(f)) = V^{p-1}/V^{p-2}
\end{tikzcd}
\end{figure}
\section{Erste Anwendungen}
\textbf{Erinnerung:} Über $\bC$ ist jede Matrix ähnlich zu einer Jordan-Matrix.
\newpar
\underline{Erste Anwendungen:}
\begin{enumerate}
 \item Hohe Potenzen von Endomorphismen ausrechnen.
 \item Die Exponentialfunktion mit Matrixwertigen Argumenten ausrechnen und damit lineare Differentialgleichungen lösen, auch in vielen Veränderlichen und mit höheren Ableitungen.
\end{enumerate}
\section{Konjugationsklassen}
\newpar
Frage: Ist in unserer Situation die Jordan-Matrix von $f$ eindeutig?
\newpar
Antwort: Nein! Blöcke können durch Umsortierung der Basis vertauscht werden.
\begin{align*}
 V = \bC^2, \vv_1, \vv_2 \text{ EW zu } \lambda_1 \neq \lambda_2\\
\end{align*}
\begin{align*}
 B &= (\vv_1, \vv_2)\\
 Mat_B^B f &= \begin{pmatrix}
              \lambda_1 & 0\\
              0 & \lambda_2\\
             \end{pmatrix}\\
 B' &= (\vv_2, \vv_1)\\
 Mat_{B'}^{B'} f &= \begin{pmatrix}
              \lambda_2 & 0\\
              0 & \lambda_1\\
             \end{pmatrix}\\
\end{align*}
$\implies$ Neue Frage: Ist die Jordan-Matrix eindeutig bis auf Umsortierung der Blöcke?
\newpar
Versuch einer Antwort: Sei $B$ eine Jordan-Basis,
\begin{align*}
 Mat_B^B (f) = \begin{pmatrix}
                Jor(\lambda_1, n_1) & 0 & 0\\
                0 & \ddots & 0\\
                0 & 0 & Jor(\lambda_r, n_r)\\
               \end{pmatrix}
\end{align*}
Beachte: Die Skalare $\lambda_1, \hdots, \lambda_r$ sind genau die Eigenwerte von $f$ und müssen deshalb in jeder Jordan-Matrix von $f$ auftreten.
\newpar
\underline{Beachte:} Wenn $\lambda$ ein Eigenwert von $f$ ist, seien $i_1, \hdots, i_k$ die Indizes mit $\lambda_i = \lambda$. Die Summe der indizes ist die Dimension des Hauptraums von $f$ zu $\lambda$, muss also für jede Jordan-Matrix von $f$ gleich sein.
\begin{align*}
 \sum_{j=1}^k n_{i_j} = \dim Hau_f (\lambda) = \text{ alg. Mult. von $f$}
\end{align*}
Die rechte Seite hängt nicht von $B$ ab, ist also eine Invariante der Jordan-Matrizen von $f$. Außerdem: Wenn $f$ auf den Hauptraum $Hau_f(\lambda)$ eingeschränkt wird und wir die Räume $V^p$ und die Partition $m_p$ betrachten, sind die Zahlen $n_{i}$ genau die Zahlen, die in der zu $m_p$ dualen Partition auftreten (wenn auch in willkürlicher Reihenfolge)
\newpar
$\implies$ \ul{Ergebnis:} Ja! Die Jordanmatrix ist eindeutig bis auf Umsortieren der Blöcke.
\newpar
\begin{definition}
 Die Ähnlichkeitsrelation zwischen Matrizen wird auch als \tbf{Konjugation} (oder genauer als \tbf{Konjugationswirkung von $Gl_n$ auf eine Matrix}) bezeichnet.
\end{definition}
\begin{definition}
 Als \tbf{Orbit (der Konjugationswirkung von $Gl_n$ auf $M$)} einer Matrix $M$ bezeichnen wir die Menge der Matrizen, die zu einer gegebenen Matrix $M$ ähnlich sind.
\end{definition}
\newpar
\ul{Beobachtung:} Jede Matrix ist einer Jordanmatrix ähnlich, also liegt in jedem Orbit mindestens eine Jordan-Matrix!
\newpar
Wären Jordan-Matrizen eindeutig, hätten wir nun eine triviale Bijektion zwischen der Menge aller Orbiten von Matrizen und den Jordanformen. Da Jordanmatrizen nur bis auf Umsortieren eindeutig ist, ist die Situation etwas komplizierter (aber nicht viel komplizierter!)
\newpar
\begin{theorem}
 Sei $B$ eine Jordan-Basis für $f$ mit Matrix $M_f = Mat_B^B (f)$. Sei $A \in Mat(n \times n)$ eine Matrix, die sich von $M_f$ nur durch Umsortierung von Blöcken unterscheidet. Dann gibt es eine Umsortierung $B'$ von $B$, sodass $Mat_{B'}^{B'} (f) = A$.
\end{theorem}
\newpar
 \ul{Äquivalent:} Wenn sich Matrizen in Jordanform nur durch Umsortierung der Blöcke unterscheiden, sind sie ähnlich.
\newpar
 \ul{Ebenfalls Äquivalent:} Wenn ein Orbit eine Jordan-Matrix $A$ enthält, so auch alle Jordan-Matrizen, welche aus $A$ durch Umsortierung der Blöcke entstehen.
\newpar
\begin{theorem}
 Wenn $B_1$, $B_2$ Jordanbasen von $f$ sind, dann unterscheiden sich die Darstellungsmatrizen $\Mat{B_1}{B_1}{f}$ und $\Mat{B_2}{B_2}{f}$ von $f$ zu den beiden Basen nur durch Umsortierung der Blöcke.
\end{theorem}
\newpar
\ul{Äquivalent:} Zwei Jordanmatrizen im selben Orbit unterscheiden sich nur durch Umsortieren der Blöcke.
\newpar
\ul{Zusammenfassung:} Es existiert eine Bijektion zwischen den Orbiten der Konjugationswirkung und den Äquivalenzklassen von Jordanmatrizen unter Umsortierung.
\section{Der Satz von Cayley-Hamilton}
Seien $V$ ein Vektorraum und $f \in End(V)$ ein Endomorphismus. Wir betrachteten bisher Ausdrücke wie $f$, $f^2$, $f^3$, $f^5 - \lambda \cdot Id_V$ (äquivalent zu $f^5 - \lambda f^0$). Diese sehen aus wie Polynome. Können wir diese Idee formalisieren?
\begin{definition}
 Gegeben ein Körper $k$ schreiben wir für die Menge der Polynome mit Variable $t$ und Koeffizienten aus $k$:
 \begin{align*}
  k[t] = \left\{\sum_{i=0}^m a_i t^i \mid i \in \bN, a_i \in k\right\}
 \end{align*}
\end{definition}
\begin{anmerkung}
Gegeben ein Polynom $p \in k[t]$ induziert dieses eine Funktion 
\begin{align*}
 k &\to k\\
 x &\to \sum a_i x^i\\
\end{align*}
Sei $\phi$ die Funktion, welche jedem Polynom die durch das Polynom induzierte Abbildung zuordnet:
\begin{align*}
 \phi : k[t] \to (t \to k)
\end{align*}
Im Allgemeinen ist $\phi$ nicht injektiv! Sei zum Beispiel $k = F_2$, dann induzieren die Polynome $f, f^2, f_3, \hdots$ alle die selbe Funktion. Polynome sind also von den von ihnen induzierten Funktionen zu unterscheiden. 
\end{anmerkung}
\begin{definition}
 Wir Betrachten die Auswertung eines Polynoms in $f$:
 \begin{align*}
 \begin{array}{ccccc}
  \phi_f & : & k[t] &\to& End(V)\\
       & &\sum a_i t^i &\to& \sum a_i \cdot f^i
 \end{array}
 \end{align*}
 Analog definieren wir die Auswertung für Matrizen $A$.
\end{definition}
\begin{beobachtung}
 Seien $A$, $B$ ähnlich mit $A = S B S^{-1}$. Dann gilt $A^n = (S B S^{-1})^n = S B^n S^{-1}$, also sind $A^n$ und $B^n$ ebenfalls ähnlich. Also gilt für jedes Polynom:
 \begin{align*}
  p = \sum a_i t^i
 \end{align*}
 \begin{align*}
  \phi_A(p) &= \sum a_i A^i\\ &= \sum a_i S B^i S^{-1}\\ &= S \left(\sum a_i B^i\right) S^{-1}\\ &= S \phi_B(p) S^{-1}
 \end{align*}
 Also bleibt Ähnlichkeit unter der Anwendung von Polynomen erhalten.
\end{beobachtung}
\begin{beobachtung}
 Betrachte $f \in End(V)$. Wähle Basis $B$ von $V$. Dann ist
 \begin{align*}
  \Mat{B}{B}{f^i} = \left[\Mat{B}{B}{f}\right]^i
 \end{align*}
 Also gilt für jedes Polynom $p = \sum a_i t^i$:
 \begin{align*}
  \Mat{B}{B}{\phi_f(p)} &= \Mat{B}{B}{\sum a_i f^i}\\ 
                        &= \sum a_i \Mat{B}{B}{f^i}\\ 
                        &= \sum a_i \left[\Mat{B}{B}{f}\right]^i\\
                        &= \phi_{\Mat{B}{B}{f}}(p)
 \end{align*}
\end{beobachtung}
\begin{anmerkung}
 Die Abbildungen $\phi_f$, $\phi_A$ werden auch \tbf{Einsetzungsabbildungen} genannt. Statt $\phi_f(p)$ und $\phi_A(p)$ schreibt man oft $p(f)$ und $p(A)$.
\end{anmerkung}
\begin{theorem}
 \emph{\tbf{Cayley-Hamilton}:} Sei $\chi_f \in k[t]$ das charakteristische Polynom von $f$. Dann ist
 \begin{align*}
  \chi_f(f) = 0.
 \end{align*}
\end{theorem}
\begin{proof}
 (Nur für $k = \bC$). Sei $B$ eine Jordan-Basis. Dann 
 \begin{align*}
  \Mat{B}{B}{\chi_f(f)} = \chi_f \left(\Mat{B}{B}{f}\right).
 \end{align*}
 Es genügt also 
 $
  \chi_f \left(\Mat{B}{B}{f}\right) = 0
 $
 zu zeigen.
 \newpar
 Schreibe
 \begin{align*}
  \Mat{B}{B}{f} = \begin{pmatrix}
                   Jor(\lambda_1, n_1) & 0 & 0\\
                   0 & \ddots & 0\\
                   0 & 0 & Jor(\lambda_r, n_r)\\
                  \end{pmatrix}
 \end{align*}
 Es gilt
 \begin{align*}
  \chi_f(t) = (t - \lambda_1)^{n_1} \cdot \hdots \cdot  (t - \lambda_1)^{n_r}
 \end{align*}
 Wegen der Blockgestalt gilt:
 \begin{align*}
  \chi_f(\Mat{B}{B}{f}) = \begin{pmatrix}
                   \chi_f(Jor(\lambda_1, n)) & 0 & 0\\
                   0 & \ddots & 0\\
                   0 & 0 & \chi_f(Jor(\lambda_r, n))\\
                  \end{pmatrix}
 \end{align*}
 Wir wissen, dass jeder Jordanblock $Jor(\lambda_i - \lambda_i, n_i$ nilpotent mit Index $n_i$ ist. Also hat das charakteristische Polynom Nullfaktoren, also ist $\chi_f \left(\Mat{B}{B}{f}\right) = 0$.
\end{proof}
%
%
%
%
%
\section{Minimalpolynome}
\begin{definition}
Wir nennen ein Polynom $p$ das Minimalpolynom eines Endomorphismus $f$, wenn gilt:
\begin{enumerate}
 \item $p$ ist nicht das Nullpolynom
 \item $p(f) = 0$
 \item Der Grad von $p$ ist minimal unter allen Polynomen mit $f$ als Nullstelle \label{minpoly-3}
 \item Der Leitkoeffizient ist $1$ ($p$ ist normiert) \label{minpoly-4}
\end{enumerate}
Analog für quadratische Matrizen statt Endomorphismen.
\end{definition}
\newpar
Normierung wird einfach durch Skalarmultiplikation des Polynoms erreicht, ist also keine starke Bedingung.
\begin{theorem}
 Ähnliche Matrizen haben das selbe Minimalpolynom.
\end{theorem}
\begin{proof}
 \begin{align*}
  A &= S B S^{-1}\\
  \implies A^n &= (S B S^{-1})^n = S B^n S^{-1}\\
  \implies p(A) &= p(S B S^{-1}) = S \cdot  p(B) \cdot S^{-1} := 0 \implies p(B) = 0\\
 \end{align*}
\end{proof}
\begin{theorem}
 $f$ hat das selbe Minimalpolynom wie seine darstellenden Matrizen.
\end{theorem}
\begin{theorem}
 Minimalpolynome existieren.
\end{theorem}
\begin{proof}
 Gemäß Satz von Cayley-Hamilton gibt es mindestens ein Polynom, welches nicht das Nullpolynom ist und $f$ als Nullstelle hat. Also finde nichttriviales Polynom $p$ von minimalem Grad finden, welches $f$ als Nullstelle hat \footnote{Die Menge der Grade von Polynomen mit $f$ als Nullstelle ist eine nichtleere Teilmenge der Natürlichen Zahlen, hat also ein Minimum.}. Sei $a$ der Leitkoeffizient von $p$. Dann ist $a^{-1}p$ ein normiertes Polynom vom selben Grad mit $f$ als Nullstelle.
\end{proof}
\begin{theorem}
Minimalpolynome sind eindeutig.
\end{theorem}
\begin{proof}
 Seien $p_1, p_2 \in k[t]$ zwei Minimalpolynome von $f$. Wir wissen, dass die Grade gleich sein müssen (\ref{minpoly-3}) und dass die Leitkoeffizienten $1$ sein müssen (\ref{minpoly-4}). Wir haben:
 \begin{align*}
  p_1(t) &= t^n + \sum^{n-1}_{i=0} a_i t^i\\
  p_2(t) &= t^n + \sum^{n-1}_{i=0} b_i t^i\\
 \end{align*}
 \begin{align*}
  \implies (p_1 - p_2)(t) = \sum^{n-1}_{i=0} (a_i - b_i) t^i\\
 \end{align*}
 Der Grad des Differenzpolynoms muss kleiner als $n$ sein, sonst wären $p_1$ und $p_2$ nicht minimal. Gleichzeitig gilt:
 \begin{align*}
  (p_1 - p_2)(f) = p_1(f) - p_2(f) = 0 - 0 = 0
 \end{align*}
 Also muss $p_1 - p_2$ das Nullpolynom sein, also ist $p_1 = p_2$.
\end{proof}
\begin{question}
 Wie viele Polynome gibt es überhaupt, die $f$ als Nullstelle haben?
\end{question}
\newpar
\ul{Antwort 1:} Viele! Wenn $p$ ein Polynom ist mit $p(f) = 0$, so hat jedes Vielfache von $p$ unter Polynommultiplikation ebenfalls $f$ als Nullstelle.
\newpar
\ul{Antwort 2:}
\begin{theorem}
 Alle Polynome, die $f$ als Nullstelle haben, sind Vielfache (unter Polynommultiplikation) des Minimalpolynoms. Wenn $p$ das Minimalpolynom von $f$ ist und $q \in k[t]$ ein beliebiges nichttriviales Polynom mit $q(f) = 0$, dann existiert ein $r \in k[t]$, sodass $q = r \cdot p$.
\end{theorem}
\begin{proof}
Sei $q$ gegeben. Polynomdivision mit Rest liefert Polynome $r_1, r_2 \in k[t]$ sd. $\deg r_2 < \deg p$ und 
\begin{align*}
 q = r_1 \cdot p + r_2
\end{align*}
Es gilt nun:
\begin{align*}
 q(f) &= r_1(f)p(f) + r_2(f)\\
 \implies 0 &= r_1(f) \cdot 0 + r_2(f)\\
 \implies r_2(f) &= 0\\
\end{align*}
Da $\deg r_2 < \deg p$ und $p$ ein Minimalpolynom muss $r_2$ also das Nullpolynom sein. Der Beweis für Matrizen funktioniert identisch.
\end{proof}
\begin{question}
 Wie findet man das Minimalpolynom für ein gegebenes $f$?
\end{question}
\newpar
\ul{Antwort}: Schwierig, es sei denn, man kennt die Jordanform!
\begin{proposition}
 Sei $A$ eine quadratische Matrix. Sei $\lambda \in k$. Die folgenden Aussagen sind äquivalent:
 \begin{enumerate}
  \item $\lambda$ ist ein Eigenwert von $A$.
  \item $\lambda$ ist eine Nullstelle des charakteristischen Polynoms.
  \item $\lambda$ ist eine Nullstelle des Minimalpolynoms.
 \end{enumerate}
\end{proposition}
\begin{proof}
\newpar
 \begin{itemize}
  \item[$1 \Leftrightarrow 2$:] Die Äquivalenz der ersten beiden Aussagen ist bereits bekannt.
  \item[$3 \Rightarrow 2$:] Das charakteristische Polynom ist ein Vielfaches des Minimalpolynoms.
  \item[$1 \Rightarrow 3$:] Sei ein Eigenwert $\lambda$ gegeben. Per Annahme existiert eine zu $A$ ähnliche Matrix $B$ der Form:
  \begin{align*}
   B = \begin{pmatrix}
        \lambda & *\\
        0 & *\\
       \end{pmatrix}
  \end{align*}
  Wenn $p$ ein beliebiges Polynom ist, dann ist:
  \begin{align*}
   p(B) = \begin{pmatrix}
           p(\lambda) & *\\
           * & *\\
          \end{pmatrix}
  \end{align*}
  Wenn $p$ das Minimalpolynom ist, ist $p(B) = 0_{n \times n}$, also insbesondere $p(B)_{11} = p(\lambda) = 0$.
 \end{itemize}
\end{proof}
\begin{application}
 Bestimmung von Minimalpolynomen.
\end{application}
\newpar
Sei $A$ eine quadratische Matrix:
\begin{align*}
 A = \begin{pmatrix}
      2 & 1 & 0\\
      0 & 2 & 1\\
      0 & 0 & 2\\
     \end{pmatrix}
\end{align*}
Dann gilt $\chi_A(t) = (t-2)^3$. Das Minimalpolynom hat also $2$ als einzige Nullstelle. $p_1(t) = (t-2)$ und $p_2(t) = (t-2)^2$ erfüllen die Bedingung $p(A) = 0_{n \times n}$ nicht, also ist $\chi_A(t)$ das Minimalpolynom.
\newpar
\begin{align*}
 B = \begin{pmatrix}
      2 & 1 & 0 & 0 & 0\\
      0 & 2 & 1 & 0 & 0\\
      0 & 0 & 2 & 0 & 0\\
      0 & 0 & 0 & 2 & 1\\
      0 & 0 & 0 & 0 & 2
     \end{pmatrix}
\end{align*}
Dann gilt $\chi_B(t) = (t-2)^5$. Allerdings ist der Nilpotenzindex der Matrix $B - \lambda E_n$ diesmal $3$, also ist bereits $p(t) = (t-2)^3$ ein Minimalpolynom.
\newpar
Analog für sehr große Matrizen mit vielen Jordanblöcken mit verschiedenen Eigenwerten.
\begin{corollary}
Sei $A$ eine $n \times n$-Matrix über $\bC$. Sei $m_i$ der längste Jordanblock der Matrix zum Eigenwert $\lambda_i$, definiert für alle Eigenwerte. Dann ist das Minimalpolynom von $A$ gegeben durch:
\begin{align*}
 p(t) = \sum_{i=1}^{d} (t - \lambda_i)^{m_i}
\end{align*}
\end{corollary}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Euklidische und Hermitische Vektorräume}
\begin{definition}
 Die folgende Abbildung $||\bR^n|| \to \bR$ nennen wir die \tbf{Euklidische Norm}:
 \begin{align*}
  || \cdot || : \bR^n &\to \bR\\
  \vx &\to \sqrt{\sum x_i^2}
 \end{align*}
\end{definition}

\begin{definition}
Wir nennen eine Abbildung $\phi$ eine \tbf{Isometrie}, wenn sie metrikerhaltend ist, also:
\begin{align*}
 d(\phi(\vx), \phi(\vy)) = d(\vx, \vy)
\end{align*}

\end{definition}
\newpar
($\hdots$)
\newpar
\ul{Konsequenz:} Die Isometrien bilden einer Untergruppe der Gruppe $Bij(\bR^n)$ der Bijektiven Selbstabbildungen (= Permutationen) des Raums.
\newpar
\ul{Ziel:} Diese Gruppe beschreiben!
\newpar
Wir können das Problem vereinfachen, indem wir die Abbildung:
\begin{align*}
 \phi_0 : \bR^n &\to \bR^n\\
 \phi(\vv) &\to \phi(\vz)
\end{align*}
betrachten, welche eine Isometrie mit $\phi_0(\vz) = \vz$ ist.

\begin{definition}
 Isometrien, für die $\phi(\vz) = \vz$ gilt, heißen \tbf{orthogonale Transformationen}. 
\end{definition}
\newpar
Orthogonale Transformationen bilden eine Untergruppe der Isometrien.
\newpar
\begin{definition}
Das Standardskalarprodukt, oder Euklidische Skalarprodukt, auf $\bR^n$ ist die Abbildung:
\begin{align*}
 \scalar : \bR^n \times \bR^n &\to \bR\\
                    (\vx, \vy) &\to \sum_{i=1}^n x_i y_i
\end{align*}
\end{definition}
\begin{anmerkung}
\begin{align*}
 \forall \vx \in \bR^n : ||\vx||^2 = \langle \vx, \vx \rangle
\end{align*}
\end{anmerkung}
\begin{anmerkung}
 Das Skalarprodukt hat folgende Eigenschaften:
 \begin{enumerate}
  \item Linearität in der ersten Komponente:
  \begin{align*}
   \langle \vx_1 + \lambda \vx_2, \vy \rangle = \langle \vx_1, \vy \rangle + \lambda \langle \vx_2, \vy \rangle
  \end{align*}
  \item Linearität in der zweiten Komponente:
  \begin{align*}
   \langle \vx, \vy_1 + \lambda \vy_2 \rangle = \langle \vx, \vy_1 \rangle + \lambda \langle \vx, \vy_2 \rangle
  \end{align*}
  \item Positive Definitheit:
  \begin{align*}
   \langle \vx, \vx \rangle \geq 0\\
   \langle \vx, \vx \rangle = 0 \Leftrightarrow \vx = \vz
  \end{align*}
  \item Symmetrie:
  \begin{align*}
   \scalar{\vx}{\vy} = \scalar{\vy}{\vx}
  \end{align*}
  \item Verallgemeinerter Satz des Pythagoras:
  \begin{align*}
   || \vx + \vy ||^2 = ||\vx||^2 + 2 \scalar{\vx}{\vy} + ||\vy||^2
  \end{align*}

 \end{enumerate}
\end{anmerkung}
\begin{definition}
 Zwei Vektoren $\vx, \vy$ heißen \tbf{orthogonal} zueinander, wenn 
 \begin{align*}
  \langle \vx, \vy \rangle = 0
 \end{align*}
\end{definition}
\begin{definition}
 Eine Basis $\vv_1, \hdots, \vv_n$ heißt \tbf{Orthonormalbasis}, wenn:
 \begin{align*}
   \forall i,j : \langle \vv_i \vv_j \rangle = \delta_{ij}
 \end{align*}
\end{definition}
\newpar
\ul{Motivation:}
Wenn irgendein $\vv \in \bR$ gegeben ist, kann ich schreiben:
\begin{align*}
 \vv = \sum_{i = 1}^n a_i \vv_i
\end{align*}
Im Allgemeinen muss man ein Gleichungssystem lösen, um die $a_i$ zu finden. Bilden die $\vv_i$ eine Orthonormalbasis, ist es jedoch einfach:
\begin{align*}
  \scalar{\vv_j}{\vv} &= \scalar{\vv_j}{\sum a_i \vv_i}\\
                      &= \sum a_i \scalar{\vv_j}{\vv_i}\\
                      &= a_j
\end{align*}
Also gilt:
\begin{align*}
 \vv = \sum_i \scalar{\vv_i}{\vv} \vv_i
\end{align*}
Diese Technik wird im Englischen als \tbf{coefficient picking} bezeichnet.
\begin{theorem}
 Orthogonale Transformationen erhalten das Skalarprodukt. Wenn also $\phi$ eine orthogonale Transformation ist und $\vx, \vy \in \bR^n$ gegeben sind, gilt:
 \begin{align*}
  \scalar{\vx}{\vy} = \scalar{\phi(\vy)}{\phi(\vy)}
 \end{align*}
\end{theorem}
\begin{proof}
 ``Doofe Rechnung mit Pythagoras, werd ich nicht machen.''
\end{proof}
\begin{konsequenz}
 \label{orthotransbasis}
 Orthogonale Transformationen bilden Orthonormalbasen auf Orthonormalbasen ab.
\end{konsequenz}
\begin{proof}
 Sei $\phi$ eine orthogonale Transformation, sei $\vv_1, \hdots, \vv_n \in \bR^n$ eine Orthonormalbasis.
 \begin{enumerate}
  \item Seien Indizes $i, j$ gegeben. Dann ist:
  \begin{align*}
   \scalar{\phi(\vv_i)}{\phi(\vv_j)} \overset{Fact}{=} \scalar{\vv_i}{\vv_j} = \delta_{ij}
  \end{align*}
  \item Sei eine lineare Relation der Bildvektoren gegeben, also:
  \begin{align*}
   \vz = \sum \lambda_i \phi(\vv_i)
  \end{align*}
  Dann gilt:
  \begin{align*}
   \vz &= \scalar{\phi(\vv_j)}{\vz}\\
       &= \scalar{\phi(\vv_j)}{\sum \lambda_i \phi(\vv_i)}\\
       &= \sum_i \lambda_i \scalar{\phi(\vv_j)}{\phi(\vv_i)}\\
       &= \lambda_j
  \end{align*}
  Also $\forall j : \lambda_j = 0$, also bilden die Bildvektoren eine Basis.
 \end{enumerate}
\end{proof}
\begin{konsequenz}
 Orthogonaltransformationen sind linear, bilden also eine Untergruppe von $GL_n(\bR)$.
\end{konsequenz}
\begin{proof}
 Die Standardbasis ist eine Orthonormalbasis, nach \ref{orthotransbasis} ist also die Bildmenge der Standardbasis unter der Orthogonaltransformation $\phi$ ebenfalls eine Orthonormalbasis. Gegeben $\vv \in \bR$ schreibe:
 \begin{align*}
  \phi(\vv) &= \sum_i \scalar{\phi(\ve_i)}{\phi(\vv)} \phi(\vv)\\
  &:= \sum_i \eta_i(\vv) \phi(\vv)
 \end{align*}
 \ul{Beobachtung:} Es genügt nun zu zeigen, dass für alle $i$ die Abbildung $\eta_i$ linear ist. Seien also $\vv_1, \vv_2 \in \bR^n$, $\lambda \in \bR$ gegeben. Dann
 \begin{align*}
  \eta_i (\vv_1 + \lambda \vv_2) &= \scalar{\phi(\ve_i)}{\phi(\vv_1 + \lambda \vv_2)}\\
  &= \scalar{\ve_i}{\vv_1 + \lambda \vv_2} \qquad\text{(Def. Orthogonaltransformationen)}\\
  &= \scalar{\ve_i}{\vv_1} + \lambda \scalar{\ve_i}{\vv_2}\\
  &= \scalar{\phi(\ve_i)}{\phi(\vv_1)} + \lambda \scalar{\phi(\ve_i)}{\phi(\vv_2)}\\
 \end{align*}
\end{proof}
\begin{question}
Wie können die Orthogonalen Transformationen als Matrizen beschrieben werden?
\end{question}
\ul{Proberechung:} Sei $\phi$ Orthogonal. Sei $B$ die Standardbasis des $\bR^n$. Dann ist:
\begin{align*}
 \Mat{B}{B}{\phi} = \left(\phi(\ve_1) \mid \hdots \mid \phi(\ve_n)\right)
\end{align*}
\ul{Beachte:} Die Spaltenvektoren bilden wieder eine Orthonormalbasis, also:
\begin{align*}
 \forall i,j : \scalar{\phi(\ve_i)}{\phi(\ve_j)} = \delta_{ij}
\end{align*}
\newpar
Wir beachten das Produkt der Matrix mit ihrer Transponierten:
\begin{align*}
\Mat{B}{B}{\phi}^T \cdot \Mat{B}{B}{\phi} &= (a_{ij}) \text{ mit } a_{ij} = \scalar{\phi(\ve_i)}{\phi{\ve_j}} = \delta_ij\\
&= E_n
\end{align*}
\begin{theorem}
 Ist $A$ darstellende Matrix einer Orthogonalen Transformationen, so ist $A^{-1} = A^T$.
\end{theorem}
\begin{proposition}
 Die Rückrichtung gilt auch - ist $A^{-1} = A^T$, so ist die dazugehörige Transformation orthogonal.
\end{proposition}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\appendix
\chapter{Ausblicke in die Zukunft}
In der Funktionalanalysis werden unendlichdimensionale Vektorräume betrachtet. 
\begin{theorem}
\label{satz:basisfunktionen}
Der Vektorraum aller Funktionen $f: \bR \to \bR$ hat eine Basis.
\end{theorem}
\newpar
Wie sieht diese Basis aus? Es stellt sich heraus, dass der Beweis nur dank Auswahlaxiom funktioniert, und dass sich diese Basis nicht explizit konstruieren lässt. Die Menge der sog. Kroneckerdeltas $\delta_{ij}$ sieht auf den ersten Blick wie ein vielversprechender Kandidat aus:
\begin{equation*}
 \delta_{ij} = \begin{cases}
            1 & i = j\\
            0 & i \neq j\\
           \end{cases}
\end{equation*}
Aber es muss bedacht werden, dass zwingende Bedingung für eine Basis ist, dass sich jeder Vektor nicht nur als Linearkombination der Basisvektoren darstellen lässt, sondern sogar als \tbf{endliche Linearkombination}. Diese Definition der Basis ist auch bekannt als Hamelbasis.
\newpar
Es stellt sich heraus, dass die Hamel-Basis aus \ref{satz:basisfunktionen} nur dank Auswahlaxiom existiert und nicht explizit dargestellt werden kann. Lockern wir den traditionellen Basisbegriff, um zählbar unendliche Linearkombinationen zu erlauben, erhalten wir den Begriff der Schauder-Basis. Die Funktionen $\delta_{ij}$ reichen jedoch immer noch nicht als Schauder-Basis des Raums $f: \bR \to \bR$, sondern nur für den Folgenraum $f: \bN \to \bR$. 
\newpar
Da sich der Begriff der Linearkombination auf keine sinnvolle Weise auf überabzählbare Mengen erweitern lässt bleibt man an diesem Punkt leider stecken, es existiert leider keine explizit angebbare Basis des Raums $f: \bR \to \bR$. :(
\end{document}
