\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz-cd} % commutative diagrams
\usepackage{physics} % \abs and \norm macros

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\cB}{\mathcal{B}}

\newcommand{\bb}{\vec{b}}
\newcommand{\vc}{\vec{c}}
\newcommand{\ve}{\vec{e}}
\newcommand{\vp}{\vec{p}}
\renewcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{0}}

\newcommand{\veta}{\vec{\eta}}

\newcommand{\Mat}[3]{\text{Mat}^{#1}_{#2}\left(#3\right)}
\newcommand{\Matnn}{\text{Mat}(n \times n)}
\newcommand{\scalar}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\Id}{\text{Id}}
\newcommand{\SO}{\mathcal{SO}}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis\newline}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Korollar}



\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{beispiel}[lemma]{Beispiel}
\newtheorem{beobachtung}[lemma]{Beobachtung}
\newtheorem{anmerkung}[lemma]{Anmerkung}
\newtheorem{question}[lemma]{Frage}
\newtheorem{application}[lemma]{Anwendung}
\newtheorem{konsequenz}[lemma]{Konsequenz}
%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
\chapter{Wiederholung}
\section{Notation}
\begin{itemize}
 \item Die Menge aller $n \times m$ Matrizen mit Einträgen aus dem Körper $k$ schreiben wir $Mat(n \times m, k)$
 \item Die Gruppe der Invertierbaren $n \times n$-Matrizen mit Einträgen aus $k$ schreiben wir $Gl_n(k)$
 \item Den Eigenraum einer Abbildung $f$ zum Eigenwert $\lambda$ schreiben wir $Eig_f(\lambda)$
 \item Die $n \times n$-Einheitsmatrix schreiben wir $I_n$ oder $E_n$, die Identitätsabbildung des Vektorraums $V$ schreiben wir $id_V$. Vermutlich werden wir die Unterscheidung zwischen den beiden Begriffen jedoch öfters ignorieren.
 \item Das characteristische Polynom eines Endomorphismus $f$ ist
 \begin{equation}
  \chi_f := det(f - t \cdot id_V).
 \end{equation}
 Analog ist das characteristische Polynom einer $n \times n$-Matrix $A$
 \begin{equation}
  \chi_A := det(A - t \cdot I_n).
 \end{equation}
\end{itemize}
%
%
\section{Determinanten}
\subsection{Axiomatische Beschreibung}
\begin{enumerate}
 \item Die Determinante ist \textbf{multilinear}, also:
 \begin{enumerate}
  \item $det(v_1 + w, v_2, \hdots, v_n) = det(v_1, \hdots, v_n) + det(w, v_2 \hdots, v_n)$, ebenso in den anderen Spalten und für Zeilenvektoren.
  \item $det(cv_1 + w, v_2, \hdots, v_n) = c \cdot det(v_1, \hdots, v_n)$, ebenso in anderen Spalten und für Zeilenvektoren
 \end{enumerate}
 \item Die Determinante ist \textbf{alternierend}: Sind zwei Spalten oder Zeilen gleich, ist die Determinante $0$.
 \item Sie ist \textbf{normiert} durch $det(I_n) := 1$
\end{enumerate}
%
%
\subsection{Weitere Eigenschaften}
\begin{enumerate}

 \item $det(A^\top) = det(A)$
 \item $det(A^{-1}) = \frac{1}{det(A)}$
 \item Für quadratische $A,B$ gleicher Größe gilt $det(AB) = det(A)det(B)$
 \item Für Konstante $c$ und $n \times n$-Matrix $A$ gilt $det(cA) = c^n det(A)$
 \item Für Dreiecksmatrizen $A$ gilt $det(A) = a_{11}a_{22}\hdots a_{nn}$

 \item Besteht eine Spalte oder Zeile aus Nullen, ist die Determinante $0$.
 \item Vertauscht man zwei Spalten oder Zeilen, ändert die Determinante ihr Vorzeichen. Dies ist äquivalent zu ``alternierend'' für Körper ohne selbstinverse Elemente $x = -x \neq 0$.
 \item Addition eines Vielfachen einer Zeile/Spalte zu einer anderen ändert die Determinante nicht.
\end{enumerate}

%
%
\subsection{Berechnung}
Die Determinante einer $2 \times 2$-Matrix ist:
\begin{align*}
 det \begin{pmatrix}
      a & b\\
      c & d
     \end{pmatrix}
    = ad - bc.
\end{align*}
Die Determinante einer $3 \times 3$-Matrix ist:
\begin{align*}
det \begin{pmatrix}
     a & b & c\\
     d & e & f\\
     g & h & i\\
    \end{pmatrix}
    = aei + bfg + cdh - ceg - bdi - afh
\end{align*}
%
Die Determinante eines Endomorphismus $f : V \to V$ ist das Produkt der Eigenwerte (unter Beachtung der algebraischen Vielfachheiten!)
%
\section{Basiswechsel}
Zu Beginn will ich einige relevante Sätze, Definitionen und Notationsstandards aus der Vorlesung ``Lineare Algebra I'' wiederholen.
\begin{definition}
Sei $f: V \to W$ eine lineare Abbildung. Sei $A = \{a_1, \hdots, a_n\}$ eine Basis von $V$ und $B = \{b_1, \hdots, b_m\}$ eine Basis von $W$. So lässt sich jeder Vektor $w \in W$ darstellen als \textbf{endliche} Linearkombination der Basisvektoren:
\begin{equation}
    w = \sum_{i=1}^{m} \alpha_i b_i
\end{equation}
Insbesondere lassen sich die Bilder der Basisvektoren $a_j \in A$ in dieser Form darstellen:
\begin{equation}
    f(a_j) = \sum_{i=1}^{m} \alpha_{ij} b_i
\end{equation}
Die \textbf{Darstellungsmatrix} $\text{Mat}_B^A f$ ist genau durch diese Koeffizienten $\alpha_{ij}$ gegeben.
\begin{equation}
    \text{Mat}_B^A f = 
    \begin{pmatrix}
       \alpha_{11} & \hdots & \alpha_{1n}\\
       \vdots      &        & \vdots\\
       \alpha_{m1} & \hdots & \alpha_{mn}\\
    \end{pmatrix}
\end{equation}
\end{definition}
\newpar
In der Regel arbeiten wir mit der Standardbasis $E = \{e_1, \hdots, e_n\}$ und interpretieren jede Matrix $M$ als Darstellungsmatrix $M_E^E f$ einer Linearen Abbildung $f$. 
\begin{definition}
Die \textbf{Basiswechselmatrix} $T_B^A$ ist die Abbildungsmatrix der Identitätsabbildung.
\begin{equation}
 T_B^A = \text{Mat}_B^A (\text{id}_V)
\end{equation}
\end{definition}
\begin{theorem}
Für jede Basis $B$ eines beliebigen Vektorraums $V$ gilt 
\begin{align}
T_B^B = I_n = \begin{pmatrix}
                1 & \hdots & 0\\
                \vdots & \ddots & \vdots\\
                0 & \hdots & 1\\
               \end{pmatrix}
\end{align}
\end{theorem}
%
%
%
\begin{proof}
Da die Darstellung jedes Vektors durch die Basisvektoren eindeutig gegeben ist, ist die Darstellung $id_V(b_j) = b_j = \sum_{i=1}^{m} \alpha_{ij} b_i$ eines Basisvektors als Linearkombination genau gegeben durch die Linearkombination mit $\alpha_{ij} = 1$ und $\alpha_{ik} = 0$ für $k \neq j$. Dies entspricht genau dem Standardbasisvektor $e_j$. Also gilt $T_B^B = (e_1 \hdots e_n) = I_n$.
\end{proof}
%
%
%
\begin{proposition}
\label{prop:basiswechsel}
Gegeben $\text{Mat}_B^A f$ lässt sich die Abbildungsmatrix $\text{Mat}_D^C f$ von $f$ bezüglich zweier neuen Basen $C$ und $D$ durch Nutzung von Basiswechselmatrizen folgendermaßen berechnen:
\begin{equation}
 \text{Mat}_D^C(f) = T_D^B \cdot \text{Mat}_B^A f \cdot T_C^B
\end{equation}
Ein besonders relevanter Spezialfall ist:
\begin{equation}
 \text{Mat}_B^B(f) = T_B^A \cdot \text{Mat}_A^A f \cdot T_A^B
\end{equation}
\end{proposition}
%
\newpar
%
\begin{theorem}
\label{satz:basiswechsel_inverse}
Es gilt $T_B^A$ = $(T_A^B)^{-1}$
\end{theorem}
\begin{proof}
\begin{align*}
T_A^B \cdot T_B^A &= T_A^B \cdot id_V \cdot T_B^A\\
                  &=\text{Mat}_A^B (\text{id}_V) \cdot \text{Mat}_B^B (id_V) \cdot \text{Mat}_B^A (\text{id}_V)\\
                  &= \text{Mat}_A^A (\text{id}_V)\\
                  &= \text{id}_V\\
\end{align*}
\end{proof}
%
\newpar
%
\begin{anmerkung}
Betrachten wir eine beliebige Basis $B = \{b_1, \hdots, b_n\}$. Die Matrix $T_B^E$ kann man trivial finden, da jeder Vektor $b_i = (b_{i1} \hdots b_{in})^T$ bezüglich der Standardbasis trivial geschrieben ist als $b_i = \sum_{i=1}^n b_{ij} \cdot e_i$. Nach \ref{satz:basiswechsel_inverse} lässt sich die Matrix $T_E^B$ ebenfalls ohne größere Probleme durch invertierung von $T_B^E$ finden.
\end{anmerkung}
%
\begin{definition}
 Wir nennen zwei Matrizen $A$ und $B$ \textbf{ähnlich}, falls eine Matrix $S$ existiert, sodass
 \begin{align*}
  A = S^{-1} B S
 \end{align*}
\end{definition}
%
\newpar
%
Ein zentrales Ziel der Vorlesung ``Lineare Algebra II'' ist es, zu einer Matrix $A$ eine besonders simple Matrix $B$ zu finden, welche $A$ ähnlich ist. Hierbei sollte man immer \ref{prop:basiswechsel} im Kopf behalten - zwei Matrizen $A$ und $B$ sind genau dann ähnlich, wenn sie die Darstellungsmatrizen der gleichen Funktion zu verschiedenen Basen sind, also wenn Basen $B_1$ und $B_2$ und ein Endomorphismus $f$ existieren, sodass
\begin{align*}
 A = Mat_{B_1}^{B_1}(f) \text{ und } B = Mat_{B_2}^{B_2}(f)
\end{align*}
%
%
%
\chapter{Die Jordansche Normalform}
Let $V$ be a vector space. Let $f: V \to V$ be a nilpotent endomorphism.
\newpar
Define
$
V^p = \ker(f^p)
$
Where $f^p$ denotes composition.
\begin{lemma}
\begin{equation*}
 V^1 \subseteq V^2 \subseteq \hdots \subseteq V
\end{equation*}

\end{lemma}
\begin{lemma}
\begin{equation*}
 \forall p : \forall v \in V : v \in V^p \Leftrightarrow f(v) \in V^{p-1}
\end{equation*}
\end{lemma}


\begin{theorem}
The \textbf{``natural mapping''}
\begin{align*}
 \overline{f}: V^p / V^{p-1} \to V^{p-1} / V^{p-2}
\end{align*}
is injective.
\end{theorem}
\begin{proof}
We know that the restriction$f|_{V^p}$ is a map $V^p \to V^{p-1}$.
\newpar
Let $q_p$ denote the quotient map $V^p \to V^p/V^{p-1}$.
\newpar
Let $q_{p-1}$ denote the quotient map $V^{p-1} \to V^{p-1}/V^{p-2}$.
\newpar
We have the following diagram:
\begin{figure}[h!]
\centering
\begin{tikzcd}[row sep = huge, column sep = huge]
V^p
\arrow[r, "f|_{V^p}"]
\arrow[d, "q_p"]
& V^{p-1}
\arrow[d, "q_{p-1}"] \\
V^p/V^{p-1}
\arrow[r, "\overline{f}"]
&  V^{p-1}/V^{p-2}
\end{tikzcd}
\end{figure}
\newpar
By \textbf{``the universal property''} we have that a unique $\overline{f}$ exists iff. $V^{p-1} \subseteq \ker(q_{p-1} \circ f|_{V^p})$. This inclusion holds, since for $v \in V^{p-1}$ we have $f(v) \in V^{p-2}$, which is exactly the kernel of $q_{p-1} V^{p-1} \to V^{p-1}/V^{p-2}$. Therefore the diagram commutes.
\newpar
We will show injectivity of $\overline{f}$ by showing $\ker(f) = \{0\}$. Let $k \in V^p / V^{p-1}$. Pick an arbitrary representative $v \in k$. Because the diagram commutes, we have
\begin{align*}
 k \in \ker f \Leftrightarrow f | V_p (v) \in \ker(q_{p-1}) \Leftrightarrow f(v) \in V^{p-2} \Leftrightarrow v \in V^{p-1}
\end{align*}
Since $k \in V^p / V^{p-1}$, we have that $v \in V^{p-1}$ must be in the same equivalence class as $0$. Therefore the kernel of $\overline{f}$ consists only of the equivalence class of the zero vector (which *is* the zero vector of the quotient space).
\end{proof}
\newpage
We have the following diagram:
\begin{figure}[h!]
\centering
\begin{tikzcd}[row sep = huge, column sep = huge]
V
\arrow[d, "q"]
& F(V) = V^p
\arrow[r, "\eta_V = f|_{V^p}"]
\arrow[d, "F(q) = q_p"]
& G(V) = V^{p-1}
\arrow[d, "G(q) = q_{p-1}"] \\
V/ker(f)
&  F(V/ker(f)) = V^p/V^{p-1}
\arrow[r, "\eta_{V/ker(f)} = \overline{f}"]
&  G(V/ker(f)) = V^{p-1}/V^{p-2}
\end{tikzcd}
\end{figure}
\section{Erste Anwendungen}
\textbf{Erinnerung:} Über $\bC$ ist jede Matrix ähnlich zu einer Jordan-Matrix.
\newpar
\underline{Erste Anwendungen:}
\begin{enumerate}
 \item Hohe Potenzen von Endomorphismen ausrechnen.
 \item Die Exponentialfunktion mit Matrixwertigen Argumenten ausrechnen und damit lineare Differentialgleichungen lösen, auch in vielen Veränderlichen und mit höheren Ableitungen.
\end{enumerate}
\section{Konjugationsklassen}
\newpar
Frage: Ist in unserer Situation die Jordan-Matrix von $f$ eindeutig?
\newpar
Antwort: Nein! Blöcke können durch Umsortierung der Basis vertauscht werden.
\begin{align*}
 V = \bC^2, \vv_1, \vv_2 \text{ EW zu } \lambda_1 \neq \lambda_2\\
\end{align*}
\begin{align*}
 B &= (\vv_1, \vv_2)\\
 Mat_B^B f &= \begin{pmatrix}
              \lambda_1 & 0\\
              0 & \lambda_2\\
             \end{pmatrix}\\
 B' &= (\vv_2, \vv_1)\\
 Mat_{B'}^{B'} f &= \begin{pmatrix}
              \lambda_2 & 0\\
              0 & \lambda_1\\
             \end{pmatrix}\\
\end{align*}
$\implies$ Neue Frage: Ist die Jordan-Matrix eindeutig bis auf Umsortierung der Blöcke?
\newpar
Versuch einer Antwort: Sei $B$ eine Jordan-Basis,
\begin{align*}
 Mat_B^B (f) = \begin{pmatrix}
                Jor(\lambda_1, n_1) & 0 & 0\\
                0 & \ddots & 0\\
                0 & 0 & Jor(\lambda_r, n_r)\\
               \end{pmatrix}
\end{align*}
Beachte: Die Skalare $\lambda_1, \hdots, \lambda_r$ sind genau die Eigenwerte von $f$ und müssen deshalb in jeder Jordan-Matrix von $f$ auftreten.
\newpar
\underline{Beachte:} Wenn $\lambda$ ein Eigenwert von $f$ ist, seien $i_1, \hdots, i_k$ die Indizes mit $\lambda_i = \lambda$. Die Summe der indizes ist die Dimension des Hauptraums von $f$ zu $\lambda$, muss also für jede Jordan-Matrix von $f$ gleich sein.
\begin{align*}
 \sum_{j=1}^k n_{i_j} = \dim Hau_f (\lambda) = \text{ alg. Mult. von $f$}
\end{align*}
Die rechte Seite hängt nicht von $B$ ab, ist also eine Invariante der Jordan-Matrizen von $f$. Außerdem: Wenn $f$ auf den Hauptraum $Hau_f(\lambda)$ eingeschränkt wird und wir die Räume $V^p$ und die Partition $m_p$ betrachten, sind die Zahlen $n_{i}$ genau die Zahlen, die in der zu $m_p$ dualen Partition auftreten (wenn auch in willkürlicher Reihenfolge)
\newpar
$\implies$ \ul{Ergebnis:} Ja! Die Jordanmatrix ist eindeutig bis auf Umsortieren der Blöcke.
\newpar
\begin{definition}
 Die Ähnlichkeitsrelation zwischen Matrizen wird auch als \tbf{Konjugation} (oder genauer als \tbf{Konjugationswirkung von $Gl_n$ auf eine Matrix}) bezeichnet.
\end{definition}
\begin{definition}
 Als \tbf{Orbit (der Konjugationswirkung von $Gl_n$ auf $M$)} einer Matrix $M$ bezeichnen wir die Menge der Matrizen, die zu einer gegebenen Matrix $M$ ähnlich sind.
\end{definition}
\newpar
\ul{Beobachtung:} Jede Matrix ist einer Jordanmatrix ähnlich, also liegt in jedem Orbit mindestens eine Jordan-Matrix!
\newpar
Wären Jordan-Matrizen eindeutig, hätten wir nun eine triviale Bijektion zwischen der Menge aller Orbiten von Matrizen und den Jordanformen. Da Jordanmatrizen nur bis auf Umsortieren eindeutig ist, ist die Situation etwas komplizierter (aber nicht viel komplizierter!)
\newpar
\begin{theorem}
 Sei $B$ eine Jordan-Basis für $f$ mit Matrix $M_f = Mat_B^B (f)$. Sei $A \in Mat(n \times n)$ eine Matrix, die sich von $M_f$ nur durch Umsortierung von Blöcken unterscheidet. Dann gibt es eine Umsortierung $B'$ von $B$, sodass $Mat_{B'}^{B'} (f) = A$.
\end{theorem}
\newpar
 \ul{Äquivalent:} Wenn sich Matrizen in Jordanform nur durch Umsortierung der Blöcke unterscheiden, sind sie ähnlich.
\newpar
 \ul{Ebenfalls Äquivalent:} Wenn ein Orbit eine Jordan-Matrix $A$ enthält, so auch alle Jordan-Matrizen, welche aus $A$ durch Umsortierung der Blöcke entstehen.
\newpar
\begin{theorem}
 Wenn $B_1$, $B_2$ Jordanbasen von $f$ sind, dann unterscheiden sich die Darstellungsmatrizen $\Mat{B_1}{B_1}{f}$ und $\Mat{B_2}{B_2}{f}$ von $f$ zu den beiden Basen nur durch Umsortierung der Blöcke.
\end{theorem}
\newpar
\ul{Äquivalent:} Zwei Jordanmatrizen im selben Orbit unterscheiden sich nur durch Umsortieren der Blöcke.
\newpar
\ul{Zusammenfassung:} Es existiert eine Bijektion zwischen den Orbiten der Konjugationswirkung und den Äquivalenzklassen von Jordanmatrizen unter Umsortierung.
\section{Der Satz von Cayley-Hamilton}
Seien $V$ ein Vektorraum und $f \in End(V)$ ein Endomorphismus. Wir betrachteten bisher Ausdrücke wie $f$, $f^2$, $f^3$, $f^5 - \lambda \cdot Id_V$ (äquivalent zu $f^5 - \lambda f^0$). Diese sehen aus wie Polynome. Können wir diese Idee formalisieren?
\begin{definition}
 Gegeben ein Körper $k$ schreiben wir für die Menge der Polynome mit Variable $t$ und Koeffizienten aus $k$:
 \begin{align*}
  k[t] = \left\{\sum_{i=0}^m a_i t^i \mid i \in \bN, a_i \in k\right\}
 \end{align*}
\end{definition}
\begin{anmerkung}
Gegeben ein Polynom $p \in k[t]$ induziert dieses eine Funktion 
\begin{align*}
 k &\to k\\
 x &\to \sum a_i x^i\\
\end{align*}
Sei $\phi$ die Funktion, welche jedem Polynom die durch das Polynom induzierte Abbildung zuordnet:
\begin{align*}
 \phi : k[t] \to (t \to k)
\end{align*}
Im Allgemeinen ist $\phi$ nicht injektiv! Sei zum Beispiel $k = F_2$, dann induzieren die Polynome $f, f^2, f_3, \hdots$ alle die selbe Funktion. Polynome sind also von den von ihnen induzierten Funktionen zu unterscheiden. 
\end{anmerkung}
\begin{definition}
 Wir Betrachten die Auswertung eines Polynoms in $f$:
 \begin{align*}
 \begin{array}{ccccc}
  \phi_f & : & k[t] &\to& End(V)\\
       & &\sum a_i t^i &\to& \sum a_i \cdot f^i
 \end{array}
 \end{align*}
 Analog definieren wir die Auswertung für Matrizen $A$.
\end{definition}
\begin{beobachtung}
 Seien $A$, $B$ ähnlich mit $A = S B S^{-1}$. Dann gilt $A^n = (S B S^{-1})^n = S B^n S^{-1}$, also sind $A^n$ und $B^n$ ebenfalls ähnlich. Also gilt für jedes Polynom:
 \begin{align*}
  p = \sum a_i t^i
 \end{align*}
 \begin{align*}
  \phi_A(p) &= \sum a_i A^i\\ &= \sum a_i S B^i S^{-1}\\ &= S \left(\sum a_i B^i\right) S^{-1}\\ &= S \phi_B(p) S^{-1}
 \end{align*}
 Also bleibt Ähnlichkeit unter der Anwendung von Polynomen erhalten.
\end{beobachtung}
\begin{beobachtung}
 Betrachte $f \in End(V)$. Wähle Basis $B$ von $V$. Dann ist
 \begin{align*}
  \Mat{B}{B}{f^i} = \left[\Mat{B}{B}{f}\right]^i
 \end{align*}
 Also gilt für jedes Polynom $p = \sum a_i t^i$:
 \begin{align*}
  \Mat{B}{B}{\phi_f(p)} &= \Mat{B}{B}{\sum a_i f^i}\\ 
                        &= \sum a_i \Mat{B}{B}{f^i}\\ 
                        &= \sum a_i \left[\Mat{B}{B}{f}\right]^i\\
                        &= \phi_{\Mat{B}{B}{f}}(p)
 \end{align*}
\end{beobachtung}
\begin{anmerkung}
 Die Abbildungen $\phi_f$, $\phi_A$ werden auch \tbf{Einsetzungsabbildungen} genannt. Statt $\phi_f(p)$ und $\phi_A(p)$ schreibt man oft $p(f)$ und $p(A)$.
\end{anmerkung}
\begin{theorem}
 \emph{\tbf{Cayley-Hamilton}:} Sei $\chi_f \in k[t]$ das charakteristische Polynom von $f$. Dann ist
 \begin{align*}
  \chi_f(f) = 0.
 \end{align*}
\end{theorem}
\begin{proof}
 (Nur für $k = \bC$). Sei $B$ eine Jordan-Basis. Dann 
 \begin{align*}
  \Mat{B}{B}{\chi_f(f)} = \chi_f \left(\Mat{B}{B}{f}\right).
 \end{align*}
 Es genügt also 
 $
  \chi_f \left(\Mat{B}{B}{f}\right) = 0
 $
 zu zeigen.
 \newpar
 Schreibe
 \begin{align*}
  \Mat{B}{B}{f} = \begin{pmatrix}
                   Jor(\lambda_1, n_1) & 0 & 0\\
                   0 & \ddots & 0\\
                   0 & 0 & Jor(\lambda_r, n_r)\\
                  \end{pmatrix}
 \end{align*}
 Es gilt
 \begin{align*}
  \chi_f(t) = (t - \lambda_1)^{n_1} \cdot \hdots \cdot  (t - \lambda_1)^{n_r}
 \end{align*}
 Wegen der Blockgestalt gilt:
 \begin{align*}
  \chi_f(\Mat{B}{B}{f}) = \begin{pmatrix}
                   \chi_f(Jor(\lambda_1, n)) & 0 & 0\\
                   0 & \ddots & 0\\
                   0 & 0 & \chi_f(Jor(\lambda_r, n))\\
                  \end{pmatrix}
 \end{align*}
 Wir wissen, dass jeder Jordanblock $Jor(\lambda_i - \lambda_i, n_i$ nilpotent mit Index $n_i$ ist. Also hat das charakteristische Polynom Nullfaktoren, also ist $\chi_f \left(\Mat{B}{B}{f}\right) = 0$.
\end{proof}
%
%
%
%
%
\section{Minimalpolynome}
\begin{definition}
Wir nennen ein Polynom $p$ das Minimalpolynom eines Endomorphismus $f$, wenn gilt:
\begin{enumerate}
 \item $p$ ist nicht das Nullpolynom
 \item $p(f) = 0$
 \item Der Grad von $p$ ist minimal unter allen Polynomen mit $f$ als Nullstelle \label{minpoly-3}
 \item Der Leitkoeffizient ist $1$ ($p$ ist normiert) \label{minpoly-4}
\end{enumerate}
Analog für quadratische Matrizen statt Endomorphismen.
\end{definition}
\newpar
Normierung wird einfach durch Skalarmultiplikation des Polynoms erreicht, ist also keine starke Bedingung.
\begin{theorem}
 Ähnliche Matrizen haben das selbe Minimalpolynom.
\end{theorem}
\begin{proof}
 \begin{align*}
  A &= S B S^{-1}\\
  \implies A^n &= (S B S^{-1})^n = S B^n S^{-1}\\
  \implies p(A) &= p(S B S^{-1}) = S \cdot  p(B) \cdot S^{-1} := 0 \implies p(B) = 0\\
 \end{align*}
\end{proof}
\begin{theorem}
 $f$ hat das selbe Minimalpolynom wie seine darstellenden Matrizen.
\end{theorem}
\begin{theorem}
 Minimalpolynome existieren.
\end{theorem}
\begin{proof}
 Gemäß Satz von Cayley-Hamilton gibt es mindestens ein Polynom, welches nicht das Nullpolynom ist und $f$ als Nullstelle hat. Also finde nichttriviales Polynom $p$ von minimalem Grad finden, welches $f$ als Nullstelle hat.\footnote{Die Menge der Grade von Polynomen mit $f$ als Nullstelle ist eine nichtleere Teilmenge der Natürlichen Zahlen, hat also ein Minimum.} Sei $a$ der Leitkoeffizient von $p$. Dann ist $a^{-1}p$ ein normiertes Polynom vom selben Grad mit $f$ als Nullstelle.
\end{proof}
\begin{theorem}
Minimalpolynome sind eindeutig.
\end{theorem}
\begin{proof}
 Seien $p_1, p_2 \in k[t]$ zwei Minimalpolynome von $f$. Wir wissen, dass die Grade gleich sein müssen (\ref{minpoly-3}) und dass die Leitkoeffizienten $1$ sein müssen (\ref{minpoly-4}). Wir haben:
 \begin{align*}
  p_1(t) &= t^n + \sum^{n-1}_{i=0} a_i t^i\\
  p_2(t) &= t^n + \sum^{n-1}_{i=0} b_i t^i\\
 \end{align*}
 \begin{align*}
  \implies (p_1 - p_2)(t) = \sum^{n-1}_{i=0} (a_i - b_i) t^i\\
 \end{align*}
 Der Grad des Differenzpolynoms muss kleiner als $n$ sein, sonst wären $p_1$ und $p_2$ nicht minimal. Gleichzeitig gilt:
 \begin{align*}
  (p_1 - p_2)(f) = p_1(f) - p_2(f) = 0 - 0 = 0
 \end{align*}
 Also muss $p_1 - p_2$ das Nullpolynom sein, also ist $p_1 = p_2$.
\end{proof}
\begin{question}
 Wie viele Polynome gibt es überhaupt, die $f$ als Nullstelle haben?
\end{question}
\newpar
\ul{Antwort 1:} Viele! Wenn $p$ ein Polynom ist mit $p(f) = 0$, so hat jedes Vielfache von $p$ unter Polynommultiplikation ebenfalls $f$ als Nullstelle.
\newpar
\ul{Antwort 2:}
\begin{theorem}
 Alle Polynome, die $f$ als Nullstelle haben, sind Vielfache (unter Polynommultiplikation) des Minimalpolynoms. Wenn $p$ das Minimalpolynom von $f$ ist und $q \in k[t]$ ein beliebiges nichttriviales Polynom mit $q(f) = 0$, dann existiert ein $r \in k[t]$, sodass $q = r \cdot p$.
\end{theorem}
\begin{proof}
Sei $q$ gegeben. Polynomdivision mit Rest liefert Polynome $r_1, r_2 \in k[t]$ sd. $\deg r_2 < \deg p$ und 
\begin{align*}
 q = r_1 \cdot p + r_2
\end{align*}
Es gilt nun:
\begin{align*}
 q(f) &= r_1(f)p(f) + r_2(f)\\
 \implies 0 &= r_1(f) \cdot 0 + r_2(f)\\
 \implies r_2(f) &= 0\\
\end{align*}
Da $\deg r_2 < \deg p$ und $p$ ein Minimalpolynom muss $r_2$ also das Nullpolynom sein. Der Beweis für Matrizen funktioniert identisch.
\end{proof}
\begin{question}
 Wie findet man das Minimalpolynom für ein gegebenes $f$?
\end{question}
\newpar
\ul{Antwort}: Schwierig, es sei denn, man kennt die Jordanform!
\begin{proposition}
 Sei $A$ eine quadratische Matrix. Sei $\lambda \in k$. Die folgenden Aussagen sind äquivalent:
 \begin{enumerate}
  \item $\lambda$ ist ein Eigenwert von $A$.
  \item $\lambda$ ist eine Nullstelle des charakteristischen Polynoms.
  \item $\lambda$ ist eine Nullstelle des Minimalpolynoms.
 \end{enumerate}
\end{proposition}
\begin{proof}
\newpar
 \begin{itemize}
  \item[$1 \Leftrightarrow 2$:] Die Äquivalenz der ersten beiden Aussagen ist bereits bekannt.
  \item[$3 \Rightarrow 2$:] Das charakteristische Polynom ist ein Vielfaches des Minimalpolynoms.
  \item[$1 \Rightarrow 3$:] Sei ein Eigenwert $\lambda$ gegeben. Per Annahme existiert eine zu $A$ ähnliche Matrix $B$ der Form:
  \begin{align*}
   B = \begin{pmatrix}
        \lambda & *\\
        0 & *\\
       \end{pmatrix}
  \end{align*}
  Wenn $p$ ein beliebiges Polynom ist, dann ist:
  \begin{align*}
   p(B) = \begin{pmatrix}
           p(\lambda) & *\\
           * & *\\
          \end{pmatrix}
  \end{align*}
  Wenn $p$ das Minimalpolynom ist, ist $p(B) = 0_{n \times n}$, also insbesondere $p(B)_{11} = p(\lambda) = 0$.
 \end{itemize}
\end{proof}
\begin{application}
 Bestimmung von Minimalpolynomen.
\end{application}
\newpar
Sei $A$ eine quadratische Matrix:
\begin{align*}
 A = \begin{pmatrix}
      2 & 1 & 0\\
      0 & 2 & 1\\
      0 & 0 & 2\\
     \end{pmatrix}
\end{align*}
Dann gilt $\chi_A(t) = (t-2)^3$. Das Minimalpolynom hat also $2$ als einzige Nullstelle. $p_1(t) = (t-2)$ und $p_2(t) = (t-2)^2$ erfüllen die Bedingung $p(A) = 0_{n \times n}$ nicht, also ist $\chi_A(t)$ das Minimalpolynom.
\newpar
\begin{align*}
 B = \begin{pmatrix}
      2 & 1 & 0 & 0 & 0\\
      0 & 2 & 1 & 0 & 0\\
      0 & 0 & 2 & 0 & 0\\
      0 & 0 & 0 & 2 & 1\\
      0 & 0 & 0 & 0 & 2
     \end{pmatrix}
\end{align*}
Dann gilt $\chi_B(t) = (t-2)^5$. Allerdings ist der Nilpotenzindex der Matrix $B - \lambda E_n$ diesmal $3$, also ist bereits $p(t) = (t-2)^3$ ein Minimalpolynom.
\newpar
Analog für sehr große Matrizen mit vielen Jordanblöcken mit verschiedenen Eigenwerten.
\begin{corollary}
Sei $A$ eine $n \times n$-Matrix über $\bC$. Sei $m_i$ der längste Jordanblock der Matrix zum Eigenwert $\lambda_i$, definiert für alle Eigenwerte. Dann ist das Minimalpolynom von $A$ gegeben durch:
\begin{align*}
 p(t) = \sum_{i=1}^{d} (t - \lambda_i)^{m_i}
\end{align*}
\end{corollary}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Euklidische und Hermitische Vektorräume}
\begin{definition}
 Die folgende Abbildung $||\bR^n|| \to \bR$ nennen wir die \tbf{Euklidische Norm}:
 \begin{align*}
  || \cdot || : \bR^n &\to \bR\\
  \vx &\to \sqrt{\sum x_i^2}
 \end{align*}
\end{definition}

\begin{definition}
Wir nennen eine Abbildung $\phi$ eine \tbf{Isometrie}, wenn sie metrikerhaltend ist, also:
\begin{align*}
 d(\phi(\vx), \phi(\vy)) = d(\vx, \vy)
\end{align*}

\end{definition}
\newpar
($\hdots$)
\newpar
\ul{Konsequenz:} Die Isometrien bilden einer Untergruppe der Gruppe $Bij(\bR^n)$ der Bijektiven Selbstabbildungen (= Permutationen) des Raums.
\newpar
\ul{Ziel:} Diese Gruppe beschreiben!
\newpar
Wir können das Problem vereinfachen, indem wir die Abbildung:
\begin{align*}
 \phi_0 : \bR^n &\to \bR^n\\
 \phi(\vv) &\to \phi(\vz)
\end{align*}
betrachten, welche eine Isometrie mit $\phi_0(\vz) = \vz$ ist.

\begin{definition}
 Isometrien, für die $\phi(\vz) = \vz$ gilt, heißen \tbf{orthogonale Transformationen}. 
\end{definition}
\newpar
Orthogonale Transformationen bilden eine Untergruppe der Isometrien.
\newpar
\begin{definition}
Das Standardskalarprodukt, oder Euklidische Skalarprodukt, auf $\bR^n$ ist die Abbildung:
\begin{align*}
 \scalar : \bR^n \times \bR^n &\to \bR\\
                    (\vx, \vy) &\to \sum_{i=1}^n x_i y_i
\end{align*}
\end{definition}
\begin{anmerkung}
\begin{align*}
 \forall \vx \in \bR^n : ||\vx||^2 = \langle \vx, \vx \rangle
\end{align*}
\end{anmerkung}
\begin{anmerkung}
 Das Skalarprodukt hat folgende Eigenschaften:
 \begin{enumerate}
  \item Linearität in der ersten Komponente:
  \begin{align*}
   \langle \vx_1 + \lambda \vx_2, \vy \rangle = \langle \vx_1, \vy \rangle + \lambda \langle \vx_2, \vy \rangle
  \end{align*}
  \item Linearität in der zweiten Komponente:
  \begin{align*}
   \langle \vx, \vy_1 + \lambda \vy_2 \rangle = \langle \vx, \vy_1 \rangle + \lambda \langle \vx, \vy_2 \rangle
  \end{align*}
  \item Positive Definitheit:
  \begin{align*}
   \langle \vx, \vx \rangle \geq 0\\
   \langle \vx, \vx \rangle = 0 \Leftrightarrow \vx = \vz
  \end{align*}
  \item Symmetrie:
  \begin{align*}
   \scalar{\vx}{\vy} = \scalar{\vy}{\vx}
  \end{align*}
  \item Verallgemeinerter Satz des Pythagoras:
  \begin{align*}
   || \vx + \vy ||^2 = ||\vx||^2 + 2 \scalar{\vx}{\vy} + ||\vy||^2
  \end{align*}

 \end{enumerate}
\end{anmerkung}
\begin{definition}
 Zwei Vektoren $\vx, \vy$ heißen \tbf{orthogonal} zueinander, wenn 
 \begin{align*}
  \langle \vx, \vy \rangle = 0
 \end{align*}
\end{definition}
\begin{definition}
 Eine Basis $\vv_1, \hdots, \vv_n$ heißt \tbf{Orthonormalbasis}, wenn:
 \begin{align*}
   \forall i,j : \langle \vv_i \vv_j \rangle = \delta_{ij}
 \end{align*}
\end{definition}
\newpar
\ul{Motivation:}
Wenn irgendein $\vv \in \bR$ gegeben ist, kann ich schreiben:
\begin{align*}
 \vv = \sum_{i = 1}^n a_i \vv_i
\end{align*}
Im Allgemeinen muss man ein Gleichungssystem lösen, um die $a_i$ zu finden. Bilden die $\vv_i$ eine Orthonormalbasis, ist es jedoch einfach:
\begin{align*}
  \scalar{\vv_j}{\vv} &= \scalar{\vv_j}{\sum a_i \vv_i}\\
                      &= \sum a_i \scalar{\vv_j}{\vv_i}\\
                      &= a_j
\end{align*}
Also gilt:
\begin{align*}
 \vv = \sum_i \scalar{\vv_i}{\vv} \vv_i
\end{align*}
Diese Technik wird im Englischen als \tbf{coefficient picking} bezeichnet.
\begin{theorem}
 Orthogonale Transformationen erhalten das Skalarprodukt. Wenn also $\phi$ eine orthogonale Transformation ist und $\vx, \vy \in \bR^n$ gegeben sind, gilt:
 \begin{align*}
  \scalar{\vx}{\vy} = \scalar{\phi(\vy)}{\phi(\vy)}
 \end{align*}
\end{theorem}
\begin{proof}
 ``Doofe Rechnung mit Pythagoras, werd ich nicht machen.''
\end{proof}
\begin{konsequenz}
 \label{orthotransbasis}
 Orthogonale Transformationen bilden Orthonormalbasen auf Orthonormalbasen ab.
\end{konsequenz}
\begin{proof}
 Sei $\phi$ eine orthogonale Transformation, sei $\vv_1, \hdots, \vv_n \in \bR^n$ eine Orthonormalbasis.
 \begin{enumerate}
  \item Seien Indizes $i, j$ gegeben. Dann ist:
  \begin{align*}
   \scalar{\phi(\vv_i)}{\phi(\vv_j)} \overset{Fact}{=} \scalar{\vv_i}{\vv_j} = \delta_{ij}
  \end{align*}
  \item Sei eine lineare Relation der Bildvektoren gegeben, also:
  \begin{align*}
   \vz = \sum \lambda_i \phi(\vv_i)
  \end{align*}
  Dann gilt:
  \begin{align*}
   \vz &= \scalar{\phi(\vv_j)}{\vz}\\
       &= \scalar{\phi(\vv_j)}{\sum \lambda_i \phi(\vv_i)}\\
       &= \sum_i \lambda_i \scalar{\phi(\vv_j)}{\phi(\vv_i)}\\
       &= \lambda_j
  \end{align*}
  Also $\forall j : \lambda_j = 0$, also bilden die Bildvektoren eine Basis.
 \end{enumerate}
\end{proof}
\begin{konsequenz}
 Orthogonaltransformationen sind linear, bilden also eine Untergruppe von $GL_n(\bR)$.
\end{konsequenz}
\begin{proof}
 Die Standardbasis ist eine Orthonormalbasis, nach \ref{orthotransbasis} ist also die Bildmenge der Standardbasis unter der Orthogonaltransformation $\phi$ ebenfalls eine Orthonormalbasis. Gegeben $\vv \in \bR$ schreibe:
 \begin{align*}
  \phi(\vv) &= \sum_i \scalar{\phi(\ve_i)}{\phi(\vv)} \phi(\vv)\\
  &:= \sum_i \eta_i(\vv) \phi(\vv)
 \end{align*}
 \ul{Beobachtung:} Es genügt nun zu zeigen, dass für alle $i$ die Abbildung $\eta_i$ linear ist. Seien also $\vv_1, \vv_2 \in \bR^n$, $\lambda \in \bR$ gegeben. Dann
 \begin{align*}
  \eta_i (\vv_1 + \lambda \vv_2) &= \scalar{\phi(\ve_i)}{\phi(\vv_1 + \lambda \vv_2)}\\
  &= \scalar{\ve_i}{\vv_1 + \lambda \vv_2} \qquad\text{(Def. Orthogonaltransformationen)}\\
  &= \scalar{\ve_i}{\vv_1} + \lambda \scalar{\ve_i}{\vv_2}\\
  &= \scalar{\phi(\ve_i)}{\phi(\vv_1)} + \lambda \scalar{\phi(\ve_i)}{\phi(\vv_2)}\\
 \end{align*}
\end{proof}
\begin{question}
Wie können die Orthogonalen Transformationen als Matrizen beschrieben werden?
\end{question}
\ul{Proberechung:} Sei $\phi$ Orthogonal. Sei $B$ die Standardbasis des $\bR^n$. Dann ist:
\begin{align*}
 \Mat{B}{B}{\phi} = \left(\phi(\ve_1) \mid \hdots \mid \phi(\ve_n)\right)
\end{align*}
\ul{Beachte:} Die Spaltenvektoren bilden wieder eine Orthonormalbasis, also:
\begin{align*}
 \forall i,j : \scalar{\phi(\ve_i)}{\phi(\ve_j)} = \delta_{ij}
\end{align*}
\newpar
Wir beachten das Produkt der Matrix mit ihrer Transponierten:
\begin{align*}
\Mat{B}{B}{\phi}^T \cdot \Mat{B}{B}{\phi} &= (a_{ij}) \text{ mit } a_{ij} = \scalar{\phi(\ve_i)}{\phi{\ve_j}} = \delta_ij\\
&= E_n
\end{align*}
\begin{theorem}
 Ist $A$ darstellende Matrix einer Orthogonalen Transformationen, so ist $A^{-1} = A^T$.
\end{theorem}
\begin{proposition}
 Die Rückrichtung gilt auch - ist $A^{-1} = A^T$, so ist die dazugehörige Transformation orthogonal.
\end{proposition}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\section{Bilinearformen}
\begin{definition}
 Sei $k$ ein Körper, $V$ ein Vektorraum. Eine Abbildung $b : V \times V \to k$ heißt \tbf{Bilinearform}, falls gilt:
 \begin{enumerate}
  \item Linearität in der ersten Komponente
  \item Linearität in der zweiten Komponente
 \end{enumerate}
\end{definition}
\begin{beispiel}
\begin{enumerate}
 \item Das Standardskalarprodukt des $\bR^n$
 \item Für $A \in (n \times n, k)$ die Abbildung $b : (\vx, \vy) \to \vx^\top A \vy$
\end{enumerate}
\end{beispiel}
\begin{definition}
 Wir nennen eine Bilinearform $b$ symmetrisch, wenn $\forall \vx, \vy \in V : b(\vx, \vy) = b(\vy, \vx)$
\end{definition}
\begin{question}
 Für welche Matrizen ist die Bilinearform $\vx^\top A \vy$ symmetrisch?
\end{question}
\hspace{-14pt}\ul{Antwort}: Die Bilinearform zu $A$ ist genau dann symmetrisch, wenn die Matrix symmetrisch ist, also $A = A^\top$.
\begin{definition}
 Wir nennen eine Bilinearform $b : V \times V \to k$ \tbf{positiv semidefinit}, wenn $\forall \vx : b(\vx,\vx) \geq 0$. Wir nennen sie \tbf{positiv definit}, wenn $\forall \vx \neq 0 : b(\vx, \vx) > 0$
\end{definition}
\hspace{-14pt}\ul{Problem}: Auf den meisten Körpern $k$ existiert keine Totalordnung! Dementsprechend definieren wir positive Definitheit nur für $k \subseteq \bR$).
\begin{beobachtung}
 Die Bilinearformen bilden einen Vektorraum über $k$ (einen Untervektorraum des $k$-Vektorraums der Abbildungen $V \times V \to k$).
\end{beobachtung}
\begin{beobachtung}
 Die symmetrischen Bilinearformen bilden einen Untervektorraum des Raums der Bilinearformen.
\end{beobachtung}
\begin{beobachtung}
 Die positiv definiten Bilinearformen sind im Vektorraum aller Bilinearformen abgeschlossen unter Addition und unter Skalarmultiplikation mit \tbf{nichtnegativen} Skalaren. Sie bilden dementsprechend keinen Untervektorraum, aber immerhin einen konvexen Kegel.
\end{beobachtung}
\begin{definition}
 Sei $k \subseteq \bR$. Ein Skalarprodukt auf einem $k$-Vektorraum $V$ ist eine symmetrische, positiv definite Bilinearform.
\end{definition}
\begin{beispiel}
 Das Euklidische Skalarprodukt auf dem $\bR^n$.
\end{beispiel}
\begin{beispiel}
 $V = \mathcal{C}([0,1])$ (Stetige Funktionen auf $[0,1]$), dann können wir ein Skalarprodukt $\scalar{\_}{\_}$ definieren als:
 \begin{align*}
  \scalar{\_}{\_} : V \times V &\to \bR\\
  (f,g) &\to \int_{0}^1 f g dx\\
 \end{align*}
\end{beispiel}
\begin{beobachtung}
 Sei $k$ ein Körper, $V$ ein endlichdimensionaler $k$-Vektorraum, $B = \{\vv_1, \hdots, \vv_n\}$ eine angeordnete Basis mit Koordinatenabbildung $\phi_B : V \to k^n$. Wir haben zwei Konstruktionen:
 \begin{enumerate}
  \item \begin{align*}
         \text{Mat}_B: \{\text{Bilinearform}\} &\to \text{Mat}(n \times n)\\
         b &\to (b(\vv_i, \vv_j))_{i,j}
        \end{align*}
  \item \begin{align*}
         S_B : \text{Mat}(n \times n) &\to \{\text{Bilinearformen}\}\\
         A &\to ((\vx,\vy) \to \phi_B(\vx)^\top A \phi_B(\vy))
        \end{align*}
 \end{enumerate}
\end{beobachtung}
\begin{proposition}
 $\text{Mat}_B$ und $S_B$ sind inverse zueinander. Insbesondere ist dementsprechend die Dimension der $V$-Bilinearformen gleich der Dimension der $V$-Matrizen, also $\dim(V)^2$. 
\end{proposition}
\begin{corollary}
 Gleichzeitig identifizieren diese Isomorphismen direkt symmetrische Bilinearformen mit symmetrischen Matrizen, also $\dim(\text{symmetrische Bilinearformen}) = \dim(\text{symmetrische Matrizen}) = \sum_{k=1}^n k = \frac{n(n+1)}{2}$
\end{corollary}
\begin{question}
 Wie genau hängt die Matrix von der Wahl der Basis ab?
\end{question}
\hspace{-14pt}\ul{Antwort:} Seien $B_1$ und $B_2$ angeordnete Basen mit Basiswechselmatrix $S = \Mat{B_1}{B_2}{\text{id}_V}$ und kommutativem Diagramm:
\begin{figure}[h!]
\centering
\begin{tikzcd}[row sep = huge, column sep = huge]
k^n
\arrow[r, "\vv \to S \cdot \vv"]
& k^n\\
V
\arrow[u, "\phi_{B_1}"]
\arrow[r,"\text{id}_V"]
&  V
\arrow[l,""]
\arrow[u, "\phi_{B_2}"]
\end{tikzcd}
\end{figure}
\newpar
Sei $b : V \times V \to k$ eine Bilinearform. \ul{Erinnerung:} Die Matrix $\Mat{}{B_1}{b}$ ist eindeutig dadurch festgelegt, dass $\forall \vx \to \vy \in V : b(\vx, \vy) = \phi_{B_1}(\vx) \cdot \Mat{}{B_1}{b} \cdot \phi_{B_2}(\vy)$. Ebenso $\Mat{}{B_2}{b}$. Also:
\begin{align*}
 \forall \vv \in V: \phi_{B_2}(\vv) &= S \phi_{B_1}(\vv)\\
 &= (S \phi_{B_1}(\vx))^\top \Mat{}{B_2}{b} S \phi_{B_1}(\vy))
\end{align*}
Also gilt 
\begin{align*}
 \Mat{}{B_1}{b} = S^\top \Mat{}{B_2}{b} S
\end{align*}
Dies erinnert natürlich stark an Ähnlichkeit von Darstellungsmatrizen von Endomorphismen, allerdings wird das Inverse durch eine Transponierte ersetzt! Das ist natürlich einfacher als das Invertieren, Basiswechsel von Bilinearformen ist dementsprechend einfacher als Basiswechsel von Endomorphismen.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\section{Hermitische Sesquilinearformen}
\ul{\tbf{Problem:}} Reelle Vektorräume sind kompliziert (Das Problem ``Wie viele reelle Nullstellen hat ein Polynom'' ist NP-Vollständig!!!) Vieles wird einfacher, wenn man über den komplexen Zahlen arbeitet. 
\newpar
Allerdings konnten wir über $\bC$ bisher zum Beispiel nicht die positive Definitheit definieren. Wir wollen nun die Definition von Bilinearformen und Symmetrie ändern, damit sie sich besser mit den komplexen Zahlen vertragen.
\begin{definition}
 Sei $V$ ein $\bC$-Vektorraum. Eine Sesquilinearform ist eine Abbildung $b : V \times V \to \bC$, sodass gilt:
 \begin{enumerate}
  \item Linearität in der ersten Komponente
  \item Sesquilinearität in der zweiten Komponente:
  \begin{align*}
   \forall \vec{a}, \vec{b}, \vc \in V : \forall \lambda \in \bC : b(\vec{a}, \vec{b} + \lambda \vc) = b(\vec{a}, \vec{b}) + \overline{\lambda}b(\vec{a}, \vc)
  \end{align*}
  (Der skalare Faktor $\lambda$ wird also komplex konjugiert.)
 \end{enumerate}
\end{definition}
\begin{beispiel}
 Die Standardsesquilinearform auf $\bC^n$:
 \begin{align*}
  \scalar{\_}{\_}  : \bC^n \times \bC^n &\to \bC\\
  (\vx, \vy) &\to \sum x_i \overline{y_i}\\
 \end{align*}
\end{beispiel}
\begin{beispiel}
 Sei $A \in \Matnn$. Dann betrachte:
 \begin{align*}
  b : \bC^n \times \bC^n &\to \bC\\
  (\vx, \vy) &\to \vx^\top A \overline{\vy}\\
 \end{align*}
\end{beispiel}
\begin{definition}
 Eine Sesquilinearform heißt \tbf{Hermitsch}, falls:
 \begin{align*}
  \forall \vx, \vy \in V : b(\vx, \vy) = \overline{b(\vy, \vx)}
 \end{align*}
\end{definition}
\begin{proposition}
 Eine Sesquilinearform ist genau dann Hermitsch, wenn $A^\top = \overline{A}$.
\end{proposition}
\begin{beobachtung}
 In einer darstellenden Matrix einer hermitschen Sesquilinearform müssen also die Diagonalelemente reell sein!
\end{beobachtung}
\begin{beobachtung}
 Wenn $b : V \times V \to \bC$ hermitsch ist, so gilt:
 \begin{align*}
  b(\vx, \vx) = \overline{b(\vx, \vx)}
 \end{align*}
 Also ist das Bild eines Vektors mit sich selbst reell - also haben wir für komplexe Sesquilinearformen nicht das Problem der Definition von Definitheit, welches wir für Bilinearformen hatten!
\end{beobachtung}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\section{Unitäre und Euklidische Vektorräume}
\begin{definition}
 Ein \tbf{Euklidischer Vektorraum} ist ein $\bR$-Vektorraum mit Skalarprodukt. Ein \tbf{Unitärer Vektorraum} ist ein $\bC$-Vektorraum mit Skalarprodukt.
\end{definition}
\begin{definition}
 Sei $V$ ein $k$-Vektorraum, wobei $k \subset \bC$ (typischerweise $\bQ, \bR$ oder $\bC$). Eine \tbf{Norm} auf $V$ ist eine Abbildung $\norm{\_,\_} : V \times V \to k$ mit folgenden Eigenschaften:
 \begin{enumerate}
  \item Absolute Homogenität:
  \begin{align*}
   \forall \vv \in V : \forall \lambda \in k : \norm{\lambda \cdot \vv} = \abs{\lambda} \cdot \norm{\vv}
  \end{align*}
  \item Dreiecksungleichung:
  \begin{align*}
   \norm{\vx + \vy} \leq \norm{\vx} + \norm{\vy}
  \end{align*}
  \item Positive Definitheit\footnote{Hier anders formuliert als für allgemeine Bilinearformen, denn Nichtgegativität folgt bereits aus Dreiecksungleichung}:
  \begin{align*}
   \norm{\vx} = 0 \Leftrightarrow \vx = \vz
  \end{align*}
 \end{enumerate}
\end{definition}
\begin{beispiel}
 Sei $V$ ein Euklidischer oder Unitärer Vektorraum mit Skalarprodukt $\scalar{\_}{\_}$. Dann gelten:
 \begin{enumerate}
  \item Cauchy-Schwarz:
  \begin{align*}
   \forall \vx : \forall \vy : \abs{\scalar{\vx}{\vy}} \leq \sqrt{\scalar{\vx}{\vx}} \sqrt{\scalar{\vy}{\vy}}
  \end{align*}
  \item Satz des Pythagoras:
  \begin{align*}
   \forall \vx : \forall \vy : \norm{\vx + \vy}^2 = \norm{\vx}^2 + 2 \Re\scalar{\vx}{\vy} + \norm{\vy}^2
  \end{align*}
 \end{enumerate}
\end{beispiel}
\begin{beispiel}
 \tbf{\ul{Operatornorm:}}
 Seien $V$, $W$ zwei endlichdimensionale $k$-Vektorräume mit Normen $\norm{}_V$ und $\norm{}_W$. Dann definieren folgende Norm auf den linearen Abbildungen $Hom(V,W)$:
 \begin{align*}
  \norm{}_{op} : Hom(V,W) &\to \bR\\
  f &\to \max \{\norm{f(\vv)}_W\}\\
 \end{align*}
\end{beispiel}
\newpar
 \ul{Nachrechnen:} Diese Operatornorm ist tatsächlich eine Norm, insbesondere kann tatsächlich von einem Maximum statt einem Supremum gesprochen werden.
 \newpar
 \ul{Spezialfall}: $W = k$. Dann ist $Hom(V,W) = V^*$ der \tbf{Dualraum}. Durch die Operatornorm erhält man aus einer beliebigen Norm auf $V$ eine Norm auf dem Dualraum.
 \begin{beispiel}
 \tbf{\ul{``Mannheim-Norm:''}}
 \begin{align*}
  \norm{}_{0} : \bR^n &\to \bR\\
  X &\to \sum_{i} \abs{x_i}
 \end{align*}
 \end{beispiel}
  \begin{beispiel}
 \tbf{\ul{Maximumsnorm:}}
 \begin{align*}
  \norm{}_{\infty} : \mathcal{C}([0,1], \bR) &\to \bR\\
  f &\to \max \{|f(z)|\}
 \end{align*}
 \end{beispiel}
 \newpar
 \ul{Provokation:} Die Beispiele fügen sich in zwei unendliche Reihen - eine für Summen und eine für Integrale - ein. Es existieren also zwei Normen $\norm{}_n$ für jedes $n \in \bN$.
 \newpar
 \ul{Frage:} Kommt die Mannheim-Norm von einem Skalarprodukt?
 \begin{beispiel}
  Sei $V$ ein Vektorraum und $\norm{} : V \to \bR$ eine Norm. Sei $W$ ein Untervektorraum von $V$. Dann ist die Einschränkung $\norm{}_W : W \to \bR$ wieder eine Norm.
 \end{beispiel}
 \begin{definition}
  Ein \tbf{normierter Vektorraum} ist ein Vektorraum mit einer Norm. Ein Vektor aus einem normiertern Vektorraum heißt normiert, wenn er Norm $1$ hat.
 \end{definition}
\section{Metriken}
\begin{definition}
 Sei $M$ eine Menge. Eine Metrik ist eine Abbildung $d : M \times M \to \bR$, sodass gilt:
 \begin{enumerate}
  \item Symmetrie: $\forall a, b \in M : d(a,b) = d(b,a)$
  \item Dreiecksungleichung: $\forall a,b,c \in M : d(a,c) \leq d(a,b) + d(b,c)$
  \item Positive Definitheit: $d(a,b) \geq 0$ und $d(a,b) = 0 \Leftrightarrow a = b$
 \end{enumerate}
\end{definition}
\begin{beispiel}
 Sei $(V, \norm{}_V)$ ein normierter Vektorraum. Dann ist 
 \begin{align*}
  d : V \times V &\to \bR\\
  (\vx, \vy) &\to \norm{\vx - \vy}
 \end{align*}
 eine Metrik.
\end{beispiel}
\begin{beispiel}
 Sei $\Gamma$ ein zusammenhängender Graph. Dann ist die Länge des kürzesten Weges zwischen zwei Punkten eine Metrik. Die Erdős-Zahl induziert dementsprechend eine Metrik auf der Menge der Mathematiker.
\end{beispiel}
\begin{beispiel}
 Die bescheuerte Metrik (auch bekannt also diskrete Metrik). Sei $M$ eine Menge. Dann ist folgedende Abbildung eine Metrik:
 \begin{align*}
  d : M \times M &\to \bR\\
      (a,b) &\to \begin{cases}
                  0 & a = b\\
                  1 & \text{sonst}\\
                 \end{cases}
 \end{align*}
\end{beispiel}
\section{Orthogonalität}
Sei $(V, \scalar{\_}{\_})$ ein Euklidischer oder Unitärer Vektorraum.
\begin{definition}
 Nenne Vektoren $\vx, \vy$ orthogonal zueinander, wenn $\scalar{\vx}{\vy} = 0$. Wir schreiben dann auch $\vx \bot \vy$.
\end{definition}
\begin{definition}
 Seien $A, B \subset V$ Untervektorräume von $V$. Wir nennen $A$ orthogonal auf $B$, wenn
 \begin{align*}
  \forall \vec{a} \in A, \vec{b} \in B : \scalar{\vec{a}}{\vec{b}} = 0
 \end{align*}
\end{definition}
\begin{definition}
 Sei $A \subset V$ ein Untervektorraum von $V$. Wir definieren:
 \begin{align*}
  A^\bot = \{\vv \in V \mid \forall \vec{a} \in A : \scalar{\vv}{\vec{a}} = 0\}
 \end{align*}
\end{definition}
\newpar
\ul{Achtung:} $A^{\bot}$ ist manchmal auch definiert als $A^\bot = \{f \in V^* \mid A \subset ker(f)\}$. Dies ist nicht die gleiche Menge, allerdings sind diese Räume kanonisch isomorph.
\begin{definition}
 Sei $(\vv_i)_{i \in I}$ eine Familie von Vektoren. Wir nennen die Familie orthogonal, falls $\forall i \neq j : \scalar{\vv_i}{\vv_j} = 0$. Wir nennen die Familie orthonormal, wenn zusätzlich $\forall i : \scalar{\vv_i}{\vv_i} = 1$. Eine orthonormale Basis nennen wir Orthonormalbasis :).
\end{definition}
\begin{theorem}
 Seien $\vv_1, \vv_2 \in V$ und $\vv_1 \neq \vz$. Sei $\vv_2' = \vv_2 - \frac{\scalar{\vv_2}{\vv_1}}{\scalar{\vv_1}{\vv_1}} \cdot \vv_1$. Dann gilt:
 \begin{enumerate}
  \item $\vv_1 \bot \vv_2'$
  \item $\{\vv_1, \vv_2'\}$ ist linear unabhängig genau dann, wenn $\{\vv_1, \vv_2\}$ linear unabhängig ist.
 \end{enumerate}
 \begin{proof}
  \begin{enumerate}
   \item \begin{align*}
          \scalar{\vv_2'}{\vv_1} &= \scalar{\vv_2 - \frac{\scalar{\vv_2}{\vv_1}}{\scalar{\vv_1}{\vv_1}} \cdot \vv_1}{\vv_1}\\
          &= \scalar{\vv_2}{\vv_1} - \scalar{\frac{\scalar{\vv_2}{\vv_1}}{\scalar{\vv_1}{\vv_1}} \cdot\vv_1}{\vv_1}\\
          &= \scalar{\vv_2}{\vv_1} - \frac{\scalar{\vv_2}{\vv_1}}{\scalar{\vv_1}{\vv_1}} \cdot \scalar{\vv_1}{\vv_1}\\
          &= \scalar{\vv_2}{\vv_1} - \scalar{\vv_2}{\vv_1}\\
          &= 0\\
         \end{align*}
  \end{enumerate}
 \end{proof}
\end{theorem}
\begin{theorem}
 Sei $V$ ein euklidischer oder unitärer Vektorraum und $U \subseteq V$ ein Untervektorraum. Dann lässt sich jede Orthonormalbasis $\{\vu_1, \hdots, \vu_m\}$ von $U$ zu einer Orthonormalbasis $\{\vu_1, \hdots, \vu_m, \vu_{m+1}, \hdots, \vu_{n}\}$ von $V$ ergänzen.
\end{theorem}
\begin{proof}
 Absteigende Induktion über die Dimension von $U$.
 \begin{enumerate}
  \item \ul{Induktionsbasis:}\\
  Sei $\dim U = \dim V$. Da $U \subseteq V$ und da alle $n$-Dimensionalen $k$-Vektorräume isomorph sind gilt dann auch $U = V$, also ist jede Orthonormalbasis von $U$ bereits eine Orthonormalbasis von $V$.
  \item \ul{Induktionsschritt:}\\
  Sei $\dim U < \dim V$. Dann existiert ein Vektor $\vv \in V \setminus U$. Da $\vz \in U$ ist $\vz \notin V \setminus U$, also $\vv \neq \vz$. Setze nun \footnote{Wir dividieren nicht separat durch $\scalar{\vu_i}{\vu_i}$, da die Vektoren $\vu_i$ bereits normal sind, also $\scalar{\vu_i}{\vu_i} = 1$.}
  \begin{align*}
   \vv' = \vv - \sum_{i=1}^m \scalar{\vv}{\vu_i} \cdot \vu_i
  \end{align*}
  So gilt für jeden Index $j$:
  \begin{align*}
   \scalar{\vv'}{\vu_j} 
   &= \scalar{\vv- \sum_{i=1}^m \scalar{\vv}{\vu_i} \cdot \vu_i}{\vu_j}\\
   &= \scalar{\vv}{\vu_j} - \sum_{i=1}^m \scalar{\vv}{\vu_i} \scalar{\vu_i}{\vu_j}\\
   &= \scalar{\vv}{\vu_j} - \sum_{i=1}^m \scalar{\vv}{\vu_i} \delta_{ij}\\
   &= \scalar{\vv}{\vu_j} - \scalar{\vv}{\vu_j}\\
   &= 0\\
  \end{align*}
  Setze nun $\vu_{m+1} := \frac{\vv'}{\norm{\vv'}}$. So gilt:
  \begin{enumerate}
  \item[(i)] $\forall j \neq {m+1} : \vu_{m+1} \top \vu_j$
  \begin{align*}
   \scalar{\vu_{m+1}}{\vu_j} = \frac{1}{\norm{\vv'}} \scalar{\vv'}{\vu_j} = 0
  \end{align*}
  \item[(ii)] $\norm{\vu_{m+1}} = 1$
  \begin{align*}
   \norm{\vu_{m+1}} 
   &= \norm{\frac{\vv'}{\norm{\vv'}}}\\
   &= \norm{\frac{1}{\norm{\vv'}} \cdot \vv'}\\
   &= \abs{\frac{1}{\norm{\vv'}}} \cdot \norm{\vv'}\\
   &= \frac{1}{\norm{\vv'}} \cdot \norm{\vv'}\\
   &= 1\\
  \end{align*}
  \end{enumerate}
  
 \end{enumerate}
\end{proof}
\begin{corollary}
 Jeder endlichdimensionale euklidische / unitäre Vektorraum besitzt eine Orthonormalbasis.
\end{corollary}
\begin{definition}
 Dieses Verfahren lässt sich zu einem Algorithmus erweitern, welcher aus einer beliebigen Basis $(v_i)$ eine Orthonormalbasis $(u_i)$ konstruiert. Dieser ist bekannt als das \tbf{Gram-Schmidt-Verfahren}.
 \begin{enumerate}
  \item Initialisierung:
  \begin{itemize}
   \item Normiere den Ersten Vektor $\vu_1 = \frac{\vv_1}{\norm{\vv_1}}$
  \end{itemize}
  \item Seien $\vu_1, \hdots, \vu_k$ bereits konstruiert.
  \begin{itemize}
   \item Definiere $\vv_{k+1}' = \vv_{k+1} - \sum_{i = 1}^k \scalar{\vv_{k+1}}{\vu_i} \cdot \vu_i$
   \item Normiere $\vu_{k+1} = \frac{\vv_{k+1}'}{\norm{\vu_{k+1}'}}$
  \end{itemize}
 \end{enumerate}
\end{definition}
\begin{theorem}\emph{\ul{\tbf{Orthogonale Zerlegung}}}:
 Seien $V$ ein endlichdimensionaler unitärer/euklidischer Vektorräume mit $U \subseteq V$. Dann lässt sich jedes $\vv \in V$ eindeutig schreiben als 
 \begin{align*}
  \vv = p(\vv) + r(\vv) \qquad p(\vv) \in U, r(\vv) \in U^{\bot},
 \end{align*}
 es gilt also $V = U \oplus U^\bot$.
\end{theorem}
\begin{proof}
 Angenommen $\vv = p_1(\vv) + r_1(\vv) = p_2(\vv) + r_2(\vv)$. Sei 
 \begin{align*}
  \vw = p_1(\vv) - p_2(\vv) = r_2(\vv) - r_1(\vv)
 \end{align*}
 Also $\vw = p_1(\vv) - p_2(\vv) \in U$ und $\vw = r_2(\vv) - r_1(\vv) \in U^\bot$, also $\scalar{\vw}{\vw} = 0$, also $\vw = 0$, also $p_1(\vv) = p_2(\vv)$ und $r_1(\vv) = r_2(\vv)$. Also ist die Darstellung eindeutig.
 \newpar
 Sei $\vv \in V$ und $(u_i)$ eine Orthonormalbasis von $U$. Definiere 
 \begin{align*}
  r(\vv) = \vv - \sum_{i=1}^m \scalar{\vv}{\vu_i} \cdot \vu_i
 \end{align*}
 Für jeden Index $j$ gilt $\scalar{r(\vv)}{\vu_j} = 0$, also $r(\vv) = U^\bot$. Definiere nun zusätzlich
 \begin{align*}
  p(\vv) = \vv - r(\vv) = \sum_{i = 1}^m \scalar{\vv}{\vu_i} \cdot \vu_i \in U
 \end{align*}
 Somit haben wir die gewünschte Zerlegung gefunden.
\end{proof}
\begin{anmerkung}
 Die Abbildung $p$ ist linear, wir nennen sie die \tbf{orthogonale Projektion auf $U$}.
\end{anmerkung}
\begin{beobachtung}
 Die Orthogonale Projektion minimiert den Abstand - $p(\vv)$ ist der nächste Punkt aus $U$ zu $V$ bezüglich der durch das Skalarprodukt induzierte Norm.
\end{beobachtung}
\begin{proof}
 \begin{align*}
  \norm{\vw - \vv} &= \norm{p(\vv) + \vw - p(\vv) - \vv}\\
  \implies \norm{\vw - \vv}^2 &= \norm{p(\vv) + \vw - p(\vv) - \vv}^2\\
  &= \norm{p(\vv) - \vv}^2 + \norm{\vw}^2 + \scalar{p(\vv) - \vv}{\vw - p(\vv)} + \scalar{\vw'}{p(\vv) - \vv}\\
  &\geq \norm{p(\vv) - \vv}^2
 \end{align*}
\end{proof}

%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{theorem}
 Eigenschaften des Orthogonalen Komplements:
 \begin{enumerate}
  \item Es gilt $U^\bot \cong U^0 := \{f \in V^* (= \textnormal{Hom}(V,k)) \mid U \subseteq \ker(f)\}$.
  
  Genauer: Sei $s : V \to V^*, \vv \to \scalar{\underline{\hphantom{v}}}{\vv}$. Dann gilt $s(U^\bot) = U^0$ und umgekehrt für die Inverse $s^{-1}$.
  \item Es gilt $\dim U^\bot = \dim V - \dim U$
  \item Es ist $V = U \oplus U^\bot$ (Also $U \cap U^\bot = \{0\}$ und $V = U + U^\bot$)
  \item Es gilt $U = (U^\bot)^\bot$
 \end{enumerate}
\end{theorem}
\begin{proof}
 \hphantom{v}
 \begin{enumerate}
  \item
  \item
  \item Sei $v \in U \cap U^\bot$. Weil $v \in U^\bot$ ist, gilt $\forall u \in U : \scalar{v}{u} = 0$. Insbesondere gilt $\scalar{\vv}{\vv} = 0$, also $\vv = \vz$.
 \newpar
 Nach Dimensionalsformel dilt $\dim U + U^\bot = \dim U + \dim U^\bot - \dim (U \cap U^\bot)$, also $\dim U + \dim U^\bot = \dim U + (\dim V - \dim U) + 0 = \dim V$, also $\dim U + \dim U^\bot = \dim V$, also $U + U^\bot = V$.
   \item Es gilt $\dim (U^\top)^\top = \dim V - \dim U^\bot = \dim V - (\dim V - \dim U) = \dim U$. Es gilt zusätzlich $\forall \vu \in (U^\bot)^\bot : \forall \vv \in U^\bot :\vu \bot \vv$, also $\vu \in V$.
 \end{enumerate} 
\end{proof}
\begin{theorem}
 Es gibt eine kanonische Identifikation, also einen kanonischen Isomorphismus, zwischen dem Quotientenvektorraum $V / U$ und dem Orthogonalraum $U^\bot$.
\end{theorem}
\begin{proof}
 Wir vergleichen die Quotientenabbildung $q : V \to V / U$ mit der orthogonalen Projektion $\pi : V \to U^\bot$. Wir haben folgendes kommutatives Diagramm:
 \begin{center}
 \begin{tikzcd}[row sep = huge, column sep = huge]
    V
    \arrow[r, "q"]
    \arrow[d, "id"]
    & V / U
    \arrow[d, "\eta"] \\
    V
    \arrow[r, "\pi"]
    &  U^\bot
\end{tikzcd}
\end{center}
\end{proof}
\hspace{-20pt}
Wobei $\eta$ gemäß universeller Eigenschaft eindeutig ist. Wir wissen, dass $\pi$ surjektiv ist, also ist $\pi$ auch injektiv. Also ist $id \circ \pi$ bijektiv, dementsprechend ist auch $q \circ \eta$ bijektiv, also ist insbesondere $\eta$ bijektiv.
%
%
%
%
%
%
%
%
%
\section{Die adjungierte Abbildung}
\ul{Situation:} Seien $(V, \scalar{\cdot}{\cdot}_V)$ und $(w, \scalar{\circ}{\circ})$ endlichdimensionale Euklidische Vektorräume mit Identifikationsabbildungen $s_v : V \to V^*$ und $s_w : W \to W^*$.
\newpar
\ul{Problem:} Sei $f : V \to W$ linear mit zugehöriger Rückzugsabbildung:
\begin{align*}
 f^* : W^* &\to V^*\\
 (\varphi : W \to \bR) &\to \varphi \circ f
\end{align*}
Wir suchen nun eine Abbildung $f^{ad}$, durch die folgendes Diagramm kommutiert:
\begin{center}
 \begin{tikzcd}[row sep = huge, column sep = huge]
    V
    \arrow[d, "s_v"]
    & W
    \arrow[l, "f^{ad}"]
    \arrow[d, "s_w"] \\
    V^*
    &  W^*
    \arrow[l, "f^*"]\\
\end{tikzcd}
\end{center}
\begin{theorem}
Für die adjungierte Abbildung $f^{ad}$ gilt:
 \begin{align*}
 \forall \vv \in V, \vw \in W : \scalar{f(\vv)}{\vw}_W = \scalar{\vv}{f^{ad}(\vw)}_V
\end{align*}
\end{theorem}
\begin{proof}
 Per Definition ist:
 \begin{align*}
  f^{ad} = s_v^{-1}  \circ f^* \circ s_w\\
  \Leftrightarrow s_v \circ f^{ad} = f^* \circ s_w\\
 \end{align*}
 Dies ist eine Gleichheit von Abbildungen $W \to V^*$. Sei also ein $\vw \in W$ gegebben. Dann ist
 \begin{align*}
  s_v \circ f^{ad}(\vw) = f^* \circ s_w(\vw) \in V^*
 \end{align*}
 Also $\forall \vv \in V$:
 \begin{align*}
  s_v \circ f^{ad}(\vw)(\vv) = f^* \circ s_w(\vw)(\vv)
 \end{align*}
 Per Definition von $s_v$ und $s_w$ ist dies genau:
 \begin{align*}
  \scalar{\vv}{f^{ad}(\vw)}_V = (\scalar{\cdot}{\vw}_W \circ f)(\vv) = \scalar{f(\vv)}{\vw}
 \end{align*}
\end{proof}
\begin{proposition}
 Diese Eigenschaft ist eine universelle Eigenschaft, welche $f^{ad}$ eindeutig festlegt.
\end{proposition}
\begin{proof}
 Wenn $f^{ad}$ und $f^{ad}_2$ Abbildungen sind, welche die Identität erfüllen, gilt 
 \begin{align*}
  \scalar{\vv}{f^{ad}(\vw)} = \scalar{f(\vv)}{\vw} = \scalar{\vw}{f^{ad}_2(\vw)}
 \end{align*}
 \ul{Erinnerung:} Wenn $(\vv_i)$ eine Orthonormalbasis ist, dann gilt $\forall \vv \in V : \vv = \sum \scalar{\vv}{\vv_i} \vv_i$. Wenn also für $\vv, \vv' \in V$ $\forall i : \scalar{\vv}{\vv_i} = \scalar{\vv'}{\vv_i}$, dann $\vv = \vv'$. Per Definition der adjungierten Abbildungen gilt:
 \begin{align*}
  \scalar{\vv_i}{f^{ad}(\vw)} = \scalar{\vv_i}{f^{ad}_2(\vw)}
 \end{align*}
 Per universeller Eigenschaften folgt:
 \begin{align*}
  \scalar{\vv_i}{f^{ad}(\vw)} = \scalar{\vv_i}{f^{ad}_2(\vw)} = \scalar{f(\vv)}{\vw}
 \end{align*}
 Also $f^{ad}$ = $f^{ad}_2$.
\end{proof}
%
%
%
%
%
%
\clearpage
[...]
\clearpage
\chapter{Orthogonale und unitäre Endomorphismen}
\begin{definition}
 Sei $V$ ein endlichdimensionaler euklidischer (bzw. unitärer) Vektorraum. Wir nennen einen Endomorphismus $f : V \to V$ euklidisch (bzw. unitär), falls für alle $\vv, \vw in V$ folgende Gleichung gilt:
 \begin{align*}
    \scalar{f(\vv)}{f(\vw)} = \scalar{\vv}{\vw}
 \end{align*}
 \end{definition}
 \begin{lemma}
  Ein Endomorphismus $f : V \to W$ ist orthogonal genau dann, wenn für alle $\vv \in V$ folgende Gleichung gilt:
  \begin{align*}
   \norm{f(\vv)} = \vv
  \end{align*}
 \end{lemma}
 \begin{theorem}
  Sei $f : V \to V$ orthogonal (bzw. unitär). Dann gilt folgendes:
  \begin{enumerate}
   \item Für alle Eigenwerte $\lambda$ von $f$ gilt $\abs{\lambda} = 1$.
   \item $\abs{\det f} = 1$
   \item Für alle $\vv, \vw \in V$ gilt:
   \begin{align*}
    \vv \bot \vw \Leftrightarrow f(\vv) \bot f(\vw)
   \end{align*}
   \item $f$ ist injektiv
   \item $f$ ist ein isomorphismus mit orthogonaler (bzw. unitärer) Umkehrabbildung $f^{-1}$
  \end{enumerate}
 \end{theorem}
 \begin{corollary}
  Es sei $(V, \scalar{\_}{\_})$ ein endlichdimensionaler euklidischer (bzw. unitärer) Vektorraum. Dann bilden die orthogonalen (bzw. unitären) Abbildungen eine Untergruppe von $\text{Aut}(V)$. Wir nennen diese Gruppe die orthogonale Gruppe (bzw. unitäre Gruppe) von $V$.
 \end{corollary}


\section{Normalformen unitärer Endomorphismen}
\begin{theorem}
 Sei $V$ ein endlichdimensionaler euklidischer Vektorraum und $f : V \to V$ orthogonal. Dann gibt es eine angeordnete orthonormalbasis $\cB$, sodass
 \begin{align*}
  \Mat{\cB}{\cB}{f} = \begin{pmatrix}
                       \Id_{a \times a} &  &  & \hdots & 0\\
                        & \Id_{b \times b} &  & & \vdots\\
                        &  & A_1 &  & \\
                       \vdots &  &  & \ddots & \\
                       0 & \hdots &   &  & A_k\\
                      \end{pmatrix}
 \end{align*}
 Wobei $A_1, \hdots, A_k \in \SO_2$ (also zweidimensionale Drehmatrizen).
\end{theorem}
\begin{proof}
 Folgt nach der nächsten Sektion zur Komplexifizierung, welche zur Vorbereitung des Beweises dient.
\end{proof}
\subsection{Komplexifizierung}
Sei $V$ ein endlichdimensionaler reeller Vektorraum. Wir konstruieren einen komplexen Vektorraum $V^\bC$ wie folgt: Wir betrachten die Menge $V \times V$, mit folgenden Operationen:
\begin{align*}
 + :  (V \times V) \times (V \times V) &\to (V \times V)\\
 ((\vv_1,\vw_1),(\vv_2, \vw_2)) &\to (\vv_1 + \vv_2, \vw_1 + \vw_2)\\
\end{align*}
\begin{align*}
 \cdot : \bC \times (V \times V) &\to (V \times V)\\
 ((a + ib), (\vv, \vw)) &\to (a\vv - b\vw, b\vv + a\vw)\\
\end{align*}
Wir schreiben die Elemente $\vv, \vw \in V^\bC$ suggestiv direkt als $\vv + i\vw$. Wir definieren zusätzlich die komplexe Konjugation:
\begin{align*}
 \overline{\cdot} : V \times V &\to V \times V\\
                    (\vv, \vw) &\to (\vv, - \vw)
\end{align*}
Und die kanonische Inklusion:
\begin{align*}
 \cdot^\bC : V &\to V \times V\\
                    \vv &\to (\vv, 0)
\end{align*}
Mithilfe der Abbildung $\cdot^\bC$ fassen wir $V$ als Teilmenge von $V^\bC$ auf. Gegeben $\vv \in V$ nennen wir $\vv^\bC \in V^\bC$ den komplexifizierten Vektor.
\begin{lemma}
 Gegeben eine Basis $\cB$ des $\bR$-Vektorraums $V$ ist die komplexifizierte Basis $\cB^\bC$ eine Basis des $\bC$-Vektorraums $V^\bC$.
\end{lemma}
\noindent
Wir können nun lineare Abbildungen $f$ durch folgende Vorschrift komplexifizieren:
\begin{align*}
 f^\bC(\bb^\bC) = (f(\bb))^\bC
\end{align*}
\begin{lemma}
 Sei $f : V \to V$ orthogonal. Dann gibt es einen Untervektorraum $U \subseteq V$, sodass $\dim U \in \{1,2\}$ und $f(U) \subseteq U$.
\end{lemma}
\begin{proof}
 Angenommen, $f$ hat einen reellen Eigenwert $\lambda \in \bR$. So ist der gesuchte Raum durch den Raum gegeben, der durch einen dazugehörigen Eigenvektor aufgespannt wird.
 \newpar
 Falls kein reeller Eigenwert existiert, muss nach dem Fundamentalsatz der Analysis ein komplexer Eigenwert $(a + bi) := \lambda \in \bC \setminus \bR$ existieren. So ist $\overline{\lambda}$ ebenfalls eine Nullstelle des charakteristischen Polynoms, also ebenfalls ein Eigenwert:
 \begin{align*}
  \chi_f(\overline{\lambda}) &= \sum a_i \cdot \overline{\lambda}^i\\
                             &= \sum a_i \cdot \overline{\lambda^i}\\
                             &= \sum \overline{a_i} \cdot \overline{\lambda^i} \qquad (a_i \in \bR)\\
                             &= \sum \overline{a_i \cdot \lambda^i}\\
                             &= \overline{\chi_f(\lambda)}\\
                             &= 0
 \end{align*}
 Wir wissen, dass $f^\bC$ einen komplexen Eigenvektor $\vv = (\vv_1, \vv_2)$ zum Eigenwert $\lambda \in \bC$ haben muss. Es gilt $f(\overline{\vv}) = \overline{f(\vv)}$, also ist $\overline{\vv}$ ein Eigenvektor zum Eigenwert $\overline{\lambda}$. $\vv$ und $\overline{\vv}$ sind Eigenvektoren zu unterschiedlichen Eigenwerten, also linear unabhängig. Es folgt, dass folgende Menge ebenfalls unabhängig ist:
 \begin{align*}
  \left\{\frac{1}{2}(\vv + \overline{\vv}), \frac{i}{2}(\vv - \overline{\vv})\right\} = \{(\vv_1,0),(\vv_2,0)\}
 \end{align*}
 Also ist $U = \text{span}(\vv_1, \vv_2)$ ein zweidimensionaler reeller Untervektorraum von $V$ mit den gewünschten Eigenschaften.
\end{proof}
\noindent
Da orthogonale Abbildungen isomorphismen sind gilt sogar $f(U) = U$. Es folgt, dass $f|_U : U \to U$ ebenfalls orthogonal ist.
\clearpage
[...]
\clearpage
\begin{theorem}
 THEOREM
\end{theorem}
\begin{proof}
 Per Induktion über $n := \dim V$.
 \begin{enumerate}
  \item[(i)] Induktionsbasis: $n = 1$. In diesem Fall ist $f = \pm \Id_V$, also sind wir fertig durch Wahl einer beliebigen Orthonormalbasis (wobei ohnehin nur $\{+1\}$ oder $\{-1\}$ als Basis in Frage kommen)
  \item[(ii)] Induktionsschritt: Sei $n > 1$ und sei die Behauptung für alle orthonormalen Endomorphismen von Vektorräumen kleinerer Dimensionen schon bewiesen.
  \newpar
  \ul{Fall 1}: $f$ hat einen reellen Eigenwert $\lambda$. Sei $\vu \in V$ ein Eigenvektor und sei $U = \langle \vu \rangle$. Dann gilt $f(U) = U$ und $f(U^\bot) = U^\bot$, $U^\bot$ hat Dimension $n - 1$. Jetzt ist $f \mid_{U^\bot}$ ein orthogonaler Endomorphismus von $U^\bot$, also gibt es per Induktionsannahme eine Orthonormalbasis $B = \vv_1, \hdots, \vv_{n-1} \in U^\bot$, sodass die Aussage gilt. Dann ist $B \cup \frac{1}{\norm{\vu}}\vu$ eine Orthonormalbasis von $V$. Nach Umsortieren ist es eine Basis mit den gesuchten Eigenschaften.
  \newpar
  \ul{Fall 2}: $f$ hat keinen reellen Eigenwert. Nach Vorbereitung 2 gibt es einen Untervektorraum $U \subseteq V$ mit $f(U) = U$ und $\dim U \in \{1,2\}$. Da $f$ keinen rellen Eigenwert hat, ist $\dim U \neq 1$, also $\dim U = 2$. Desweiteren kann $f \mid_U$ keine Spiegelung sein, sonst gäbe es ebenfalls einen reellen Eigenwert, also ist $f \mid_U$ eine Drehung ($\det (f \mid_U) = 1$) und für eine ONB $B_1 = \{\vv_1, \vv_2\}$ ist $\Mat{B_1}{B_1}{f \mid_U} \in \mathcal{SO}_2$. Wählen wir eine Basis $B_2 = (\vw_i)_{i \leq n-2}$ von $U^\bot$, so ist nach Induktionsannahme $B = B_1 \cup B_2$ eine Orthonormalbasis mit den gesuchten Eigenschaften.
 \end{enumerate}
\end{proof}
\section{Selbstadjungierte Endomorphismen}
\begin{definition}
 Ein Endomorphismus $f : V \to V$ heißt selbstadjungiert, wenn $f = f^{ad}$, also:
 \begin{align*}
  \scalar{f(\vv)}{\vw} = \scalar{\vv}{f(\vw)}
 \end{align*}
\end{definition}
\begin{beispiel}
 Sei $V = \bR^n$ mit Standardskalarprodukt, $A \in \Matnn$, $f : \vv \to A \vv$. Die Abbildung $f$ ist selbstadjungiert gdw:
 \begin{align*}
  \scalar{f(\vv)}{\vw} = \scalar{\vv}{f(\vw)}
  &\Leftrightarrow \scalar{A\vv}{\vw} = \scalar{\vv}{A \vw}\\
  &\Leftrightarrow (A\vv)^\top \vw = \vv^\top A \vw\\
  &\Leftrightarrow \vv^\top \cdot A^\top \cdot \vw = \vv^\top \cdot A \cdot \vw
 \end{align*}
 Durch $\vv, \vw = e_i, e_j$ erhält man direkt $A = A^\top$.
\end{beispiel}
\begin{beispiel}
 Sei $V = \bC^n$ mit Standardskalarprodukt und $A = \text{Mat}(n \times n, \bC)$ mit dazugehörigem Endomorphismus $f$. Dann ist $f$ selbstadjungiert gdw. $A^\top = \overline{A}$, also $A$ Hermitsch.
\end{beispiel}
\begin{proposition}
Sei $B$ eine Orthonormalbasis von $V$. Dann ist $f$ selbstadjungiert genau dann, wenn $\Mat{B}{B}{f}$ symmetrisch oder Hermitsch ist.
\end{proposition}
\begin{theorem}
 Alle Eigenwerte einer selbstadjungierten Abbildung sind reell.
\end{theorem}
\begin{proof}
 Für den Unitären Fall: Sei $f$ selbstadjungiert, $\lambda$ ein Eigenwert und $\vv \in V \setminus \{0\}$ ein zugehöriger Eigenvektor. Dann gilt:
 \begin{align*}
  \scalar{f(\vv)}{\vw} = \scalar{\lambda \vv}{\vv} = \lambda \scalar{\vv}{\vv}\\
  \scalar{\vv}{f(\vv} = \scalar{\vv}{\lambda \vv} = \overline{\lambda} \scalar{\vv}{\vv}\\
 \end{align*}
 Es gilt $\scalar{\vv}{\vv} \neq 0$, also $\lambda = \overline{\lambda}$, also $\lambda \in \bR$.
\end{proof}
\begin{corollary}
 Selbstadjungierte Endomorphismen sind diagonalisierbar.
\end{corollary}
\begin{theorem}
 Für jede selbstadjungierte Abbildung $f$ existiert eine Orthonormalbasis aus Eigenvektoren von $f$.
\end{theorem}
\begin{proof}
 Per Induktion über $n = \dim V$.
 \begin{enumerate}
  \item[(IB)] Sei $\vv \in V$ ein Eigenvektor mit Länge $1$. Dann ist $\{\vv\}$ die gesuchte Basis.
  \item[(IV)] Sei $n > 1$. Sei die Aussage für selbstadjungierte Endomorphismen von Vektorräumen kleinerer Dimension bereits bewiesen. Wir wissen: es existiert ein reeller Eigenwert $\lambda$. Sei $\vu \in V$ ein zugehöriger Eigenvektor der Länge 1 und $U = \langle \vu \rangle$. Sei $\veta$ in $f(U^\bot)$. Wähle $\veta' \in U^\bot$, sodass $\veta = f(\veta')$.  Es gilt:
  \begin{align*}
   \scalar{f(\veta')}{\vu} = \scalar{\veta'}{f(\vu)} = \lambda\scalar{\veta'}{\vu} = 0
  \end{align*}
  Also $\scalar{\eta}{\vu} = 0$, also $\eta \in U^\bot$, also $f(U^\bot) \subseteq U^\bot$.
  \newpar
  \ul{Behauptung:} $f \mid_{U^\bot}$ ist selbstadjungiert. Da $\dim U^\bot < \dim V$ liefert die Induktionsannahme eine Orthonormalbasis $B = (\vv_i)_{i < n}$ von $U^\bot$. Dann ist $\vu \cup B$ eine gesuchte Basis.
 \end{enumerate}
\end{proof}
\begin{proposition}
 Sei $A \in \text{Mat}(n \times n, \bR)$ symmetrisch. Dann gibt es eine orthogonale Matrix $S$, sodass $SAS^{-1}$ diagonal ist.
\end{proposition}
\begin{section}{Hauptachsentransformation}
 \begin{theorem}
 \label{thrm:hauptachsen}
  Sei $V$ ein endlichdimensionaler Vektorraum über $\bR$ oder $\bC$, sei $s : V \times V \to k$ eine symmetrische oder Hermitische Form. Weiter sei $\mc{A}$ eine angeordnete Basis von $V$ und $A = \text{Mat}_\mc{A}(s)$ die dazugehörige Matrix. Dann gibt es eine Basis $\mc{B} \subset V$, sodass:
  \begin{enumerate}
   \item Die Matrix $B = \text{Mat}_\mc{B}(s)$ diagonal ist,
   \item Die Koordinatenwechselmatrix $S = \Mat{\mc{B}}{\mc{A}}{\text{Id}_V}$ orthogonal oder unitär ist,
   \item Die Einträge der Diagonalmatrix $B$ relle Eigenwerte von $A$ sind.
  \end{enumerate}
 \end{theorem}
 \begin{definition}
   In dieser Situation nennen wir die Vektoren aus $\mc{B}$ die \tbf{Hauptachsen von $s$} und den Basiswechsel von $\mc{A}$ zu $\mc{B}$ die \tbf{Hauptachsentransformation}.
 \end{definition}
 \begin{theorem}
  In der Situation von Satz \ref{hauptachsen} sind folgende Aussagen äquivalent:
  \begin{enumerate}
   \item Die Form $s$ ist positiv definit.
   \item Alle Eigenwerte von $A$ sind größer als Null.
  \end{enumerate}
 \end{theorem}
 \begin{subsection}{Anwendung - Rotation von Körpern im Raum}
  Wir betrachten Lineare Bewegung / Translation, also einen Körper mit Masse $m$ der sich mit einer konstanten Geschwindigkeit $\vv$ bewegt. Der Impuls ist gegeben durch $\vp = m \cdot \vv$. Es stellt sich heraus, dass jeder Begriff der linearen Bewegung einen Analogen Begriff in der Rotation hat, z.B. Impuls $\leftrightarrow$ Drehimpuls - Bei symmetrischer Massenverteilung ist der Drehimpuls gegeben durch Trägheitsmoment $\cdot$ Drehgeschwindigkeit. Im Allgemeinen ist die Formel etwas komplizierter (Drehimpuls = Trägheitstensor $\cdot$ Drehgeschwindigkeit):
  \begin{align*}
   \vec{L} = \Theta \cdot \vec{\omega}
  \end{align*}
  Wobei die Richtung von $\vec{\omega}$ die Rotationsachse ist und die Länge der Rotationsgeschwindigkeit entspricht. In dieser spezifischen Situation ist der Trägheitstensor $\Theta$ einfach eine $3 \times 3$-Matrix - spezifischer handelt es sich um eine \textit{symmetrische} Matrix! Eine Beobachtung: Drehimpuls und Drehgeschwindigkeit zeigen genau dann in die selbe Richtung, wenn die Drehachse ein Eigenvektor des Trägheitstensors ist! Der sylvestersche Trägheitssatz sagt uns nun, dass es genau drei Richtungen gibt, in denen man ohne Umwucht drehen kann.
 \end{subsection}
 \begin{subsection}{Anwendung - Kovarianz}
  Untersucht man gleichzeitig Zufallsvariablen $X_1, \hdots, X_n$, so bilden die Kovarianzen der Zufallsvariablen eine symmetrische \tbf{Kovarianzmatrix}:
  \begin{align*}
   A_{ij} = \text{Kov}(i,j), \quad A_{ii} = \text{Var}(i)
  \end{align*}
Da die Matrix symmetrisch ist, existiert eine Orthonormalbasis aus Eigenvektoren. Welche Bedeutung hat ein Eigenvektor der Kovarianzmatrix? Gegeben einen Vektor $\vv = (\lambda_1, \hdots, \lambda_n)^\top$, betrachte die Zufallsvariable $X_{\vv} = \sum \lambda_i X_i$. Zentrale Beobachtung: Die Kovarianzmatrix gibt ebenfalls die Kovarianz solcher Linearkombinationen aus Zufallsvariablen - Gegeben $\vv, \vw \in \bR^n$ gilt 
  \begin{align*}
   \text{Kov}(\vv, \vw) = \vv^\bot A \vw
  \end{align*}
 \end{subsection}
 Die Hauptachsentransformation erlaubt uns nun, das Problem zu reduzieren, indem als Basis die ``Richtungen'' der Kovarianzen genommen werden, und die dazu orthogonalen Vektoren. Daraufhin können dann Achsen, entlang denen die Varianz gering ist, ignoriert werden, ohne die Daten zu sehr zu verfälschen.
\end{section}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\appendix
\chapter{Ausblicke in die Zukunft}
In der Funktionalanalysis werden unendlichdimensionale Vektorräume betrachtet. 
\begin{theorem}
\label{satz:basisfunktionen}
Der Vektorraum aller Funktionen $f: \bR \to \bR$ hat eine Basis.
\end{theorem}
\newpar
Wie sieht diese Basis aus? Es stellt sich heraus, dass der Beweis nur dank Auswahlaxiom funktioniert, und dass sich diese Basis nicht explizit konstruieren lässt. Die Menge der sog. Kroneckerdeltas $\delta_{ij}$ sieht auf den ersten Blick wie ein vielversprechender Kandidat aus:
\begin{equation*}
 \delta_{ij} = \begin{cases}
            1 & i = j\\
            0 & i \neq j\\
           \end{cases}
\end{equation*}
Aber es muss bedacht werden, dass zwingende Bedingung für eine Basis ist, dass sich jeder Vektor nicht nur als Linearkombination der Basisvektoren darstellen lässt, sondern sogar als \tbf{endliche Linearkombination}. Diese Definition der Basis ist auch bekannt als Hamelbasis.
\newpar
Es stellt sich heraus, dass die Hamel-Basis aus \ref{satz:basisfunktionen} nur dank Auswahlaxiom existiert und nicht explizit dargestellt werden kann. Lockern wir den traditionellen Basisbegriff, um zählbar unendliche Linearkombinationen zu erlauben, erhalten wir den Begriff der Schauder-Basis. Die Funktionen $\delta_{ij}$ reichen jedoch immer noch nicht als Schauder-Basis des Raums $f: \bR \to \bR$, sondern nur für den Folgenraum $f: \bN \to \bR$. 
\newpar
Da sich der Begriff der Linearkombination auf keine sinnvolle Weise auf überabzählbare Mengen erweitern lässt bleibt man an diesem Punkt leider stecken, es existiert leider keine explizit angebbare Basis des Raums $f: \bR \to \bR$. :(
\end{document}
