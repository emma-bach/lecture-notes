\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{bm}
%\usepackage{bbm}
%\usepackage{biblatex}
%\addbibresource{bib.bib}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}

\newcommand{\bN}{\mathbb{N}}
\newcommand{\bR}{\mathbb{R}}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{anmerkung}[lemma]{Anmerkung}

\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}
%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
\chapter{Wiederholung}
\section{Notation}
\begin{itemize}
 \item Die Menge aller $n \times m$ Matrizen mit Einträgen aus dem Körper $k$ schreiben wir $Mat(n \times m, k)$
 \item Die Gruppe der Invertierbaren $n \times n$-Matrizen mit Einträgen aus $k$ schreiben wir $Gl_n(k)$
 \item Den Eigenraum einer Abbildung $f$ zum Eigenwert $\lambda$ schreiben wir $Eig_f(\lambda)$
 \item Die $n \times n$-Einheitsmatrix schreiben wir $I_n$ oder $E_n$, die Identitätsabbildung des Vektorraums $V$ schreiben wir $id_V$. Vermutlich werden wir die Unterscheidung zwischen den beiden Begriffen jedoch öfters ignorieren.
 \item Das characteristische Polynom eines Endomorphismus $f$ ist
 \begin{equation}
  \chi_f := det(f - t \cdot id_V).
 \end{equation}
 Analog ist das characteristische Polynom einer $n \times n$-Matrix $A$
 \begin{equation}
  \chi_A := det(A - t \cdot I_n).
 \end{equation}
\end{itemize}
%
%
\section{Determinanten}
\subsection{Axiomatische Beschreibung}
\begin{enumerate}
 \item Die Determinante ist \textbf{multilinear}, also:
 \begin{enumerate}
  \item $det(v_1 + w, v_2, \hdots, v_n) = det(v_1, \hdots, v_n) + det(w, v_2 \hdots, v_n)$, ebenso in den anderen Spalten und für Zeilenvektoren.
  \item $det(cv_1 + w, v_2, \hdots, v_n) = c \cdot det(v_1, \hdots, v_n)$, ebenso in anderen Spalten und für Zeilenvektoren
 \end{enumerate}
 \item Die Determinante ist \textbf{alternierend}: Sind zwei Spalten oder Zeilen gleich, ist die Determinante $0$.
 \item Sie ist \textbf{normiert} durch $det(I_n) := 1$
\end{enumerate}
%
%
\subsection{Weitere Eigenschaften}
\begin{enumerate}

 \item $det(A^\top) = det(A)$
 \item $det(A^{-1}) = \frac{1}{det(A)}$
 \item Für quadratische $A,B$ gleicher Größe gilt $det(AB) = det(A)det(B)$
 \item Für Konstante $c$ und $n \times n$-Matrix $A$ gilt $det(cA) = c^n det(A)$
 \item Für Dreiecksmatrizen $A$ gilt $det(A) = a_{11}a_{22}\hdots a_{nn}$

 \item Besteht eine Spalte oder Zeile aus Nullen, ist die Determinante $0$.
 \item Vertauscht man zwei Spalten oder Zeilen, ändert die Determinante ihr Vorzeichen. Dies ist äquivalent zu ``alternierend'' für Körper ohne selbstinverse Elemente $x = -x \neq 0$.
 \item Addition eines Vielfachen einer Zeile/Spalte zu einer anderen ändert die Determinante nicht.
\end{enumerate}

%
%
\subsection{Berechnung}
Die Determinante einer $2 \times 2$-Matrix ist:
\begin{align*}
 det \begin{pmatrix}
      a & b\\
      c & d
     \end{pmatrix}
    = ad - bc.
\end{align*}
Die Determinante einer $3 \times 3$-Matrix ist:
\begin{align*}
det \begin{pmatrix}
     a & b & c\\
     d & e & f\\
     g & h & i\\
    \end{pmatrix}
    = aei + bfg + cdh - ceg - bdi - afh
\end{align*}
%
%
\section{Basiswechsel}
Zu Beginn will ich einige relevante Sätze, Definitionen und Notationsstandards aus der Vorlesung ``Lineare Algebra I'' wiederholen.
\begin{definition}
Sei $f: V \to W$ eine lineare Abbildung. Sei $A = \{a_1, \hdots, a_n\}$ eine Basis von $V$ und $B = \{b_1, \hdots, b_m\}$ eine Basis von $W$. So lässt sich jeder Vektor $w \in W$ darstellen als \textbf{endliche} Linearkombination der Basisvektoren:
\begin{equation}
    w = \sum_{i=1}^{m} \alpha_i b_i
\end{equation}
Insbesondere lassen sich die Bilder der Basisvektoren $a_j \in A$ in dieser Form darstellen:
\begin{equation}
    f(a_j) = \sum_{i=1}^{m} \alpha_{ij} b_i
\end{equation}
Die \textbf{Darstellungsmatrix} $\text{Mat}_B^A f$ ist genau durch diese Koeffizienten $\alpha_{ij}$ gegeben.
\begin{equation}
    \text{Mat}_B^A f = 
    \begin{pmatrix}
       \alpha_{11} & \hdots & \alpha_{1n}\\
       \vdots      &        & \vdots\\
       \alpha_{m1} & \hdots & \alpha_{mn}\\
    \end{pmatrix}
\end{equation}
\end{definition}
\newpar
In der Regel arbeiten wir mit der Standardbasis $E = \{e_1, \hdots, e_n\}$ und interpretieren jede Matrix $M$ als Darstellungsmatrix $M_E^E f$ einer Linearen Abbildung $f$. 
\begin{definition}
Die \textbf{Basiswechselmatrix} $T_B^A$ ist die Abbildungsmatrix der Identitätsabbildung.
\begin{equation}
 T_B^A = \text{Mat}_B^A (\text{id}_V)
\end{equation}
\end{definition}
\begin{theorem}
Für jede Basis $B$ eines beliebigen Vektorraums $V$ gilt 
\begin{align}
T_B^B = I_n = \begin{pmatrix}
                1 & \hdots & 0\\
                \vdots & \ddots & \vdots\\
                0 & \hdots & 1\\
               \end{pmatrix}
\end{align}
\end{theorem}
%
%
%
\begin{proof}
Da die Darstellung jedes Vektors durch die Basisvektoren eindeutig gegeben ist, ist die Darstellung $id_V(b_j) = b_j = \sum_{i=1}^{m} \alpha_{ij} b_i$ eines Basisvektors als Linearkombination genau gegeben durch die Linearkombination mit $\alpha_{ij} = 1$ und $\alpha_{ik} = 0$ für $k \neq j$. Dies entspricht genau dem Standardbasisvektor $e_j$. Also gilt $T_B^B = (e_1 \hdots e_n) = I_n$.
\end{proof}
%
%
%
\begin{proposition}
\label{prop:basiswechsel}
Gegeben $\text{Mat}_B^A f$ lässt sich die Abbildungsmatrix $\text{Mat}_D^C f$ von $f$ bezüglich zweier neuen Basen $C$ und $D$ durch Nutzung von Basiswechselmatrizen folgendermaßen berechnen:
\begin{equation}
 \text{Mat}_D^C(f) = T_D^B \cdot \text{Mat}_B^A f \cdot T_C^B
\end{equation}
Ein besonders relevanter Spezialfall ist:
\begin{equation}
 \text{Mat}_B^B(f) = T_B^A \cdot \text{Mat}_A^A f \cdot T_A^B
\end{equation}
\end{proposition}
%
\newpar
%
\begin{theorem}
\label{satz:basiswechsel_inverse}
Es gilt $T_B^A$ = $(T_A^B)^{-1}$
\end{theorem}
\begin{proof}
\begin{align*}
T_A^B \cdot T_B^A &= T_A^B \cdot id_V \cdot T_B^A\\
                  &=\text{Mat}_A^B (\text{id}_V) \cdot \text{Mat}_B^B (id_V) \cdot \text{Mat}_B^A (\text{id}_V)\\
                  &= \text{Mat}_A^A (\text{id}_V)\\
                  &= \text{id}_V\\
\end{align*}
\end{proof}
%
\newpar
%
\begin{anmerkung}
Betrachten wir eine beliebige Basis $B = \{b_1, \hdots, b_n\}$. Die Matrix $T_B^E$ kann man trivial finden, da jeder Vektor $b_i = (b_{i1} \hdots b_{in})^T$ bezüglich der Standardbasis trivial geschrieben ist als $b_i = \sum_{i=1}^n b_{ij} \cdot e_i$. Nach \ref{satz:basiswechsel_inverse} lässt sich die Matrix $T_E^B$ ebenfalls ohne größere Probleme durch invertierung von $T_B^E$ finden.
\end{anmerkung}
%
\begin{definition}
 Wir nennen zwei Matrizen $A$ und $B$ \textbf{ähnlich}, falls eine Matrix $S$ existiert, sodass
 \begin{align*}
  A = S^{-1} B S
 \end{align*}
\end{definition}
%
\newpar
%
Ein zentrales Ziel der Vorlesung ``Lineare Algebra II'' ist es, zu einer Matrix $A$ eine besonders simple Matrix $B$ zu finden, welche $A$ ähnlich ist. Hierbei sollte man immer \ref{prop:basiswechsel} im Kopf behalten - zwei Matrizen $A$ und $B$ sind genau dann ähnlich, wenn sie die Darstellungsmatrizen der gleichen Funktion zu verschiedenen Basen sind, also wenn Basen $B_1$ und $B_2$ und ein Endomorphismus $f$ existieren, sodass
\begin{align*}
 A = Mat_{B_1}^{B_1}(f) \text{ und } B = Mat_{B_2}^{B_2}(f)
\end{align*}

%
%
%
%
%
\appendix
\chapter{Ausblicke in die Zukunft}
In der Funktionalanalysis werden unendlichdimensionale Vektorräume betrachtet. 
\begin{theorem}
\label{satz:basisfunktionen}
Der Vektorraum aller Funktionen $f: \bR \to \bR$ hat eine Basis.
\end{theorem}
\newpar
Wie sieht diese Basis aus? Es stellt sich heraus, dass der Beweis nur dank Auswahlaxiom funktioniert, und dass sich diese Basis nicht explizit konstruieren lässt. Die Menge der sog. Kroneckerdeltas $\delta_{ij}$ sieht auf den ersten Blick wie ein vielversprechender Kandidat aus:
\begin{equation*}
 \delta_{ij} = \begin{cases}
            1 & i = j\\
            0 & i \neq j\\
           \end{cases}
\end{equation*}
Aber es muss bedacht werden, dass zwingende Bedingung für eine Basis ist, dass sich jeder Vektor nicht nur als Linearkombination der Basisvektoren darstellen lässt, sondern sogar als \tbf{endliche Linearkombination}. Diese Definition der Basis ist auch bekannt als Hamelbasis.
\newpar
Es stellt sich heraus, dass die Hamel-Basis aus \ref{satz:basisfunktionen} nur dank Auswahlaxiom existiert und nicht explizit dargestellt werden kann. Lockern wir den traditionellen Basisbegriff, um zählbar unendliche Linearkombinationen zu erlauben, erhalten wir den Begriff der Schauder-Basis. Die Funktionen $\delta_{ij}$ reichen jedoch immer noch nicht als Schauder-Basis des Raums $f: \bR \to \bR$, sondern nur für den Folgenraum $f: \bN \to \bR$. 
\newpar
Da sich der Begriff der Linearkombination auf keine sinnvolle Weise auf überabzählbare Mengen erweitern lässt bleibt man an diesem Punkt leider stecken, es existiert leider keine explizit angebbare Basis des Raums $f: \bR \to \bR$. :(
\end{document}
