\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{bm}
%\usepackage{bbm}
%\usepackage{biblatex}
%\addbibresource{bib.bib}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

\newcommand{\tbf}{\textbf}
\newcommand{\argmin}[1] {
    \begin{array}{c}
        \vphantom{#1}\\[-3pt] %vphantom makes sure text is centered vertically, [-{x}pt] decreases vertical space between lines so that they are not unreasonably far apart.
        \text{argmin}\\[-3pt]
        #1\\
        \end{array}
    }
\renewcommand{\min}[1] {
    \begin{array}{c}
        \vphantom{#1}\\[-3pt] %vphantom makes sure text is centered vertically, [-{x}pt] decreases vertical space between lines so that they are not unreasonably far apart.
        \text{min}\\[-3pt]
        #1\\
        \end{array}
    }

\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% yay I love having to write very slightly less :D
\newcommand{\xk}{\bm{x}^{(k)}}
\newcommand{\yk}{\bm{y}^{(k)}}

\newcommand{\mk}{\bm{m}^{(k)}}

\newcommand{\dk}{\bm{d}^{(k)}}
\newcommand{\sk}{\bm{s}^{(k)}}

\newcommand{\tauk}{\tau^{(k)}}
\newcommand{\rhok}{\rho^{(k)}}

\newcommand{\Bk}{B^{(k)}}
\newcommand{\Qk}{Q^{(k)}}

\newcommand{\Jrx}{J_{\bm{r}}(\bm{x})}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}

\renewcommand*\contentsname{Inhalt}


\begin{document}
\include{title}
\tableofcontents
\thispagestyle{fancy}
\chapter{Einführung}
Ein Optimierungsproblem besteht aus einer \tbf{zulässigen Menge} $G$ und einer \tbf{Zielfunktion} $f: G \to H$, wobei wir im Allgemeinen von $H = \mathbb{R}$ ausgehen werden. Wir schreiben dann zum Beispiel
\begin{align*}
 \min{x \in \mathbb{R}} f(x),
\end{align*}
um das Problem ``finde den kleinsten Wert, den $f(x)$ bei reellem $x$ annimt'' zu notieren, und
\begin{align*}
 \argmin{x \in \mathbb{R}} f(x) = 4
\end{align*}
für das Problem ``finde die kleinste reelle Zahl $x$, sodass $f(x) = 4$ ist.
\newpar
Historisch ist das Feld der mathematischen Optimierung unter Nebenbedingungen stark verankert im Feld der \tbf{Operations Research}, welches sich mit der Optimierung von Produktionskosten unter gegebenen Bedingungen beschäftigt. Heutzutage hat die Optimierung zahlreiche Anwendungen, z.B. in der Pfadplanung, in Computer Vision, in der Bioinformatik, im maschinellen Lernen oder im Hardwaredesign.
\section{Anwendungsbeispiel: Support Vector Machines}
Ein klassisches Problem des maschinellen Lernes ist die Klassifizierung von Daten durch eine lineare Entscheidungsgrenze - alle Punkte $(x_i,y_i)$ über einer Ebene werden einer Klasse zugeordnet, und alle Punkte unter der Ebene einer anderen Klasse. Das Problem besteht daraus, eine Optimale Trennungsebene zu finden. Diese wird beschrieben durch eine Gleichung der Form
\begin{align*}
 \hat{y}(x) = w^T x + w_0.
\end{align*}
Sei $w^*$ der Vektor $(w_0, w_1, \hdots, w_{n-1})$.
Es stellt sich heraus, dass das zu lösende Optimierungsproblem gegeben ist durch:
\begin{align*}
\begin{array}{cc}
\argmin{w^{*} \in \mathbb{R}^n} &\frac{1}{2} ||{w^*}||^2\\
 \text{s.t. } &\forall i\ y_i\hat{y}(x_i) \geq 1
\end{array}
\end{align*}
Wir wollen die Länge $||w^{*}||$ des Vektors $w^*$ minimieren, da dies zu einem größeren Abstand zwischen unserer Ebene und den Datenpunkten führt, wodurch unser Modell besser generalisiert. Die Nebenbedingung entspricht der Anforderung, dass alle Punkte korrekt klassifiziert werden sollen.
\section{Typische Problemklassen}
Optimierungsprobleme können gemäß diverser Kriterien klassifiziert werden:
\begin{itemize}
 \item Probleme mit Nebenbedingungen vs Probleme ohne Nebenbedingungen
 \item Optimierung mit Variablen aus verschiedenen Mengen, insbesondere kontinuierliche Variablen vs diskrete Variablen
 \item Lineare vs nichtlineare Funktionen
 \item Eindimensionale vs mehrdimensionale Funktionen
 \item Konvexe Funktionen vs nicht konvexe Funktionen
 \item Konvexe Mengen vs nicht konvexe Mengen
\end{itemize}
Diese verschiedenen Problemklassen führen zu unterschiedlich schwierigen Problemen. Insbesondere sind konvexe Probleme einfacher zu lösen als nicht konvexe Probleme, kontinuierliche Probleme sind in der Regel einfacher zu lösen als diskrete Probleme, und lineare Probleme sind einfacher als nichtlineare Probleme. Relevante Fragen sind dann:
\begin{itemize}
 \item Wie schnell kovergiert das Verfahren zu einer Lösung? Wie viele Iterationen sind nötig? Was ist die Komplexität (in $O$-Notation) einer einzelnen Iteration?
 \item Konvergiert das Verfahren immer gegen ein Globales Optimum? Falls nein, gibt es garantierte obere/untere Schranken für die maximale Abweichung vom globalen Optimum?
\end{itemize}
\section{Konvexität}
Eine Menge $G$ ist \tbf{konvex}, wenn für beliebige Punkte $x,y \in G$ auch beliebige lineare Interpolationen zwischen den Punkten in der Menge enthalten sind:
\begin{align*}
 x,y \in G \implies \{(1-\alpha)x + \alpha y\ |\ \alpha \in [0,1]\} \subset G
\end{align*}
Intuitiv entspricht das der Forderung, dass zwischen für alle Paare von Punkten eine Verbindungsstrecke zwischen den Punkten in der Menge enthalten sein muss.
\newpar
Die \tbf{konvexe Hülle} einer Menge $G$ ist die kleinste konvexe Menge $H$ sodass $G \subset H$.
\newpar
Analog zur Definition einer konvexen Menge ist eine \tbf{konvexe Funktion} definiert als eine Funktion, für die gilt:
\begin{align*}
 x,y \in G \implies f((1-\alpha)x+\alpha y) \leq (1-\alpha)f(x) + \alpha f(y)\ \forall \alpha \in [0,1]
\end{align*}
Dies entspricht der Forderung, dass jede Verbindungsstrecke zwischen zwei Punkten auf dem Graphen unterhalb des Graphen liegen muss.
\newpar
Eine nicht konvexe Funktion, in der jedes lokale Minimum auch ein globales Minimum ist, wird als \tbf{quasikonvex} bezeichnet. Da dies dem Hauptvorteil von konvexen Funktionen in der Optimierung entspricht, ist die Optimierung von quasikonvexe Funktionen ebenfalls einfacher als die Optimierung allgemeiner nichtlinearer Funktionen.
\chapter{Gradientenverfahren}
\section{Wiederholung: Mehrdimensionale Differentiation}
Zur Erinnerung: der \tbf{Gradient} $\nabla f$ ist ein Vektor, der alle partiellen Ableitungen von $f$ enthält, also die Ableitung von $f$ in alle Koordinatenrichtungen:
\begin{align*}
 \nabla f = \left(\pd{f}{x_1}, \hdots, \pd{f}{x_n}\right)^T
\end{align*}
Im eindimensionalen Fall entspricht das genau der gewöhnlichen Ableitung, da partielle Ableitungen letztendlich genau wie ordinäre Ableitungen berechnet werden. Sei $f_1(x_1, x_2, x_3)$ eine beliebige Funktion $\mathbb{R}^3 \to \mathbb{R}$ und sei $f_2(x_1) = f_2(x_1, x_2, x_3)$ für beliebige Konstante $x_2,x_3$. Dann ist
\begin{align*}
 \pd{}{x_1}f_1(x_1,x_2,x_3) = \frac{d}{dx_1} f_2(x_1)
\end{align*}
Zum Beispiel:
\begin{align*}
 f_1(x_1,x_2,x_3) &= x_1^2x_2 + x_1 x_3 + 2x_2\\
  \pd{}{x_1}f_1(x_1,x_2,x_3) &= 2x_1 x_2 + x_3
\end{align*}
entspricht der Ordinären Ableitung
\begin{align*}
f_2(x_1) &= x_1^2 x_2 + x_1 x_3 + 2x_2\\
\frac{d}{dx_1} f_2(x_1) &= 2x_1 x_2 + x_3
\end{align*}
Eine mehrdimensionale Funktion $f: \mathbb{R}^n \to \mathbb{R}$ kann nicht nur in die Koordinatenrichtungen abgeleitet werden, sondern in beliebige Richtungen $\bm{d} \in \mathbb{R}^n$
    {Um Formeln verständlicher zu notieren, werde ich so gut es geht Variablen $\bm{x} \in \mathbb{R}^n$ im Fall $n > 1$ immer durch fette Buchstaben notieren.}.
Diese \tbf{Richtungsableitung} wird geschrieben als $\nabla_d f(x)$. Analog zur ordinären Definition der Ableitung ist die Richtungsableitung definiert als:
\begin{align*}
 \nabla_d f(x) = \lim_{h \to 0}\frac{f(x + h\bm{d}) - f(x)}{h}
\end{align*}
$f$ wird als differenzierbar bezeichnet, falls die Richtungsableitung in allen Richtungen existiert. Für ein differenzierbares $f$ kann die Richtungsableitung mit Hilfe des Gradienten sehr einfach als Skalarprodukt berechnet werden:
\begin{align*}
 \nabla_d f(x) = \nabla f(x)^T \bm{d}
\end{align*}
Im Rahmen dieser Vorlesung gehen wir davon aus, dass $f \in C^1$, also dass die zu minimierende Funktion mindestens einmal differenzierbar ist. Im Allgemeinen ist es jedoch wichtig, anzumerken, dass $f$ nicht umbedingt differenzierbar ist, nur weil partielle Ableitungen in alle Richtungen existieren.
%
\section{Iterative Optimierung}
Eine Funktion $f(\bm{x}): \mathbb{R}^n \to \mathbb{R}$ kann numerisch durch iteratives ''Ausprobieren`` verschiedener Werte von $\bm{x}$ optimiert werden. Wir starten mit einem beliebigen Startwert $\bm{x}^{(0)}$
    \footnote{Ich schreibe hier $\bm{x}^{(k)}$ mit der $k$ in Klammern, um klarzustellen, dass $\bm{x}^{(k)}$ nicht ''$\bm{x}$ hoch $k$`` ist, sondern ''das $k$-te $\bm{x}$``. Eigentlich würde ich das am liebsten als $\bm{x}_k$ notieren, aber da $\bm{x} \in \mathbb{R}^n$ ist, ist diese Notation viel zu leicht mit der Notation $x_k$ für die verschiedenen Komponenten von $\bm{x}$ zu verwechseln ($\bm{x} = (x_1, \hdots, x_n)^T$). Vermutlich werde ich an einigen Stellen aus Versehen trotzdem $\bm{x}^k$ schreiben, ich entschuldige mich im Voraus. (´-w-`)}
, und bewegen uns dann bei jedem Schritt $k$ in eine Richtung $\bm{d}^{(k)} \in \mathbb{R}^n$, welche uns hoffentlich näher zum Optimum bringt. Wir nutzen zusätzlich einen Parameter $\tau^{(k)} \in \mathbb{R}$, welcher die Schrittweite beschreibt.
\begin{align*}
 \bm{x}^{(0)} &\in \mathbb{R}^n\\
 \bm{x}^{(k+1)} &= \bm{x}^{(k)} + \tau^{(k)} \bm{d}^{(k)}
\end{align*}
In der Praxis wird oft einfach $\bm{x}^{(0)} = \bm{0}$ gewählt. Ist jedoch $f$ nicht quasikonvex, so kann eine schlechte Wahl von $\bm{x}^{(0)}$ dazu führen, dass das Verfahren nur gegen ein lokales Minimum kovergiert, welches oft kein globales Minimum ist und somit in vielen Fällen ein schlechtes Ergebnis.
\subsection{Welche Richtung ist optimal?}
Das Optimale $\bm{x}^{(k+1)}$ ist das, das $f(\bm{x}^{(k+1)}$ minimiert. Wir können $f(\bm{x}^{(k+1)})$ für kleine Schrittweiten $\tau^{(k)}$ folgendermaßen approximieren (\tbf{Taylor-Approximation}):
\begin{align*}
f(\bm{x}^{(k+1)}) = f(\bm{x}^{(k)} + \tau^{(k)}\bm{d}^{(k)}) \approx f(\bm{x}^{k}) + \tau^{(k)} \nabla f(\bm{x}^{(k)})\bm{d}^{(k)}
\end{align*}
Da wir den Wert von $f$ möglichst stark verringern wollen, sollte $\tau^{(k)} \nabla f(\bm{x}^{(k)})\bm{d}^{(k)}$ ein negativer Term mit möglichst großem Betrag sein. Dabei können wir $\tau^{(k)}$ aber nicht zu hoch wählen, da sonst die Approximation ungenau wird.
\newpar
Nach der Definition des Skalarprodukts gilt:
\begin{align}
 \nabla f(\bm{x}^{(k)})\bm{d}^{(k)} = ||\nabla f(\bm{x}^{(k)})||\ ||\bm{d}^{(k)}|| \cos(\theta)
\end{align}
Wobei $\theta$ der Winkel zwischen $\nabla f(\bm{x}^{(k)})$ und $\bm{d}^{(k)}$ ist. Diese Gleichung ist minimal, wenn $\cos(\theta) = -1$, also $\theta = 180$. Dementsprechend muss $\bm{d}^{(k)}$ in die umgekehrte Richtung von $\nabla f(\bm{x}^{(k)})$ zeigen, also $\bm{d}^{(k)} = -\nabla f(\bm{x}^{(k)})$.
    \footnote{Technisch gesehen wird die Gleichung durch $\bm{d}^{(k)} = -\alpha \cdot \nabla f(\bm{x}^{(k)})$ mit möglichst großen $\alpha \in \mathbb{R}$ minimiert. Die ''Rolle`` dieses Parameters $\alpha$ wir jedoch bereits durch unseren Schrittweiten-Parameter $\tau^{(k)}$ abgedeckt, und es gilt auch für dieses theoretische $\alpha$, dass die Approximation sehr ungenau wird, wenn wir ein hohes $\alpha$ wählen.}
\newpar
Mit dieser optimalen Wahl der Abstiegsrichtung $\bm{d}^{(k)}$ erhalten wir das \tbf{Gradientenverfahren} (auch bekannt als ''Gradientenabstieg``, auf Englisch ''\tbf{Gradient Descent}''):
\begin{align*}
 \bm{x}^{(0)} &\in \mathbb{R}^n\\
 \bm{x}^{(k+1)} &= \bm{x}^{(k)} - \tau^{(k)} \nabla f(\bm{x}^{(k)})
\end{align*}
\section{Bestimmung der Schrittweite}
Die Wahl der Schrittweite $\tau^{(k)}$ ist für das Gradientenverfahren von entscheidender Bedeutung. Ist $\tau^{(k)}$ zu klein, wird die Konvergenz des Algorithmus oft deutlich verlangsamt. Ist $\tau^{(k)}$ zu groß, wird die Approximation ungenau. Dies kann dazu führen, dass das Verfahren über das Ziel hinausschießt, und im schlimmsten Fall eventuell gar nicht konvergiert.
\newpar
Die Bestimmung eines optimalen Werts für $\tau^{(k)} \in \mathbb{R}$ ist ein eigenes Optimierungsproblem.  In einigen Fällen ist ein dauerhaft konstantes $\tau$ genug, um die Konvergenz zu garantieren. In diesen Fällen ist eine konstante Schrittweite oft effizienter.
\newpar
Um ein optimales $\tau^{(k)}$ für eine gegebene Iteration $k$ zu finden, definieren wir eine neue Funktion $h: \mathbb{R} \to \mathbb{R}$:
\begin{align*}
h(\tau) &= f(\bm{x}^{(k)} + \tau \bm{d}^{(k)})\\
 \tau^{(k)} &= \argmin{\tau \in \mathbb{R}} h(\tau)
\end{align*}
Um die Ableitung von $h$ zu finden, beschreiben wir den Term $\bm{x}^{(k)} + \tau \bm{d}^{(k)}$ als eine eigene eindimensionale Funktion $g(\tau)$. Dann ist $h(\tau) = f(\bm{g(\tau)})$, gemäß der mehrdimensionalen Kettenregel gilt also:
\begin{align*}
 \frac{d}{d\tau}h(\tau) &= \frac{d}{d\tau} f(\bm{g(\tau)})\\
 &= \sum_{i=1}^n \left(\frac{d}{d \tau} \bm{g(\tau)}\right) \pd{}{g(\tau)_i}f(\bm{g(\tau)})\\
 &= \sum_{i=1}^n d^{(k)}_i \pd{}{g(\tau)_i}f(\bm{g(\tau)})\\
 &= \nabla f(\bm{g(\tau)}) \cdot \bm{d}^{(k)}\\
 &= \nabla f(\bm{x}^{(k)} + \tau \bm{d}^{(k)}) \dk
\end{align*}
Eine exakte Bestimmung des Optimalwerts ist mit hohem Aufwand Verbunden und ist oft sowieso nur von geringem Vorteil, da in der Regel auch bei wiederholter optimaler Wahl von $\tauk$ für den letztendlichen Gradientenabstieg viele Schritte nötig sind. Stattdessen werden in der Praxis approximative Lösungen gefunden, welche bestimmte \tbf{Qualitätsbedingungen} erfüllen.
\newpar
Eine einfache Suche nach einem beliebigen Wert, welcher den Funktionswert reduziert, also
\begin{align*}
f(\xk + \tauk \dk) - f(\tauk) \geq 0,
\end{align*}
ist dabei nicht genug, da bei hinreichend schneller Konvergenz von $f(\xk + \tauk \dk) - f(\tauk)$ gegen $0$ eventuell nie das Optimum erreicht wird.
%
\subsection{Die Armijo-Bedingung}
Die \tbf{Armijo-Bedingung} hat das Ziel, sicherzustellen, dass die Zielfunktion bei einem Schritt um die gegebenen Schrittweite $\tauk$ hinreichend reduziert wird. Die Bedingung ist gegeben durch die folgende Ungleichung:
\begin{align*}
f(\xk + \tauk \dk) &\leq f(\xk) + \delta \tauk \nabla f(\xk)^T \dk
\end{align*}
Die minimal notwendige Reduktion wird dabei durch einen Parameter $\delta \in (0,1)$ gesteuert. Ein üblicher Wert ist dabei $\delta \approx 10^{-4}$. Die Multiplikation mit dem Gradienten bedeutet, dass bei einem steileren Gradienten auch nach einer höheren Reduktion gesucht wird.
%
\subsection{Die Wolfe-Bedingungen}
Die \tbf{schwache Wolfe-Bedingung} fordert, dass der Gradient an der Zielposition entweder weniger steil als an der Startposition ist oder das Vorzeichen ändert:
\begin{align*}
-\nabla f(\xk + \tauk \dk)^T \dk \leq -\eta \nabla f(\xk)^T \dk
\end{align*}
Das Minus auf beiden Seiten kommt davon, dass bei einer Richtung $\dk$, welche zu einem Abstieg führt, notwendigerweise $\nabla f(\xk )^T \dk$ negativ ist.
\newpar
Wir wählen hier $\eta \in (\delta, 1)$.  In der Regel wird $\eta$ deutlich größer als $\delta$ gewählt, Wikipedia zitiert einen Richtwert von $\eta \approx 0.9$.
\newpar
Bei der \tbf{starken Wolfe-Bedingung} wird der Betrag der Terme genommen, also gefordert, dass auch bei Vorzeichenwechsel die Steigung nach dem Schritt weniger steil als davor sein soll:
\begin{align*}
|\nabla f(\xk + \tauk \dk)^T \dk| \leq \eta |\nabla f(\xk)^T \dk|
\end{align*}
Intuitiv verhindern die Wolfe-Bedingungen ineffizient kleine Schritte, bei denen man ohne Probleme in die gleiche Richtung weitergehen könnte.
\newpar
Schrittweiten, welche sowohl die starke Wolfe-Bedingung als auch die Armijo-Bedingung
\footnote{In dieser Vorlesung wurden die starke Wolfe-Bedingung und die Armijo-Bedingung als seperate Bedingungen vorgestellt. In der Literatur wird die Armijo-Bedingung oft als Teil der starken Wolfe-Bedingung definiert, mit ``Erfüllung der starken Wolfe-Bedingungen'' ist dann die Erfüllung beider Bedingungen gemeint.}
erfüllen, existieren unter den von uns gemachten Annahmen immer. Als Erinnerung: Wir haben folgende Annahmen gemacht:
\begin{itemize}
 \item $f(x)$ ist kontinuierlich differenzierbar ($f(x) \in C^1$)
 \item $\dk$ ist eine Abstiegsrichtung an der Stelle $\xk$ (also $\nabla f(\xk )^T \dk < 0$)
 \item $0 \leq \delta \leq \eta \leq 1$
\end{itemize}
Für die schwache Wolfe-Bedingung muss zusätzlich gegeben sein, dass die Menge $\{f(\xk + \tau \dk) \mid \tau > 0\}$ von unten beschränkt ist.
\newpar
\tbf{Beweis}, nach Nocedal und Wright \footnote{J. Nocedal, S. J. Wright: Numerical Optimization, Springer, 2006, Seite 35ff} (In der Vorlesung nicht gegeben):
\begin{itemize}
 \item $h(\tau) = f(\xk + \tau \dk)$ ist nach Annahme für alle $\tau$ nach unten beschränkt.
 \item Da $\dk$ eine Abstiegsrichtung und $\delta$ positiv ist, ist die Linie $l_{Arm}(\tau) := f(\xk) + \delta \tau \nabla f(\xk)^T \dk$ nach unten unbeschränkt.
 \item Es gilt außerdem $h(0) = l_{Arm}(0)$ und $l_{Arm}(0) = \delta h'(0) < h'(0)$, also muss für kleine $\tau$ in Abstiegsrichtung gelten, dass $h(\tau) < l_{Arm}(\tau)$.
 \item Somit muss $l_{Arm}(\tau)$ den Graphen von $h(\tau)$ für mindestens einen Wert $\tau > 0$ schneiden. Sei $\tau'$ der kleinste Wert, bei dem es das tut.
 \item Es folgt $\forall \tau < \tau': h(\tau) < l_{Arm}(\tau)$, da sonst $\tau'$ nicht der minimale Schnittpunkt wäre. Also erfüllen alle $\tau \in (0, \tau')$ die Armijo-Bedingung.
 \item Nach dem Mittelwertsatz muss es nun ein $\tau'' \in (0, \tau')$ geben, sodass
 \begin{align*}
  f(\xk + \tau' \dk) - f(\xk) = \tau' \nabla f(\xk \tau'' \dk)^T \dk
 \end{align*}
 \item Da $\tau'$ ein Schnittpunkt ist, gilt nun
 \begin{align*}
  f(\xk + \tau' \dk) &= f(\xk) + \tau' \delta \nabla f(\xk)^T \dk\\
  \implies f(\xk + \tau' \dk) - f(\xk) &= \tau' \delta \nabla f(\xk)^T \dk\\
  \implies \tau' \nabla f(\xk \tau'' \dk)^T \dk &= \tau' \delta \nabla f(\xk)^T \dk\\
  \implies \nabla f(\xk \tau'' \dk)^T \dk &= \delta \nabla f(\xk)^T \dk\\
  \overset{\eta > \delta}{\implies} \nabla f(\xk \tau'' \dk)^T \dk &> \eta\nabla f(\xk)^T \dk\\
 \end{align*}
Also erfüllt $\tau''$ sowohl die Armijo-Bedingung als auch die schwache Wolfe-Bedingung. Desweiteren ist der Term auf der linken Seite der finalen Ungleichung negativ, also gilt sogar die starke Wolfe-Bedingung. Da die Ungleichungen strikt sind existiert desweiteren ein Intervall um $\tau''$, welches die beiden Bedingungen ebenfalls erfüllt.
\end{itemize}
\subsection{Line Search}
Ein einfaches Verfahren zur Bestimmung eines geeigneten $\tau$ ist die sogenannte \tbf{Backtracking Line Search}:
\begin{itemize}
 \item Definiere $\tau^{(0)} = 1$
 \item Falls die Armijo-Bedingung nicht erfüllt ist, probiere als nächstes $\tau^{(k+1)} = \beta \tauk$, wobei $\beta \in (0,1)$.
\end{itemize}
Unter Nutzung des Gradienten lässt sich auch ein \tbf{Interpolationsverfahren} anwenden, welches schneller konvergiert:
\begin{itemize}
 \item Definiere $\tau^{(0)} = 1$
 \item Betrachte die Quadratische Taylor-Approximation von $h(\tau)$:
 \begin{align*}
  h_q(\tau) = \left(\frac{h(\tau^{(0)}) - h(0) - \tau^{(0)}h'(0)}{{\tau^{(0)}}^2}\right)\tau^2 + h'(0)\tau + h(0)
 \end{align*}
 Diese ist minimiert durch
 \begin{align*}
  \tau^{(1)} = \frac{h'(0) {\tau^{(0)}}^2}{2(h(\tau^{(0)}) - h(0) - h'(0)\tau^{(0)}}
 \end{align*}
 \item Solange $\tau^{(k)}$ die Armijo-Bedingung noch nicht erfüllt, bestimme $\tau^{(k+1)}$ durch ``kubische Interpolation mit $h(0), h'(0), h(\tau^{(0)}, h(\tau^{(1)})$ \footnote{Die Vorlesung war hier extrem vage - ich vermute, wir interpolieren iterativ immer $h(0), h'(0), h(\tau^{(k-1)}), h(\tau^{(k)})$? Wie genau man eine kubische Interpolation durchführt wurde aber nicht erklärt.}
\end{itemize}
Line Search Verfahren, welche die Wolfe-Bedingungen sicherstellen, sind komplizierter. Solche Verfahren bestehen aus zwei Komponenten:
\begin{itemize}
 \item \tbf{Bracketing} erweitert das Suchintervall, bis darin geeignete Schrittweiten garantiert werden können.
 \item \tbf{Zooming} reduziert das Suchintervall, bis darin per Interpolationsverfahren eine geeignete Schrittweite gefunden wird.
\end{itemize}
\subsection{Conjugate Gradient-Verfahren}
Angenommen, unsere Zielfunktion ist eine quadratische Funktion in folgender Form:
\begin{align}
 f(\bm{x}) = \frac{1}{2}\bm{x}^T A \bm{x} - \bm{b}^T \bm{x}
\end{align}
Das sogenannte \tbf{Conjugate Gradient-Verfahren} funktioniert dann folgendermaßen:
\begin{itemize}
 \item Wähle als erste Richtung den Gradienten an der Startposition:
\begin{align*}
 \bm{d}^{(0)} = - \nabla f(\bm{x}^{(0)} = \bm{b} - A \bm{x}^0
\end{align*}
 \item Bestimme die optimale Schrittweite:
\begin{align*}
 \tauk = \frac{|\nabla f(\xk)|}{{\dk}^T A \dk}
\end{align*}
\begin{align*}
 \bm{x}^{(k+1)} = \xk + \tauk \dk
\end{align*}
 \item Wähle eine neue Richtung so, dass $\bm{d}^{(k+1)}$ und $\dk$ gemäß der von $A$ induzierten Metrik orthogonal sind, also ${\bm{d}^{(k+1)}}^T \cdot A \cdot \dk = 0$. Dies wird garantiert durch:
\begin{align*}
 \beta^{(k)} = \frac{|\nabla f(\bm{x}^{(k+1)})|^2}{|\nabla f(\xk)|^2}
\end{align*}
\begin{align*}
 \bm{d}^{(k+1)} = - \nabla f(\bm{x}^{k+1}) + \beta^{(k)} \dk
\end{align*}
\end{itemize}
Das Minimieren von $f$ entspricht genau dem Lösen des Gleichungssystems $Ax = b$. Das CG-Verfahren garantiert dadurch für die Matrix $A = \mathbb{R}^{n \times n}$ eine bei exakter Zahlendarstellung exakte Lösung nach nur $n$ Schritten. Wenn die \tbf{Konditionszahl} von $A$, definiert als der größte Eigenwert geteilt durch den kleinsten, klein ist, konvergiert es sogar wesentlich schneller.
\newpar
Für nicht quadratische Funktionen kann das Conjugate-Gradient verfahren nahezu identisch angewandt werden, allerdings existiert im Allgemeinen für die Schrittweitenbedingung keine analytische Lösung , weshalb wieder Line Search nötig wird.
%
\chapter{Newton und Quasi-Newton-Verfahren}
\section{Das Newton-Verfahren}
Das \tbf{Newton-Verfahren} ähnelt dem Gradientenverfahren, verwendet jedoch auch die Krümmung der Funktion, also die zweite Ableitung, gegeben durch die Hesse-Matrix $H_f$:
\begin{align*}
 \bm{x}^{(k+1)} = \xk - \tauk H_f^{-1} \xk \nabla f(\xk)
\end{align*}
Dadurch ermöglicht das Newton-Verfahren größere Schritte in Richtungen mit schwacher Krümmung, bei denen die Approximation genauer und somit das ''Risiko`` geringer ist.
\newpar
Zur Erinnerung: Die Hesse-Matrix $H_f$ ist gegeben durch:
\begin{align*}
 H_f =
 \begin{pmatrix}
  \frac{\partial^2 f}{\partial x_1 \partial x_1} & \hdots & \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
  \vdots & \ddots & \vdots\\
  \frac{\partial^2 f}{\partial x_n \partial x_1} & \hdots & \frac{\partial^2 f}{\partial x_n \partial x_n}\\
 \end{pmatrix}
\end{align*}
Die Hesse-Matrix ist symmetrisch, da $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$.
\newpar
Während das Gradientenverfahren also mit einer linearen Taylor-Approximation arbeitet, nutzt das Newton-Verfahren eine quadratische Taylor-Approximation:
\begin{align*}
 \mk(\bm{d}) &:= f(\xk + \bm{d}) \approx f(\xk) + {\bm{d}}^T \nabla f(\xk) + \frac{1}{2} {\bm{d}}^T H_f \xk\bm{d}\\
 \nabla \mk(\bm{d}) &= \nabla f(\xk) + H_f(\xk)\bm{d}
\end{align*}
Für das optimale $d$ ist $\nabla \mk(\bm{d}) = 0$ und wir erhalten:
\begin{align*}
 H_f(\xk)\bm{d} &= - \nabla f(\xk)\\
 \implies  \bm{d} &= - H_f^{-1} \xk \nabla f(\xk)
\end{align*}
Die Optimale Schrittweite $\tauk$ muss auch beim Newton-Verfahren durch Line Search bestimmt werden.
\section{Konvergenzordnungen}
Sei $s$ eine beliebige Folge und
$\overline{s} := \lim_{k \to \infty} s_k$. Wir betrachten nun die Konvergenzordnung der Folge $s$, also die Geschwindigkeit, mit der die Folge konvergiert. In dieser Vorlesung unterscheiden wir folgende Konvergenzordnungen:
\begin{itemize}
 \item $s$ konvergiert \tbf{linear}, wenn:
 \begin{align*}
  \lim_{k \to \infty} \frac{|s_{k+1} - \overline{s}|}{|s_{k} - \overline{s}|} < 1
 \end{align*}
 ein Beispiel für eine linear konvergierende Folge ist $s_k = 0.9^k$. Das Gradientenverfahren konvergiert ebenfalls linear.
 \item $s$ konvergiert \tbf{superlinear}, wenn:
 \begin{align*}
  \lim_{k \to \infty} \frac{|s_{k+1} - \overline{s}|}{|s_{k} - \overline{s}|} = 0
 \end{align*}
 ein Beispiel für eine superlinear konvergierende Folge ist $s_k = \frac{1}{k!}$.
 \item $s$ konvergiert \tbf{quadratisch}, wenn:
 \begin{align*}
  \lim_{k \to \infty} \frac{|s_{k+1} - \overline{s}|}{|s_{k} - \overline{s}|^2} < 1
 \end{align*}
 ein Beispiel für eine superlinear konvergierende Folge ist $s_k = 0.9^{(2^k)}$.
\end{itemize}
Das Newton-Verfahren konvergiert für glatte, konvexe Funktionen nahe der Lösung quadratisch. Nominal ist es also wesentlich effizienter als Gradientenabstieg. Dabei ist jedoch zu berücksichtigen, dass die Berechung und Invertierung der Hesse-Matrix wesentlich aufwendiger ist als die Berechnung des Gradienten. Demenssprechend ist ein einzelner Iterationsschritt des Newton-Verfahrens auch wesentlich aufwendiger als ein einzelner Schritt des Gradientenverfahrens.
\subsection{Konvergenzradius des Newton-Verfahren}
Damit $\dk = - H_f^{-1} \xk \nabla f(\xk)$ eine Abstiegsrichtung ist, muss $H_f$ positiv definit sein, also die Krümmung positiv. Bei negativer Krümmung läuft das Newton-Verfahren in die falsche Richtung! Dies ist kein Problem für konvexe Funktionen, da diese überall positive Krümmung haben. Bei nicht konvexen Funktionen muss die Hesse-Matrix erst manipuliert werden, damit alle ihre Eigenwerte positiv werden.
\section{Quasi-Newton Verfahren}
Die Idee hinter Quasi-Newton-Verfahren ist es, die Hesse-Matrix mithilfe der Gradienten sukzessiv zu approxmieren. Man ersetzt also in der Iterationsformel des Newton-Verfahrens die Matrix $H_f$ durch eine Approximation $B$:
\begin{align*}
 \bm{x}^{(k+1)} = \xk - \tauk (B^k)^{-1} \nabla f(\xk)
\end{align*}
Die einfachste Möglichkeit ist eine lineare Taylor-Approximation des Gradienten. Sei $s = \bm{x}^{(k+1)} - \xk$. Dann haben wir:
\begin{align*}
 \nabla f(\xk + \bm{s}) &\approx \nabla f(\xk) + H_f(\xk)\bm{s}\\
 \implies H_f(\xk)(\bm{x}^{(k+1)} - \xk) &\approx \nabla f(\bm{x}^{(k+1)}) - \nabla f(\xk)
\end{align*}
Wir suchen also in jedem Schritt eine Matrix $\Bk$, sodass:
\begin{align}
 \Bk (\bm{x}^{(k+1)} - \xk) = \nabla f(\bm{x}^{(k+1)}) - \nabla f(\xk)
\end{align}
Abgekürzt schreiben wir auch:
\begin{align*}
 \Bk \sk = \yk
\end{align*}
Aus $\Bk \sk = \yk$ folgt ${\sk}^T\Bk \sk = {\sk}^T\yk$. Wir gehen von einer konvexen Funktion $f$ aus, also ist $\Bk$ positiv definit. Daraus folgt ${\sk}^T\Bk \sk > 0$, damit gilt auch ${\sk}^T\yk > 0$. Diese Ungleichung können wir als Bedingung für die Schrittweitensuche nutzen.
\newpar
Um die Anzahl an möglichen $\Bk$ einzuschränken, können wir als weitere Bedingung fordern, dass $\Bk$ in jedem Schritt möglichst ähnlich zur letzten Iteration sein soll:
\begin{align*}
 \Bk = \min{B} ||B - B^{(k-1)}||
\end{align*}
$||\cdot||$ kann eine beliebige Matrixnorm sein. An diesem Punkt führen verschiedene Normen zu verschiedenen Unterverfahren. Ein beliebtes Verfahren ist das \tbf{Davidon-Fletcher-Powell Verfahren}, welches eine sog. gewichtete Frobeniusnorm benutzt.
\subsection{Das Davidon-Fletcher-Powell Verfahren}
Beim Davidon-Fletcher-Powell (DFPV)-Verfahren werden die Approximation $B^{k+1}$ und ihr Inverses $Q^{(k+1)}$ direkt aus den vorherigen Werten berechnet:
\begin{align*}
 \rhok &= \frac{1}{(\yk)^\top \yk}\\
 B^{k+1} &= (I - \rhok \yk (\sk)^\top) \Bk (I - \rhok \sk (\yk)^\top) + \rhok \yk (\yk)^\top\\
 Q^{(k+1)} &= \Qk - \frac{\Qk \yk (\yk)^\top \Qk}{(\yk)^\top \Qk \yk} + \frac{\sk (\sk)^\top}{(\yk)^\top \sk}
\end{align*}
\subsection{Das Broyden-Fletcher-Goldfarb-Shanno Verfahren}
Das Broyden-Fletcher-Goldfarb-Shanno (BFGS)-Verfahren ist eine etwas effizientere Variante des DFPV-Verfahrens, bei dem $Q^{(k+1)}$ direkt folgendermaßen berechnet wird:
\begin{align*}
 Q^{k+1} &= (I - \rhok \sk (\yk)^\top) \Qk (I - \rhok \yk (\sk)^\top) + \rhok \yk (\yk)^\top\\
\end{align*}
$Q^{k+1}$ ist immer positiv definit, wenn $Q^{(k)}$ positiv definit war.
\newpar
Das volle BFGS-Verfahren verläuft dann also folgendermaßen:
\begin{enumerate}
 \item $\bm{x}^{(0)} \in \mathbb{R}$, $Q^{(0)} \in \mathbb{R}^{n \times n}$ (z.B. $Q^{(0)} = I_n$)
 \item $\dk = \Qk \nabla f(\xk)$
 \item Bestimme optimale Schrittweite $\tauk$ durch Line Search mit Wolfe-Bedingungen. Es sollte immer zuerst $\tauk = 1$ getestet werden, da diese Schrittweite zur schnellsten Konvergenz führt und in den meisten Iterationen in der Praxis akzeptiert wird.
 \item Berechne neue Lösung $\bm{x}^{(k+1)} = \xk + \tauk \dk$
 \item Berechne neue Approximation der inversen Hesse Matrix:
 \begin{align*}
  \sk &= \bm{x}^{(k+1)} - \xk\\
  \yk &= \nabla f(\bm{x}^{(k+1)}) - \nabla f(\xk)\\
  \rhok &= \frac{1}{(\yk)^\top \yk}\\
  Q^{k+1} &= (I - \rhok \sk (\yk)^\top) \Qk (I - \rhok \yk (\sk)^\top) + \rhok \yk (\yk)^\top\\
 \end{align*}
\end{enumerate}
Die Vorlesung gibt folgende Iterationszahlen für ein Beispielproblem aus der Praxis:
\begin{itemize}
 \item Gradientenabstieg: $5264$ Iterationen
 \item BFGS: $34$ Iterationen
 \item Newton: $21$ Iterationen (Aber wie bereits gesagt ist dafür jede Iteration sehr viel teurer als bei BFGS)
\end{itemize}

\section{Least-Squares-Aufgaben}
Eine sehr häufige Klasse von Zielfunktionen, welche inbesondere in der Statistik und somit auch im maschinellen Lernen wichtig sind, ist gegeben durch \tbf{Least Squares}, auch bekannt als $L_2$-Loss:
\begin{align*}
 f(x) = \frac{1}{2}\sum_{j=1}^m r_j(\bm{x})^2
\end{align*}
Wobei $r_j$ in der Regel ein Klassifikationsfehler für einen Datenpunkt $x_j, y_j$ ist, also:
\begin{align*}
 r_j(\bm{x}) = (\hat{y}_j(\bm{x}) - y_j) \in \mathbb{R}
\end{align*}
Wenn wir die Residuen $r_j$ als einen \tbf{Residuenvektor} $\bm{r}(\bm{x}) = (r_1(\bm{x}), \hdots, r_m(\bm{x}))^\top$ schreiben, erhalten wir eine gekürzte Schreibweise des Problems:
\begin{align*}
 f(x) = \frac{1}{2}||\bm{r}(\bm{x})||^2
\end{align*}
Der Residuenvektor ist eine Funktion $r: \mathbb{R}^n \to \mathbb{R}^m$, also eine Funktion mit mehrdimensionaler Bildmenge. Die Ableitung einer solchen Funktion ist gegeben durch die sog. \tbf{Jacobimatrix}:
\begin{align*}
\Jrx = \begin{pmatrix}
        \nabla r_1(\bm{x}^\top)\\
        \vdots\\
        \nabla r_m(\bm{x}^\top)\\
       \end{pmatrix}
       \in \mathbb{R}^{m \times n}
\end{align*}
Im Least-Squares-Problem lässt sich damit der Gradient von $f(x)$ sehr kompakt darstellen:
\begin{align*}
 \nabla f(\bm{x}) = \frac{1}{2}\sum_{j=1}^m 2 r_j(\bm{x}) \nabla r_j(\bm{x}) = \Jrx^\top \bm{r(x)}
\end{align*}
Die Hesse-Matrix ist dann gegeben durch:
\begin{align*}
 H_f(\bm{x}) &= \sum_{j=1}^m \nabla r_j(\bm{x}) \nabla r_j(\bm{x})^\top + \sum_{j=1}^m r_j(x) \nabla^2 r_j(\bm{x})\\
 &= J(\bm{x})^\top J(\bm{x}) + \sum_{j=1}^m r_j(x) \nabla^2 r_j(\bm{x})
\end{align*}
\subsection{Lineare Residuen}
Ist die Residuenfunktion $\bm{r(x)}$ linear, dann ist  $\bm{r(x)} = J\bm{x} - \bm{b}$ mit $J \in \mathbb{R}^{m \times n}$. In diesem Fall lässt sich das Ergebnis weiter vereinfachen: Wir haben dann $\Jrx = J$, und der zweite Teil der Hesse-Funktion fällt weg. Dann gilt also ganz einfach
\begin{align*}
 H_f(\bm{x}) = J^\top J.
\end{align*}
Das Problem der Optimierung von $f$ ist dann durch folgendes lineares Gleichungssystem beschrieben:
\begin{align*}
 \nabla f(\bm{x}) &= 0\\
 \implies J^\top \bm{r(x)} &= 0\\
 \implies J^\top (J\bm{x} - \bm{b}) &= 0\\
  \implies J^\top J\bm{x} &= J^\top \bm{b}\\
\end{align*}
%
\subsection{Nichtlineare Residuen}
Für nichtlineare Residuen bietet es sich an, die Hesse-Matrix einfach durch der ersten Term zu approximieren:
\begin{align*}
 H_f(\bm{x}) &= J(\bm{x})^\top J(\bm{x}) + \sum_{j=1}^m r_j(x) \nabla^2 r_j(\bm{x})\\
 &\approx J(\bm{x})^\top J(\bm{x})
\end{align*}
Dies entspricht der in dieser Vorlesung bereits oft verwendeten linearen Taylor-Approximation einer Funktion. Anders als bei BFGS wird hier die spezifische Problemstruktur von Summen quadrierter Residuen ausgenutzt. Wir können die Newtonsche Gleichung von ganz am Anfang nutzen, um ein simples lineares Gleichungssystem zu erhalten:
\begin{align*}
 H_f(\xk)\bm{d} &= - \nabla f(\xk) = -J_r(\xk)^\top \bm{r(x)}\\
 \implies J_r(\xk)^\top J_r(\xk) \dk &= -J_r(\xk)^\top \bm{r(x)}\\
\end{align*}
Die Lösung ist dann eine optimale Abstiegsrichtung $\dk$. Dieses Verfahren ist bekannt als \tbf{Gauss-Newton-Verfahren}, da es das Newton-Verfahren durch das Lösen einer Sequenz von linearen Gleichungssystem approximiert (Welche dann wiederum durch das Gauss-Verfahren gelöst werden können). Das Verfahren funktioniert genau dann gut, wenn ale Eigenwerte von $J^\top J$ deutlich positiv sind. Bei Eigenwerten Nahe null sind andere Verfahren notwendig, welche in dieser Vorlesung nicht besprochen werden, z.B. das Verfahren von Levenberg-Marquardt.
%
%
%
\chapter{Optimierung mit Nebenbedingungen}
Bisher war der Suchraum für ein Optimum einer Funktion $f: \mathbb{R}^n \to \mathbb{R}$ immer der gesamte Urbildraum $\mathbb{R}^n$. In vielen Fällen ist der Suchraum aber eine Teilmenge $G \subset \mathbb{R}^n$, welche eingeschränkt ist durch Nebenbedingungen wie ''$x_1 \leq 5$``. Die Lösung eines Problems ohne Nebenbedingung gibt offensichtlich immer eine untere Schranke für die tatsächliche Lösung, da natürlich jede Lösung des Problems mit Nebenbedingungen auch eine Lösung des Problems ohne Nebenbedingungen ist. Oft, aber nicht immer, liegt das Optimum auf dem Rand von $G$. Wir gehen in der Regel davon aus, dass $G$ Kompakt ist, da sonst in diesem Fall kein tatsächliches Optimum existiert.
\newpar
Ein Optimierungsproblem wird im Allgemeinen geschrieben als:
\begin{align*}
\begin{array}{cc}
\argmin{x \in \mathbb{R}^n} &f(x)\\
 \text{subject to} &c_i(\bm{x}) = 0 \quad \forall i \in \mathcal{E}\\
 &c_i(\bm{x}) \geq 0 \quad \forall i \in \mathcal{I}\\
\end{array}
\end{align*}
%
\section{Optimierung mit Gleichheitsbedingungen}
Die $m$ Gleichheitsbedingungen sind zusammengefasst durch eine vektorwertige Funktion $\bm{c(x)}: \mathbb{R}^n \to \mathbb{R}^m$ gegeben. Es gilt demenssprechend genau dann $\bm{c(x)} = \bm{0}$, wenn alle Gleichheitsbedingungen erfüllt sind. Ein Schritt in Richtung $\bm{d}$ ist dementsprechend genau dann erlaubt, wenn $\nabla \bm{c}^\top \bm{d} = 0$ und $\nabla f^\top \bm{d} \leq 0$, also wenn $\bm{d}$ eine Abstiegsrichtung ist und wenn sich der Wert von $\bm{c(x)}$ nach dem Schritt nicht ändert (und demenssprechend die Gleichheitsbedingungen danach immer noch erfüllt sind).
\newpar
Es stellt sich heraus, dass in einem Extrempunkt die Gradienten $\nabla f$ und $\nabla c$ immer parallel sein müssen, also
\begin{align*}
 f(\bm{x})  = \bm{\lambda} (\bm{x}) \quad \bm{\lambda} \in \mathbb{R}^m
\end{align*}
Ist diese Bedingung nicht gegeben, so existiert ein Schritt, welcher $f$ ändert, aber nicht $c$, und somit ist die momentane Position $\bm{x}$ kein Extremum. \footnote{Falls dies intuitiv noch unklar ist empfehle ich als zusätzliche Erklärung das Video \href{https://www.youtube.com/watch?v=5A39Ht9Wcu0}{\textit{Understanding Lagrange Multipliers Visually}, von \textit{Serpentine Integral}.}}
\newpar
Diese Bedingung können wir nun ausnutzen. Angenommen, $f$ ist eine Funktion $\mathbb{R}^n \to \mathbb{R}$ und $c$ ist eine Funktion $\mathbb{R}^n \to \mathbb{R}^m$. Dann ist die \tbf{Lagrange-Funktion} $\mathcal{L}$ des Problems die folgende Funktion $\mathbb{R}^{n+m} \to \mathbb{R}$:
\begin{align*}
 \mathcal{L}(\bm{x}, \bm{\lambda}) = f(\bm{x}) - \bm{\lambda} \bm{c}(\bm{x})
\end{align*}
Die Schreibweise ''$\mathcal{L}(\bm{x}, \bm{\lambda})$`` fand ich persönlich sehr gewöhnungsbedürftig und anfangs sehr verwirrend. Es ist wichtig, sich zu merken, dass $\bm{x}$ selbst ein $n$-dimensionaler Vektor ist. Letzendlich ''erfinden`` wir quasi eine neue Menge an Eingabevariablen, gegeben durch den Vektor $\bm{\lambda} = (\lambda_1, \hdots, \lambda_m)$, welchen wir hinten an den ''Eingabevektor`` $\bm{x} = (x_1, \hdots, x_n)^\top$ hängen. So erhalten wir einen neuen Vektor $\bm{x}' = (x_1, \hdots, x_n, \lambda_1, \hdots, \lambda_m)^\top$, welchen wir dann in die Lagrange-Funktion einsetzten. Eine intuitivere, und arguably korrektere, Schreibweise dafür wäre $\mathcal{L}(x_1, \hdots, x_n, \lambda_1, \hdots, \lambda_m)$. Da das jedoch den meisten Mathematikern zu viel Schreibarbeit ist wird in der Praxis nunmal $\mathcal{L}(\bm{x}, \bm{\lambda})$ geschrieben. Die ''Original-Variablen`` $x_i$ werden oft als die \tbf{primalen Variablen} bezeichnet, die neuen Variablen $\lambda_i$ hingegen als die \tbf{Lagrange-Multiplikatoren}.
\newpar
Die Bedingung für ein Minimum von $f$ kann nun mithilfe der Lagrange-Funktion ausgedrückt werden als:
\begin{align*}
 \nabla_{\bm_x} \mathcal{L}(\bm{x}, \lambda) = \nabla f(\bm{x}) - \bm{\lambda} \nabla \bm{c}(\bm{x}) = 0
\end{align*}
Wir können also nun unser Optimierungsproblem mit Gleichheitsbedingungen in ein äquivalentes Optimierungsproblem ohne Nebenbedingungen umformen. Wenn für eine Komponente $i$ diese Bedingung bereits durch $\lambda_i = 0$ erfüllt ist, dann ist $c_i$ eine \tbf{Nebenbedingung ohne Relevanz}, d.h. das Minimum unter Einschränkung durch die Nebenbedingung $c_i$ ist dasselbe wie das Minimum ohne die Nebenbedingung. Man sagt in diesem Fall auch, dass die Nebenbedingung \tbf{inaktiv} ist. Die Menge der aktiven Nebenbedingungen wir auch als die \tbf{aktive Menge} bezeichnet.
%
\section{Optimierung mit Ungleichheitsbedingungen}
Auch hier ist die Parallelität der Gradienten eine notwendige Bedingung. Allerdings zählt nun das Vorzeichen von $\bm{\lambda}$, da wir $\bm{c}$ am Rand zwar nicht verringern können, aber beliebig erhöhen. Wir erhalten also die Bedingung
\begin{align*}
 f(\bm{x})  = \bm{\lambda c}(\bm{x}) \quad \bm{\lambda} \in \mathbb{R}_+^m
\end{align*}
bzw.
\begin{align*}
 \nabla_{\bm_x} \mathcal{L}(\bm{x}, \bm{\lambda}) = \nabla f(\bm{x}) - \bm{\lambda} \nabla \bm{c}(\bm{x}) = 0, \quad \forall i\ \lambda_i > 0
\end{align*}
Zusätzlich gilt am Optimum immer
\begin{align*}
 \bm{\lambda c}(\bm{x}) = 0,
\end{align*}
da sich für jedes $i$ entweder das Optimum auf dem Rand befindet, also $c_i(\bm{x}) = 0$, oder die Nebenbedingung inaktiv ist, also $\lambda_i = 0$. Wir sagen, eine Nebenbedingung ist \tbf{streng komplementär}, wenn immer nur genau einer dieser beiden Fälle auftritt.
\newpar
Zusammenfassend sind die \tbf{notwendigen Bedingungen} für ein lokales Optimum die folgenden:
\begin{align*}
\begin{array}{rl}
 \nabla_x \mathcal{L}(\bm{x}, \bm{\lambda}) = 0 \\
 c_i(\bm{x}) = 0 &\forall i \in \mathcal{E}\\
 \lambda_i c_i(\bm{x}) = 0 &\forall i \in \mathcal{I}\\
 c_i(\bm{x}) \geq 0 &\forall i \in \mathcal{I}\\
 \lambda_i \geq 0 &\forall i \in \mathcal{I}\\
\end{array}
\end{align*}
Diese Bedingungen sind bekannt als die \tbf{Karush-Kuhn-Tucker-Bedingungen (KKT)}. Später werden wir sehen, dass wir durch die Einführung weiterer Lagrange-Multiplikatoren Ungleichheitsbedingungen als weitere Gleichheitsbedingungen ausdrücken können. So werden wir letztendlich bei der Folgenden deutlich übersichtlicheren Liste landen:
\begin{align*}
\begin{array}{rl}
 \nabla_x {\mathcal{L}(\bm{x}, \bm{\lambda})} = \bm{0}\\
 c_i(\bm{x}) = 0 &\forall i \in (1,\hdots,m),\\
\end{array}
\end{align*}
oder noch genauer:
\begin{align*}
\begin{array}{rl}
 \pd{\mathcal{L}(\bm{x}, \bm{\lambda})}{x_i} = 0 &\forall i \in (1,\hdots,n)\vspace{1mm}\\
 c_i(\bm{x}) = 0 &\forall i \in (1,\hdots,m)\\
\end{array}
\end{align*}
%
\section{Dualität}
Mithilfe der Lagrange-Funktion können wir nun aus dem Originalproblem (auch \tbf{primales Problem} genannt)
\begin{align*}
\begin{array}{cc}
\argmin{x \in \mathbb{R}^n} &f(x)\\
 \text{subject to} &c_i(\bm{x}) = 0 \quad \forall i \in \mathcal{E}\\
 &c_i(\bm{x}) \geq 0 \quad \forall i \in \mathcal{I}\\
\end{array}
\end{align*}
ein sogenanntes \tbf{duales Problem} erhalten:
\begin{align*}
q(\bm{\lambda}) := \inf_{\bm{x} \in \mathbb{R}^n} \mathcal{L}(\bm{x}, \bm{\lambda}), \forall i: \lambda_i \geq 0\\
 \max_{\bm{\lambda} \in \mathbb{R}^m} q(\bm{\lambda})\\
\end{align*}
Intuitiv kann man sich vorstellen, dass für ein festes $\bm{x}$ die Lagrange-Funktion einen höheren Wert hat, je mehr Bedingungen erfüllt sind. Somit führt die Maximierung der dualen Funktion auch zu einer Optimierung der Originalfunktion. Die duale Problemlösung ist hauptsächlich dann sinnvoll, wenn es eine analytische Lösung zur Maximierung von $q(\bm{\lambda})$ gibt, und/oder wenn im dualen Problem keine Nebenbedingungen mehr existieren.
\newpar
\tbf{Lemma}: Für beliebige $\hat{\bm{\lambda}} \in \mathbb{R}^m$, $\hat{\bm{x}} \in \mathbb{R}^n$ gilt $q(\hat{\bm{\lambda}}) \leq f(\hat{\bm{x}} \in \mathbb{R}^n)$.\\
\tbf{Beweis}:
\begin{align*}
 q(\hat{\bm{\lambda}}) = \inf_{\bm{x}} f(\bm{x}) - \hat{\bm{\lambda}}^\top \bm{c(x)} \leq f(\hat{\bm{x}}) - \hat{\bm{\lambda}}^\top \bm{c(x)} \leq f(\hat{\bm{x}})
\end{align*}
Also erhalten wir eine untere Schranke für $f(\bm{x})$, indem wir $q(\hat{\bm{\lambda}})$ maximieren.
\newpar
\tbf{Lemma}: $q(\bm{\lambda})$ ist konkav.\\
\tbf{Beweis}:
\begin{align*}
 \mathcal{L}(\bm{x}, (1-\alpha)\bm{\lambda}_0 + \alpha \bm{\lambda}_1) &= f(\bm{x}) - (1 - \alpha) \bm{\lambda_0} \bm{c}(x) - \alpha \bm{\lambda}_1 \bm{c}(x)\\
 &= (1-\alpha) f(\bm{x}) + \alpha f(x) - (1 - \alpha) \bm{\lambda_0} \bm{c}(x) - \alpha \bm{\lambda}_1 \bm{c}(x)\\
 &= (1- \alpha) \mathcal{L}(\bm{x},\bm{\lambda}_0) + \alpha \mathcal{L}(\bm{x},\bm{\lambda}_1)\\
\end{align*}
Für beliebige Funktionen $f,g$ gilt im Allgemeinen $\inf_x(f(x) + g(x)) \leq \inf_x f(x) + \inf_x g(x)$, also:
\begin{align*}
 \inf_{\bm{x}}\mathcal{L}(\bm{x}, (1-\alpha)\bm{\lambda}_0 + \alpha \bm{\lambda}_1) \leq (1- \alpha) \inf_{x}  \mathcal{L}(\bm{x},\bm{\lambda}_0) +  \alpha \inf_{x} \mathcal{L}(\bm{x},\bm{\lambda}_1). \quad \square\\
\end{align*}
In vielen Fällen ist das duale Problem einfacher zu lösen als das primale Problem. Das duale Problem liefert eine untere Schranke für die Lösung des Originalproblems:
\begin{align*}
 \inf_{x} f(\bm{x}) - \bm{\lambda}^\top \bm{c(x)} \leq f(\bm{x}) - \bm{\lambda}^\top \bm{c}(\bm{x}) \leq f(\bm{x})
\end{align*}
Wir sprechen von \tbf{starker Dualität}, wenn das Optimum des dualen Problems auch das primale Problem minimiert. Dies ist der Fall unter folgenden Bedingungen:
\begin{itemize}
 \item $f(x)$ ist konvex
 \item Die gültige Menge $G$ ist  konvex
 \item $\mathcal{L}(x, \hat{\bm{\lambda}})$ ist \tbf{streng} konvex, wobei $\hat{\bm{\lambda}}$ die Lösung des dualen Problems ist.
\end{itemize}
Für ein ''gut benommenes`` Problem ist die Lösung des dualen Problems äquivalent zum Wert der konvexen Entspannung des primalen Problems, also zu dem Problem, was wir erhalten, wenn wir das feasible Set $G$ durch seine konvexe Hülle und die Funktion $f(\bm{x})$ durch ihre konvexe Schließung ersetzen\footnote{\href{https://en.wikipedia.org/wiki/Duality_gap?useskin=vector}{''Duality Gap``, Wikipedia}}. ''Gut benommen`` bedeutet in diesem Kontext, dass das Minimum die KKT-Bedingungen erfüllt - Gegenbeispiele sind hier oft Probleme mit seltsamen Feasible Sets, bei denen z.B. Ungleichheitsbedingungen existieren, welche nur einen einzigen Punkt zulassen. Es gibt eine lange Reihe an sogenannten Regularitätsbedingungen, die zu dieser Bedingung äquivalent sind. Einige Beispiele sind:
\begin{itemize}
 \item \tbf{Linearity Constraint Qualification (LCQ)}: Alle $c_i$ sind affin.
 \item \tbf{Linear Independence Constraint Qualification (LICQ)}: Die Gradienten der Ungleichheitsbedingungen und die Gradienten der Gleichheitsbedingungen sind am Minimum linear unabhängig.
 \item \tbf{Slater's Condition}: In einem Problem mit konvexem $f$, konvexen Ungleichheitsbedingungen und affinen Gleichheitsbedingungen existiert ein Punkt $\bm{x}$, welcher alle Gleichheitsbedingungen erfüllt und alle Ungleichheitsbedingungen Strikt erfüllt (also $c_i(\bm{x}) > 0$ anstatt nur $c_i(\bm{x}) \geq 0$).
 \item etc. etc.
\end{itemize}


%
%
%
\subsection{Beispiel: Optimierung mit Hilfe des dualen Problems}
Gegeben sei folgendes konvexes Problem:
\begin{align*}
\begin{array}{cc}
\argmin{x_1, x_2 \in \mathbb{R}} &\frac{1}{2} (x_1^2 + x_2^2)\\
 \text{s.t. } &x_1 - 1 \geq 0\\
\end{array}
\end{align*}
Die Lagrange-Funktion ist dann:
\begin{align*}
 \mathcal{L}(x_1, x_2, \lambda) = \frac{1}{2} (x_1^2 + x_2^2) - \lambda(x_1 - 1)
\end{align*}
Nötige Bedingung ist, dass die partiellen Ableitung von $\mathcal{L}$ Null sind, also:
\begin{align*}
 \pd{\mathcal{L}(x_1, x_2, \lambda)}{x_1} = x_1 - \lambda &= 0\\
 \pd{\mathcal{L}(x_1, x_2, \lambda)}{x_2} =x_2 &= 0\\
\end{align*}
Wir setzen diese Werte in die Lagrange-Funktion ein und erhalten:
\begin{align*}
 \mathcal{L}(x_1, x_2, \lambda) = \frac{1}{2} (x_1^2 + x_2^2) - \lambda(x_1 - 1) = \frac{1}{2} (\lambda^2 + 0^2) - \lambda(\lambda - 1) = -\frac{1}{2}\lambda^2 + \lambda\\
\end{align*}
Das Maximum dieser Funktion ist $\lambda = 1$. Unsere Lösung ist also $x_1 = \lambda = 1, x_2 = 0$.
%
%
%
\subsection{Wichtige Probleme mit starker Dualität}
\tbf{Lineare Programme} sind Probleme der Form:
\begin{align*}
\begin{array}{cc}
 \min{\bm{x}} &\bm{w}^{\top} \bm{x}\\
 \text{s.t.} &A \bm{x} - \bm{b} \geq \bm{0}
\end{array}
\end{align*}
Das duale Problem eines linearen Programms ist:
\begin{align*}
 \max_{\bm{\lambda}} \bm{b}^\top \bm{\lambda}, A^\top \bm{\lambda} = \bm{w}, \bm{\lambda} \geq 0
\end{align*}
\newpar
\tbf{Quadratische Programme} sind Probleme der Form:
\begin{align*}
\begin{array}{cc}
 \min{\bm{x}} &\frac{1}{2}\bm{x}^\top Q \bm{x} + \bm{w}^T \bm{x}\\
 \text{s.t.} &A \bm{x} - \bm{b} \geq \bm{0}
\end{array}
\end{align*}
Das duale Problem eines quadratischen Programms ist:
\begin{align*}
 \max_{\bm{\lambda}} -\frac{1}{2}(A^\top \bm{\lambda} - \bm{w})^\top Q^{-1}(A^\top \bm{\lambda} - \bm{w}) + \bm{b}^\top \bm{\lambda}, \bm{\lambda} \geq 0
\end{align*}
%
%
%
\iffalse \subsection{Alternative Herleitung}
Wir betrachten ein generelles Optimierungsproblem mit Ungleichheitsbedingungen:
\begin{align*}
\begin{array}{cc}
\argmin{x \in \mathbb{R}^n} &f(x)\\
 \text{subject to}
 &c_i(\bm{x}) \geq 0 \quad \forall i \in \mathcal{I}\\
\end{array}
\end{align*}
Dann ist die eigentlich zu minimierende Funktion gegeben durch:
\begin{align*}
 J(\bm{x}) = \begin{cases}
              f(x) & c_i(\bm{x}) \geq 0\ \forall i\\
              \infty & \text{else}\\
             \end{cases}
\end{align*}
Diese Funktion lässt sich mithilfe einer Step-Funktion $S(\bm{x})$ folgendermaßen Ausdrücken:
\begin{align*}
 S(\bm{x}) = \begin{cases}
              0 & x_i \geq 0\ \forall i\\
              -\infty & \text{else}\\
             \end{cases}\\
J(\bm{x}) = f(\bm{x}) - \sum_{i=1}^{m}S(c_i(\bm{x}))
\end{align*}
$J(x)$ ist nicht differenzierbar. Um sie zu optimieren, wollen wir sie also durch eine lineare Funktion approximieren. Das heißt, wir   suchen $\bm{\lambda}$, sodass $\bm{\lambda}^\top \bm{x} \approx S(x)$. Damit dies der Funktion $S(x)$ entspricht, wollen wir $\lambda_i \to -\infty$, wenn $c_i(\bm{x}) < 0$, und $\lambda_i = 0$ sonst. Für festes $\bm{x}$ gilt also
\begin{align*}
 &\max_{\bm{\lambda}} \mathcal{L}(x, \lambda) = J(\bm{x})\\
 \implies &\min{\bm{x}} J(\bm{x}) = \min{\bm{x}}\max_{\bm{\lambda}} \mathcal{L}(\bm{x}, \bm{\lambda})
\end{align*}
Im dualen Problem vertauschen wir die Reihenfolge der Zielfunktionen:
\begin{align*}
 \max_{\bm{\lambda}} \min{\bm{x}} \mathcal{L}(\bm{x}, \bm{\lambda}) \leq \min{\bm{x}}  \max_{\bm{\lambda}} \mathcal{L}(\bm{x}, \bm{\lambda})
\end{align*}
Was für manche Probleme einfacher ist und für andere schwieriger. Somit besteht das Duale Problem einfach aus der Suche nach der größten unteren Schranke. Diese ist exakt für konvexe Funktionen mit konvexen Nebenbedingungen.
\fi
%
%
%
\chapter{Lineare Programmierung}
Ist eine Zielfunktion linear, so ist auch ihr Gradient konstant. Dementsprechend gilt für die Lösung eines linearen Problems immer Einer der folgenden drei Fälle:
\begin{itemize}
 \item Es existiert keine Lösung, da die Nebenbedingungen widersprüchlich sind.
 \item Die Lösung liegt auf einer ''Ecke`` des feasible Sets. (Oder auf einer Kante, oder die Zielformel ist konstant und alle Punkte sind eine Lösung.)
  \item Das feasible Set ist in Richtung des negativen Gradienten nicht geschlossen, die Funktion wird beliebig klein.
\end{itemize}
Dieser Fakt wird genutzt durch das sogenannte \tbf{Simplex-Verfahren}, welches zu Beginn eine beliebige Ecke ''ausprobiert``, und dann immer zu der benachbarten Ecke geht, welche den Funktionswert verringert. Dabei wird der Fakt ausgenutzt, dass wir genau dann auf einer Ecke liegen, wenn $m$ Variablen $0$ sind, wobei $m$ der Anzahl an Variablen in der Zielfunktion entspricht.
\newpar
Ein Lineares Programm in \tbf{Standardform} sieht im Allgemeinen folgendermaßen aus:
\begin{align*}
  \min{\bm{x} \in \mathbb{R}^{m+n}}&\bm{w}^\top (x_0, \hdots, x_m)^\top\\
  s.t. & A\bm{x} - \bm{b}= 0\\
  &x_i \geq 0\\
\end{align*}
Wobei $A$ die Matrix ist, welche die Gleichheitsbedingungen enthält. Sind also alle Gleichheitsbedingungen gegeben durch $\bm{c}_i^\top \bm{x} = 0$, so ist \footnote{Ich betreibe hier im Namen der Übersichtlichkeit etwas unsaubere Notation, indem ich $c_i$ als eine Funktion interpretiere und $\bm{c}_i$ als einen Vektor, mit dem in der Funktion multipliziert wird, also $c_i(\bm{x}) = \bm{c}_i \bm{x}$}
\begin{align*}
A =
\begin{pmatrix}
\bm{c}_1\\
\vdots\\
\bm{c}_n
\end{pmatrix}
\text{, also }
A \bm{x} =
\begin{pmatrix}
c_1(\bm{x})\\
\vdots\\
c_n(\bm{x})
\end{pmatrix}
\end{align*}
Falls wir mit einem Programm mit $n$ Ungleichheitsbedingungen starten, so sind $x_{m+1}, \hdots, x_{m+n}$ sogenannte ''Schlupfvariablen``, welche wir hinzufügen, um eine Ungleichheitsbedingung $c_i(x_1, \hdots, x_m) \geq b_i$ als eine Gleichheitsbedingung $c_i(x_1, \hdots, x_m) - b_i - x_j = 0$ mit $x_i, x_{m+i} \geq 0$ auszudrücken. In der Vorlesung wurde dies komplizierter geschrieben - die Schlupfvariablen wurden $\xi_i$ genannt und das Gleichungssystem wurde kompliziert umgeformt, um auszudrücken, dass wir keine negativen $x_i$ haben dürfen. Ich persönlich finde diese vereinfachte Schreibweise mit $\bm{x} \in \mathbb{R}^{m+n}$ jedoch deutlich sinnvoller und  einleuchtender.
\newpar
An dieser Stelle möchte ich noch einmal kurz zur geometrischen Intuition hinter diesem Problem kommen. Jede der ursprünglichen Ungleichheitsbedingungen spannt eine Hyperebene auf, welche den Raum $\mathbb{R}^m$ in zwei Hälften teilt - für eine Ungleichheitsbedingung $c_i(x_1, \hdots, x_m) \geq b_i$ ist diese Hyperebene durch die Menge $H_i = \{(x_0, \hdots, x_m) \mid c_i(x_1, \hdots, x_m) = b_i\}$ gegeben. Die Punkte $\geq b_i$ sind dann die Hälfte des Raumes, welche in der Bedingung erlaubt ist, und die Punkte $< b_i$ sind die Hälfte des Raumes, welche nicht erlaubt ist. Durch die Bedingungen $x_i \geq 0$ an die ursprünglichen Variablen werden $m$ zusätzliche Hyperebenen an den Koordinatenebenen aufgespannt. Nimmt man alle Bedingungen zusammen, so ist das feasible Set $G$ eine konvexe Menge, deren Seiten durch die Hyperebenen begrenzt sind. Das ist entweder ein normaler Polyeder, oder bei unbeschränkten Problemen eine Art ''offener Polyeder``, welchen man sich als Polyeder vorstellen kann, bei dem die Ecken an Punkten in der Unendlichkeit liegen dürfen.
\newpar
Die Punkte im Lösungsraum sind trotz Einführung der Schlupfvariablen intuitiv gegeben durch die ursprünglichen $\bm{x} \in \mathbb{R}^m$. Also befinden wir uns im $m$-Dimensionalen Raum, dies bedeutet ein Punkt $\bm{x}$ ist auf einer Ecke, wenn sich an ihm $m$ Hyperebenen schneiden. Dies ist der Fall, wenn $\bm{x}$ auf mindestens $m$ der Hyperebenen gleichzeitig liegt.
\newpar
Was bedeutet das nun für unser Problem in Standardform, in dem die Ungleichheitsbedingungen durch Gleichheitsbedingungen ersetzt sind? Wenn $c_i(x_1, \hdots, x_m) \geq b_i$ die ursprüngliche Ungleichheitsbedingung ist, dann ist die äquivalente Gleichheitsbedingung $c_i(x_1, \hdots, x_m) - x_{m+1} = b_i$. Wenn also in der ursprünglichen Ungleichheitsbedingung $c_i$ bereits Gleichheit gegeben ist, so muss in der Gleichheitsbedingung $x_{m+i} = 0$ gelten.
\newpar
Wir befinden uns also genau dann auf einer Hyperebene, wenn entweder eine Schlupfvariable $x_{m+i}= 0$ ist - dann befinden wir uns auf der Ebene $H_i = \{(x_0, \hdots, x_m) \mid c_i(x_1, \hdots, x_m) = b_i\}$, die der Bedingung $c_i$ entspricht - oder wenn eine der Ursprünglichen Variablen $0$ ist - dann befinden wir uns auf der Ebene $x_i = 0$, welche der Bedingung $x_i \geq 0$ entspricht. Also sind wir immer dann auf einer Ecke, wenn mindestens $m$ der Variablen $0$ sind.
\newpar
Dies ist genau der Fakt, welchen sich das Simplex-Verfahren zunutze macht. Wir starten mit einem beliebigen Punkt $\bm{x} \in \mathbb{R}^{m+n}$. An jedem Schritt nehmen wir dann eine Variable $x_i$, welche momentan $0$ ist - es bietet sich an, die zu nehmen, welche in der Zielfunktion den negativen Koeffizienten mit dem höchsten Betrag hat, da diese zur Stärksten Verbesserung führt - und erhöhen diese. Dafür müssen wir dann eine andere Variable, welche mit $x_i$ in Relation steht, auf $0$ setzen. An jedem Schritt stellen wir unsere Gleichheitsbedingungen so um, dass jede Gleichheitsbedingung auf der linken Seite genau eine Variable enthält, welche momentan nicht $0$ ist, und setzen ebenso in der Zielfunktion immer die Variablen ein, welche gerade $0$ sind.
\newpar
Indem wir die Zielfunktion umstellen, erhalten wir eine simple Bedingung, an der wir erkennen können, ob wir das Optimum erreicht haben: Wir haben das Optimum erreicht, wenn alle Koeffizienten in der Zielfunktion positiv sind, da in diesem Fall jede Veränderung der Variablen zu einer Verschlechterung des Ergebnisses führt.
\newpar
Ich persönlich fand die in der Vorlesung gegebene Erklärung des Simplex-Verfahrens viel zu kompliziert und insgesamt sehr verwirrend. Dementsprechend möchte ich an dieser Stelle explizit das Video \href{https://www.youtube.com/watch?v=E72DWgKP_1Y}{''The Art of Linear Programming``, von Tom S}, erwähnen, durch welches das Simplex-Verfahren für mich endlich übersichtlich und intuitiv verständlich wurde.
%
%
%
\chapter{Nichtlineare Programmierung}
\end{document}

