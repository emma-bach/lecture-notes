\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{tikz-cd}
\usepackage{physics}
\usepackage{placeins}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\Hess}[1]{\text{Hess}(#1)}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\ve}{\vec{e}}
\newcommand{\vh}{\vec{h}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{0}}
\newcommand{\zz}{\vec{z}}

\newcommand{\tbA}{\mathbf{A}}
\newcommand{\tbB}{\mathbf{B}}
\newcommand{\tbC}{\mathbf{C}}
\newcommand{\tbD}{\mathbf{D}}
\newcommand{\tbE}{\mathbf{E}}
\newcommand{\tbY}{\mathbf{Y}}
\newcommand{\tbZ}{\mathbf{Z}}

\renewcommand{\tr}{\text{tr}}

\newcommand{\Mat}[3]{\text{Mat}^{#1}_{#2}\left(#3\right)}
\newcommand{\scalar}[2]{\left\langle #1, #2 \right\rangle}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Korollar}



\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{beispiel}[lemma]{Beispiel}
\newtheorem{beobachtung}[lemma]{Beobachtung}
\newtheorem{anmerkung}[lemma]{Anmerkung}
\newtheorem{question}[lemma]{Frage}
\newtheorem{application}[lemma]{Anwendung}
\newtheorem{konsequenz}[lemma]{Konsequenz}
%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
%
%
%
%
%
%
%\chapter{Wiederholung}
%
%
%
%
%
%
%
%
%
\chapter{Der Euklidische Raum}
\begin{lemma}
 Sei $(V, \scalar{\_}{\_})$ ein euklidischer Vektorraum. Dann wird durch
 \begin{align*}
  \norm{u} = \sqrt{\scalar{u}{u}}
 \end{align*}
auf $V$ eine Norm erklärt. Diese bezeichnet man als die durch das Skalarprodukt induzierte Norm.
\end{lemma}
\begin{definition}
 Seu $(V, \scalar{\_}{\_})$ ein euklidischer Vektorraum, Die Vektoren $u, v \in V$ heißen \tbf{orthogonal}, wenn \begin{align*}
   \scalar{u}{v} = 0                                                                                                                 
 \end{align*}
 ist. Für $u, v \in V \setminus \{0\}$ Wird die reelle Zahl
 \begin{align*}
  \phi = \arccos \frac{\scalar{u}{v}}{\norm{u}\ \norm{v}}
 \end{align*}
 als der Winkel zwischen $u$ und $v$ bezeichnet.
\end{definition}
\begin{anmerkung}
 Es gilt
 \begin{align*}
  \frac{\abs{\scalar{u}{v}}}{\norm{u}\ \norm{v}} \leq 1
 \end{align*}
\end{anmerkung}
\begin{lemma}
\label{lemma:normequiv}
 Für $X = (x_1, \hdots, x_n) \in \bR^n$ sei
 \begin{align*}
  \norm{X}_{\max} := \max\{\abs{x_1}, \hdots, \abs{x_n}\}
 \end{align*}
 Dann ist $||\_||_{\max}$ eine Norm auf $\bR^n$ und es gilt
 \begin{align*}
  \norm{X}_{\max} \leq \norm{X} \leq \sqrt{n}\norm{X}_{\max}
 \end{align*}
\end{lemma}
\begin{theorem}
Die Menge $\bQ^n$ der Punkte mit rational Koordinaten ist dicht in $\bR^n$.
\end{theorem}
\begin{proof}
Sei $X \in \bR^n$ und $\epsilon \in \bR^+$. Da $\bQ$ dicht in $\bR$ ist gilt
\begin{align*}
 \forall i \in \{1, \hdots, n\} : \exists y_i \in \bQ : \abs{x_i - y_i} \leq \frac{\epsilon}{\sqrt{n}}
\end{align*}
Durch Lemma \ref{lemma:normequiv} folgt:
\begin{align*}
 \norm{x-y} \leq \sqrt{n}\norm{X - Y} < \epsilon
\end{align*}
\end{proof}
\begin{theorem}
\label{theorem:componentcauchy}
 Sei $(X_k)_{k \in \bN}$ eine Folge aus $\bR^n$. Sei $X_k = (x_1^{(k)}, \hdots, x_n^{(k)})$. Dann gilt:
 \begin{align*}
  \lim_{k \to \infty} X_k = X \Leftrightarrow \forall i : \lim_{k \to \infty} x_i^{(k)} = x_i
 \end{align*}
 Insbesondere ist $X_k$ eine Cauchyfolge, wenn die Komponenten Cauchyfolgen sind.
\end{theorem}
\begin{proof}
 $X_k \to X$, $i \in \{1, \hdots, n\}$, $\epsilon \in \bR^+$. Dann gilt
 \begin{align*}
  \exists k_o \in \bN : \forall k \geq k_0 : \norm{X_k - X} \leq \epsilon \implies \forall i : \abs{x_i^{(k)} - x_i} < \epsilon \implies \lim_{k \to \infty} x_i^{(k)} = x_i
 \end{align*}
 Und umgekehrt:
 \begin{align*}
  \forall i : x_i^{(k)} \to x_i, \epsilon \in \bR^+ \implies \exists k_0^i \in \bN : \forall k \geq k_0^i \abs{x_i^{(k)} - x_i} \leq \frac{\epsilon}{\sqrt{n}}
 \end{align*}
 \[
  k_0 := \max\{k_0^n, \hdots, k_0^n\} \implies \forall k \geq k_0 : \abs{x_i^{(k)} - x_i} < \frac{\epsilon}{\sqrt{n}} \implies \norm{X_k - X} \leq \sqrt{n}\norm{X_k - X} < \epsilon
 \]
\end{proof}
\begin{theorem}
 Für konvergente Folgen $(X_k),(Y_k) \in \bR^n$, $(\lambda_k) \in \bR$ gilt:
\begin{align}
 \lim_{k \to \infty} (X_k + Y_k) = \lim_{k \to \infty} X_k + \lim_{k \to \infty} Y_k\\
 \lim_{k \to \infty} \lambda_k X_k = \left(\lim_{k \to \infty} \lambda_k\right)\left(\lim_{k \to \infty}
 X_k\right)\\
 \lim_{k \to \infty} \scalar{X_k}{Y_k} = \scalar{\lim_{k \to \infty} X_k}{\lim_{k \to \infty} Y_k}
\end{align}
\end{theorem}
\begin{theorem}
 $\bR^n$ ist vollständig.
\end{theorem}
\begin{proof}
Ist $X_k$ eine Cauchyfolge in $\bR^n$, so sind nach Satz \ref{theorem:componentcauchy} alle Teilfolgen Cauchy in $\bR$. Also:
\begin{align*}
 \exists x_i \in \bR : x_i^{(k)} \to x_i \implies \exists X \in \bR^n : X_k \to X
\end{align*}
\end{proof}
\begin{theorem}
\emph{\textbf{(Bolzano-Weierstrass:)}} Jede beschränkte Folge in $\bR^n$ besitzt eine konvergente Teilfolge.
\end{theorem}
\begin{proof}
 Sei $(X_k)$ eine beschränkte Folge in $\bR^n$. Nach \ref{lemma:normequiv} müssen die Komponentenfolgen ebenfalls beschränkt sein. Nach dem eindimensionalen Fall des Satzes von Bolzano-Weierstrass existieren also konvergente Teilfolgen der Koordinatenfolgen. Angenommen, die konvergente Teilfolge der ersten Komponente ist gegeben durch $x_1^{(k_n)} \to x_1$. So ist $x_2^{(k_n)}$ ebenfalls eine beschränkte Teilfolge, also existiert eine Teilfolge $x_2^{(k_n)_m}$ welche in den ersten beiden Komponenten konvergiert. Führt man dieses Verfahren induktiv fort, erhält man eine konvergente Teilfolge von $(X_k)$.
\end{proof}
\begin{theorem}
 Sei $(A_i)_{i \in \bN}$ eine Folge abgeschlossener beschränkter nichtleerer Teilmengen des $\bR^n$, sodass $A_1 \supseteq A_2 \supseteq \hdots$ Dann ist $\bigcap_{i \in \bN} \neq \emptyset$
\end{theorem}
\begin{proof}
 $A_i \neq \emptyset \implies \exists X_i \in A$ sd. $(X_i)_{i \in \bN}$ eine Folge ist. Da $A_i$ beschränkt ist ist $(X_i)_{i \in \bN}$ beschränkt, also hat $X_i$ eine konvergente Teilfolge $X_{i_k}$ mit Limes $X$. Es gilt $X_{i_k} \in A_{i_k} \subseteq A_i$, also ist $X$ ein Berührpunkt von $A_i$, also $X \in A_i$.
\end{proof}
\begin{theorem}
 Jede abgeschlossene beschränkte Teilmenge des $\bR^n$ ist kompakt.
\end{theorem}
\begin{proof}
 Analog zur eindimensionalen Version, wobei statt Intervallen $[a_i,b_i]$ Hyperwürfel $[a_i^{(1)}, b_i^{(1)}] \times \hdots \times [a_i^{(n)}, b_i^{(n)}]$ genutzt werden müssen.
\end{proof}
\begin{theorem}
\label{theorem:allnormsequiv}
 Seien $\norm{\_}_1$ und $\norm{\_}_2$ Normen auf $\bR^n$. So existieren $k, K \in \bR^+$ mit
 \begin{align*}
  \forall X \in \bR^n : k\norm{X}_1 \leq \norm{X}_2 \leq K\norm{X}_1
 \end{align*}
\end{theorem}
\begin{proof}
Diese Normenäquivalenz bildet eine Äquivalenzrelation. Es reicht also, zu zeigen, dass jede Norm $||\_||_2$ äquivalent zu einer spezifischen Norm $\norm{\_}_1$ ist. Wir wählen $\norm{\_}_{\max}$.\\
Sei $(E_i)$ die Standardbasis des $\bR^n$. Wir definieren:
\begin{align*}
 K := \norm{E_1}_2 + \hdots + \norm{E_n}_2
\end{align*}
Dann gilt:
\begin{align*}
 \norm{X}_2 &= \norm{x_1 E_1 + \hdots + x_n E_n}\\
         &\leq \abs{x_1}\norm{E_1}_2 + \hdots + \abs{x_n} \norm{E_n}_2\\
         &\leq \norm{X}_{\max} K \quad ^\text{[citation needed]}
\end{align*}
Es bleibt die Rückrichtung zu zeigen. 
\begin{lemma}
 $f(X) := \norm{X}_2$ ist stetig.
\end{lemma}
\begin{proof}
\begin{align*}
 \abs{\norm{X}_2 - \norm{Y}_2} \leq \norm{X - Y}_2 \leq K\norm{X - Y}_{\max} \leq K \norm{X - Y}
\end{align*}
Also ist $\norm{\_}_2$ stetig bezüglich der euklidischen Norm $\norm{\_}$.
\end{proof}
Wir definieren nun:
\begin{align*}
 A := \{X \in \bR^n \mid ||X||_{\max} = 1\}
\end{align*}
Diese Menge ist beschränkt. Wir wollen Zeigen, dass sie außerdem abgeschlossen ist. Sei $X_i \to X$, $X_i \in A$. Es gilt:
\begin{align*}
  \abs{\norm{X_i}_{\max} - \norm{X}_{\max}} \leq \norm{X_i - X}_{\max} \leq \norm{X_i - X}
\end{align*}
Also konvergiert jede Menge, also ist $A$ kompakt, also auch abgeschlossen. Dementsprechend muss $f$ auf $A$ ein Minimum $k$ annehmen. Wir wissen $f \geq 0$, also ist$k \geq 0$. Es gilt sogar $k > 0$, da keiner der Vektoren in $A$ der Nullvektor ist. Nun gilt also $\forall X \in A : ||X||_2 \geq k$. Wir definieren:
\begin{align*}
 \lambda := \frac{1}{\norm{X}_{\max}}
\end{align*}
\begin{align*}
 \norm{\lambda X}_{\max} = \abs{\lambda} \norm{X}_{\max} = 1 
\end{align*}
\begin{align*}
 |\lambda| \norm{X}_2 = \norm{\lambda X}_2 \geq k \implies \norm{X_2} \geq k \norm{X}_{\max}
\end{align*}
\end{proof}
\begin{anmerkung}
 Im unendlichdimensionalen Fall gilt Satz \ref{theorem:allnormsequiv} nicht.
\end{anmerkung}
%
%
%
%
%
%
%
%

%
%
%
%
\section{Abbildungen und Koordinatenfunktionen auf $\bR^n$}
In diesem Abschnitt betrachten wir Funktionen $F: \bR^n \to \bR^k$. Betrachten wir zuerst den Spezialfall Linearer Funktionen, also $\forall X,Y \in \bR^n : \forall \lambda, \mu \in \bR : F(\lambda X + \mu Y) = \lambda F(X) + \mu F(Y)$.
\newpar
Sei $(E_i)$ die Standardbasis des $\bR^n$ und sei $(E_i')$ die Standardbasis des $\bR^k$. Nun gilt:
\begin{align*}
 F(E_j) = \sum_{i=1}^k a_{ij} E_i'
\end{align*}
Daraus erhalten wir Koeffizienten $a_{ij}$, welche eine Matrix bilden. Umgekehrt können wir aus den Koeffizienten die Abbildung $F$ rekonstruieren, indem wir definieren:
\begin{align*}
 F(X) &= F\left(\sum_{j=1}^n x_j E_j\right) \\
 &= \sum x_j F(E_j) \\
 &= \sum_{j=1}^n x_j \sum_{i=1}^k a_{ij} E_i'\\
 &= \sum_{i=1}^k \left(\sum_{j=1}^n a_{ij} x_j\right) E_i'
\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\newpage
[missing stuff here]
\newpage
\begin{definition}
 Wir bezeichnen als $p_i : M \to k$ die Projektion eines Vektors auf die $i$-te Komponente.
\end{definition}
\begin{theorem}
\label{theorem:componentwisecontinuous}
 Sei $M$ ein metrischer Raum, $F : M \to \bR^n$ eine Abbildung und $x \in M$. Dann ist $F$ stetig in $x$ genau dann, wenn $p_i \circ F$ stetig für alle $i$ ist.
\end{theorem}
\begin{proof}
\begin{enumerate}
 \item $p_i$ ist stetig. Ist also $F$ stetig folgt direkt, dass auch $p_i \circ F$ stetig ist.
 \item Angenommen, $p_i \circ F$ ist stetig $\forall i$, $\epsilon \in \bR^+$. Da $p_i \circ F$ stetig ist existiert eine Umgebung $U_i$ von $x$, sodass $|f_i(x) - f_i(y)| < \frac{\epsilon}{\sqrt{n}} \forall y \in U_i$. Ebenso für die anderen Komponenten. Nun gilt:
 \begin{align*}
  \norm{F(y) - F(x)} \leq \sqrt{n} \norm{F(x) - F(y)}_{\max} \leq \epsilon
 \end{align*}
\end{enumerate} 
\end{proof}
Analog gilt das Selbe für Stetigkeit auf $M$, gleichmäßige Stetigkeit, etc.
\begin{definition}
 Sei $M \subseteq \bR^n$, $F : M \to \bR^k$ eine Abbildung, $x_0$ ein Häufungspunkt, $y \in \bR^k$. Dann definieren wir:
 \begin{align*}
  \lim_{x \to x_0} F(x) = y \Leftrightarrow \forall \epsilon \in \bR^+ : \exists \delta \in \bR^+ : \forall x \in M \setminus \{x_0\} : \norm{x - x_0} \leq \delta \implies \norm{F(x) - y} < \epsilon
 \end{align*}
 $F$ ist stetig in $x_0$ genau dann, wenn $\lim_{x \to x_0} F(x) = F(x_0)$.
\end{definition}
\begin{theorem}
 Sei $M \subseteq \bR^n$, $F : M \to \bR^k$ eine Abbildung, $X_0 \in M$ ein Häufungspunkt, $Y \in \bR^k$ und $f_i = p_i \circ F$. Dann gilt:
 \begin{align*}
  \lim_{X \to X_0} F(X) = Y \Leftrightarrow \forall i : \lim_{X \to X_0} f_i(X) = y_i
 \end{align*}
\end{theorem}
\begin{proof}
 Analog zu Beweis \ref{theorem:componentwisecontinuous}.
\end{proof}
\begin{corollary}
 \begin{align*}
  F(X) \to Y, G(X) \to Z \implies F(X) + G(X) \to Y + Z
 \end{align*}
\end{corollary}
\section{Mehrdimensionale Ableitungen}
\begin{beispiel}
 Sei $f : M \to \bR$ definiert auf einer offenen Menge $M \subseteq \bR^n$.
 \begin{align*}
  f(X) = f(x_1, \hdots, x_n) \text{ bzgl. der Standardbasis }
 \end{align*}
 Wir können aber auch $X = \sum x_i' E_i'$ bezüglich einer beliebigen anderen Basis darstellen. Also:
 \begin{align*}
  f(X) = f(x_1, \hdots, x_n) = g(x_1', \hdots, x_n')
 \end{align*}
 Da $f$ in der Regel nicht linear ist, ist ein solcher Basiswechsel sehr viel komplizierter als in der Linearen Algebra! Wo möglich ist es also besser, über $f(X)$ zu reden.
\end{beispiel}
\begin{definition}
 Sei $f : \bR^n \to \bR$, $\overline{X} \in M$. Betrachte die Abbildung \[t \to f(\overline{x}_1, \hdots \overline{x}_{i-1}, t, \overline{x}_{i+1}, \hdots, \overline{x}_n),\] welche eine Mehrdimensionale Funktion $f(x_1, \hdots x_n)$ auf eine eindimensionale Funktion $f(t)$ abbildet. \ul{Achtung:} Wir nehmen hier implizit eine Darstellung bezüglich der Standardbasis an!
\end{definition}
\begin{beispiel}
 Betrachte folgende Funktion:
 \begin{align*}
  f(x,y) = \begin{cases}
            \frac{xy}{x^2 + y^2} & (x,y) \neq (0,0)\\
            0 & (x,y) = (0,0)\\
           \end{cases}
 \end{align*}
 $f$ ist an $(0,0)$ partiellen differenzierbar, die Partiellen Ableitungen sind $0$. Allerdings gilt \begin{align*}
  \forall x : f(x,x) = \frac{1}{2}
 \end{align*}
 Also ist $f$ an $0$ nicht stetig! Es existieren also Funktionen, die an einem Punkt partiell Differenzierbar sind, an dem sie nicht stetig sind.
\end{beispiel}
\ul{Idee:} Fordere partielle Differenzierbarkeit bezüglich jeder möglichen Basis, also partielle Differenzierbarkeit in jedem Vektor.
\begin{beispiel}
 \begin{align*}
  f(x,y) = \begin{cases}
            \frac{x^2y}{x^4 + y^2} & (x,y) \neq (0,0)\\
            0 & (x,y) = (0,0)\\
           \end{cases}
 \end{align*}
 Wir betrachten die ``Linearisierung'' $t \to f(t, \alpha t)$. Einsetzen liefert:
 \begin{align*}
  f(t, \alpha t) = \frac{\alpha t}{t^2 + a^2}
 \end{align*}
 Diese Funktion ist differenzierbar, also ist $f$ differenzierbar bezüglich beliebiger Basen. Das reicht jedoch immer noch nicht:
 \begin{align*}
  f(a, a^2) = \frac{a^2a^2}{a^4+a^4} = \frac{1}{2}
 \end{align*}
 Also ist $f$ immer noch nicht stetig - es ist stetig für Folgen, welche den Nullpunkt durch Geraden erreichen, aber nicht, wenn wir durch kompliziertere Pfade gegen den Nullpunkt gehen.
\end{beispiel}
\newpar
Wir wollen die Begriffe aus der Analysis I über Stetigkeit und Ableitbarkeit retten, also brauchen wir einen komplizierteren Ableitungsbegriff.
\section{Differenzierbarkeit}
Sei $f$ eine beliebige Funktion $\bR \to \bR$. Die Ableitung gibt uns die Tangente der Funktion an einem beliebigen Punkt, also die beste affine Approximation der Funktion an diesem Punkt.
\begin{definition}
 Eine Funktion $F : \bR^n \to \bR^k$ heißt \tbf{affin}, wenn es eine Lineare Funktion $L : \bR^n \to \bR^k$ und eine Konstante $Z \in \bR^k$ gibt, sodass:
 \begin{align*}
  F(X) = L(X) + Z
 \end{align*}
\end{definition}
\newpar
Sei $g : \bR \to \bR$ affin, also $g(x) = cx + t$ für $c,t \in \bR$. Sei $f : \bR \to \bR$. Wir wollen eine beliebige Funktion $f$ an der Stelle $x_0$ approximieren. Für eine gute Approximation wollen wir $f(x_0) = g(x_0)$, also erhalten wir:
\begin{align*}
 g(x) = c(x - x_0) + f(x_0).
\end{align*}
Schreibe $x = x_0 + h$ und lasse $h$ gegen $0$ gehen.
\begin{align*}
 h \to f(x_0 + h) - g(x_0 + h) = f(x_0 + h) - f(x_0) - ch
\end{align*}
Wir sagen, die Approximation ist gut, wenn $f(x_0 + h) - f(x_0) - ch$ schneller gegen $0$ geht als $h$ selbst, also:
\begin{align}
\label{eq:goodapprox}
 \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0) - ch}{h} = 0
\end{align}
Was äquivalent ist zu:
\begin{align*}
 c = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}\\
\end{align*}
Wir sagen also, $f$ ist in $x_0$ differenzierbar, genau dann, wenn eine lineare Abbildung $L$ existiert, sodass:
\begin{align*}
 \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0) - L(h)}{h} = 0
\end{align*}
Diese geometrische Intuition, nach der die Ableitung die beste affine Approximation der Funktion an einem gegebenen Punkt ist, können wir auf den $\bR^n$ übertragen. Analog zu der Interpretation affiner Funktionen als Geraden in $\bR$, also der Ableitung als das Finden einer Tangentengeraden auf dem Funktionengraph, sucht man beim Ableiten einer Mehrdimensionalen Funktion eine Tangenten(hyper-)ebene auf dem Funktionengraph.
\begin{definition}
 Sei $M \subset \bR^n$ offen, $F : M \to \bR^k$ eine Abbildung, sei $X_0 \in M$. Die Abbildung $F$ heißt \tbf{differenzierbar} am Punkt $X_0$, wenn es eine Lineare Abbildung $L : \bR^n \to \bR^k$ gibt, sodass:
 \begin{align*}
  \lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - L(H)}{\norm{H}} = 0.
 \end{align*}
 Wir nennen sie das \tbf{Differenzial von $F$ im Punkt $X_0$} und notieren sie als $DF_{X_0}$. $F$ heißt differenzierbar, wenn sie differenzierbar an jedem Punkt $X \in M$ ist.
\end{definition}
\begin{anmerkung}
 Differenzierbarkeit kann analog über die Eigenschaften des Restglieds $R(X, X_0)$ definiert werden: Sei
 \begin{align*}
  f(X) = f(X_0) + D f_{X_0}(X - X_0) + R(X, X_0).
 \end{align*}
 Dann ist $f$ genau dann differenzierbar, wenn:
 \begin{align*}
  \lim_{X \to X_0} \frac{R(X, X_0)}{\norm{X - X_0}} = 0
 \end{align*}

\end{anmerkung}

\begin{theorem}
 Gibt es ein Differential, ist es eindeutig bestimmt.
\end{theorem}
\begin{proof}
 Seien $L_1, L_2$ Differentiale. Es folgt:
 \begin{align*}
  \lim_{H \to 0} \frac{L_1(H) - L_2(H)}{\norm{H}} = 0
 \end{align*}
 Sei $X \in \bR^n \setminus \{0\}$. Dann gilt:
 \begin{align*}
  \lim_{t \to 0} \frac{L_1(tX) - L_2(tX)}{\norm{tX}} = 0
 \end{align*}
 %also:
 \begin{align*}
  \implies \frac{L_1(X) - L_2(X)}{\norm{X}} = 0
 \end{align*}
 %also:
 \begin{align*}
  \implies L_1(X) - L_2(X) = 0
 \end{align*}
 also sind die beiden Differentiale identisch.
\end{proof}
\begin{anmerkung}
 Unserer Differenzierbarkeitsbegriff wird insbesonders in der älteren Literatur oft als \tbf{totale Differenzierbarkeit} bezeichnet.
\end{anmerkung}
\begin{theorem}
 Ist $F: M \to \bR^k$ an einem Punkt $X_0$ differenzierbar, so ist $F$ an diesem Punkt stetig.
\end{theorem}
\begin{proof}
 Sei $F$ differenzierbar. Da die Differenzierbarkeit über den Limes des Differentialquotienten definiert ist folgt direkt:
 \begin{align*}
  \forall \epsilon \in \bR^+ : \exists \delta_1 \in \bR^+ : \forall H \in M : (X_0 + H \in M) \wedge (0 \leq \norm{H} \leq \delta_1)\\ 
  \implies \frac{\norm{F(X_0 + M) - F(X_0) - DF_{X_0}(H)}}{\norm{H}} \leq \frac{\epsilon}{2}\\
  \implies \norm{F(X_0 + M) - F(X_0) - DF_{X_0}(H)} \leq \frac{\epsilon}{2}\norm{H}\\
 \end{align*}
 Da $DF_{X_0}$ eine lineare Abbildung ist ist $DF_{X_0}$ gleichmäßig stetig, also gilt:
 \begin{align*}
  \exists \delta_2 \in \bR^+ : \norm{H} < \delta_2 \implies \norm{DF_{X_0}(H)} \leq \frac{\epsilon}{2} 
 \end{align*}
 Also gilt für $\norm{H} \leq \delta := \min\{\delta_1, \delta_2, 1\}$
 \begin{align*}
  &\norm{F(X_0 + H) - F(X_0)}\\
  = &\norm{F(X_0 + H) - F(X_0) - DF_{X_0}(H) + DF_{X_0}(H)}\\
  \leq &\norm{F(X_0 + H) - F(X_0) - DF_{X_0}(H)} + \norm{DF_{X_0}(H)}\\
  \leq &\frac{\epsilon}{2}\norm{H} + \frac{\epsilon}{2}\\
  \leq &\epsilon
 \end{align*}

\end{proof}

\begin{theorem}
 Sind $F$ und $G$ differenzierbar, so auch $F + G$, und es gilt
 \begin{align*}
  D(F + G)_{X_0} = DF_{X_0} + DG_{X_0}
 \end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
 &\lim_{H \to 0} \frac{(F+G)(X_0 + H) - (F + G)(X_0) - (DF_{X_0} + DG_{X_0})}{\norm{H}}\\
 = &\lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - DF_{X_0}}{\norm{H}} + \lim_{H \to 0} \frac{G(X_0 + H) - G(X_0) - DG_{X_0}}{\norm{H}}\\
 = &0\\
\end{align*}
\end{proof}

\begin{theorem}
\ul{\tbf{Kettenregel:}} Seien $M \subseteq \bR^n$, $N \subseteq \bR^n$ offen, seien $F: M \to N$, $G : N \to \bR^m$ Abbildungen, sei $X_0$
\end{theorem}
\begin{proof}
 Sei $F(X_0) = Y_0, F(X_0 + H) - F(X_0) = Z$, $H \in \bR^n \setminus \{0\}$, $X_0 + H \in M$.
 \begin{align*}
    &\frac{1}{\norm{H}} ((G \circ F)(X_0 + H) - (G \circ F)(X_0) - DG_{F(X_0)} \circ DF_{X_0}(H))\\
    =&\frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H))\\
    =&\frac{1}{\norm{H}}(DG_{Y_0}(F(X_0 + H) - F(X_0)) - DG_{Y_0}(DF_{X_0}(H)))\\
    =&\frac{1}{\norm{H}}DG_{Y_0}((F(X_0 + H) - F(X_0)) - DF_{X_0}(H))\\
 \end{align*}
 \begin{align*}
  \lim_{H \to 0}\frac{1}{\norm{H}}DG_{Y_0}((F(X_0 + H) - F(X_0)) - DF_{X_0}(H)) = DG_{Y_0}(0) = 0\\
 \end{align*}
 \begin{align*}
  \frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H)) = 
  \begin{cases}
    0 & Z_H = 0\\
    \frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H)) & Z_H \neq 0 
  \end{cases}
 \end{align*}
 Der Term zweite Term in $Z_H \neq 0$ geht gegen $0$ für $H \to 0 \implies Z_H = F(X_0 + H) - F(X_0) \to 0$
 \begin{align*}
  \frac{\norm{Z_H}}{\norm{H}} &= \frac{\norm{F(X_0 + H) - F(X_0)}}{\norm{H}} \\
  &= \frac{\norm{DF_{X_0}(H) - R(X_0, H)}}{\norm{H}}\\ 
  &\leq \frac{\norm{DF_{X_0}(H)}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\leq \frac{\norm{DF_{X_0}(H)}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\overset{???}{\leq} \frac{\norm{DF_{X_0}}\norm{H}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &= \norm{DF_{X_0}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\leq c\\
 \end{align*}
\end{proof}
\begin{theorem}
 Seien $I \subseteq \bR$, $N \subseteq \bR^k$ offen, seien $F : I \to N$, $G : N \to \bR^n$ Abbildungen.
 \newpar
 Ist $F$ differenzierbar in $t_0 \in I$ und $G$ differenzierbar in $F(t_0)$, so gilt:
 \begin{align*}
  (G \circ F)'(t_0) = DG_{F(t_0)} (F'(t_0))
 \end{align*}
\end{theorem}
\begin{proof}
 Gemät Kettenregel gilt $D(G \circ F) = DG_{F(t_0)} \circ DF_{t_0}$. Nun gilt:
 \begin{align*}
  h(G \circ F)'(t_0) &= h D(G \circ F)_{t_0} (1)\\ &= D(G \circ F)_{t_0}(h)\\ &= DG_{F(t_0)}(DF_{t_0}(h))\\
  &= h DG_{F(t_0)}(F'(t_0))
 \end{align*}
\end{proof}
\newpar
Mittelwertsatz: $f : [x,y] \to \bR$, dann $\exists y : f(y) - f(x) = f'(z)(y-x)$. Im Allgemeinen ist dieser im Mehrdimensionalen Fall leider falsch.
\newpar
Betrachte allerdings die folgende Ungleichung, welche die Wichtigste Konsequenz des Mittelwertsatzes ist: $\abs{f(y) - f(x)} \leq \abs{f'(z)}\abs{y-x} \leq c \abs{y-x}$. Diese kann im Allgemeinen erhalten werden.
\newpar
$F : M \subset \bR^n \to \bR^k$ $X,Y \in M$. Sei $[X,Y] = \{(1- \lambda)X + \lambda Y\}$ die Verbindungslinie zwischen den beiden Vektoren.
\begin{theorem}
 Sei $M \subseteq \bR^n$ offen, $X,Y \in M$ mit $[X,Y] \subseteq M$. Die Abbildung $F : M \to \bR^k$ sei stetig in $M$ und differenzierbar in den Punkten $(1-\lambda)X + \lambda Y$ mit $\lambda \in (0,1)$. Gilt
 \begin{align*}
  \forall \lambda \in (0,1) : \forall (1-\lambda)X + \lambda Y : \norm{DF_Z} \leq c 
 \end{align*}
 so gilt auch
 \begin{align*}
  \norm{F(Y) - F(X)} \leq c\norm{Y - X}
 \end{align*}
 \begin{proof}
  Angenommen $G : [0,1] \to \bR^k$ ist stetig auf $[0,1]$ und differenzierbar auf $(0,1)$. So gilt 
  \begin{align*}
  \forall t \in (0,1) : \norm{G'(t)} \leq c
  \end{align*}
 \newpar
 Sei $\epsilon \in \bR^+$ und 
 \begin{align*}
  A := \{t \in [0,1] \mid \norm{G(t) - G(0)} \leq (c + \epsilon) t + \epsilon\}
 \end{align*}
 Da $G$ stetig in $0$ ist gilt $[0, \tau] \subseteq A$.
 \newpar
 Sei $s = \sup A$. Es gilt $0 < s \leq 1$, also ist $G$ stetig in $s$.
 \newpar
 Da $t \in A \implies t \leq s$ 
 \begin{align*}
  \norm{G(t) - G(0)} \leq (c + \epsilon) t + \epsilon \to s
  \implies \norm{G(s) - G(0)} \leq (c + \epsilon) s + \epsilon
 \end{align*}
 also $s \in A$. Angenommen, $s < 1$. Dann gilt $\exists h > 0 : s + h < 1$.
 \begin{align*}
  \norm{\frac{G(s + h) - G(s)}{h} - G'(s)} \leq \epsilon
 \end{align*}
 \begin{align*}
  \implies \norm{\frac{G(s + h) - G(s)}{h}} \leq \epsilon +  G'(s) \leq c + \epsilon
 \end{align*}
 \begin{align*}
  \norm{G(s + h) - G(0)} &\leq \norm{G(s + h) - G(s)} + \norm{G(s) - G(0)}\\
                         &\leq (c + \epsilon) h + (c + \epsilon) s + \epsilon\\
                         &\leq (c + \epsilon)(s + h) + \epsilon\\
 \end{align*}
 Daraus folgt $s + h \in A$. Da $s$ das Supremum ist ist dies ein Widerspruch. Also gilt $h = 1$.
 \begin{align*}
  \forall \epsilon \in \bR^+ : \norm{G(1) - G(0)} \leq &c + \epsilon + \epsilon = c + 2 \epsilon\\
  \implies &\norm{G(1) - G(0)} \leq c
 \end{align*}
 Sei $F$ wie im Satz. Sei $K : [0,1] \to \bR^n : t \to (1-t)X + tY$. Diese Abbildung ist affin, also differenzierbar. Es gilt $K'(t) = Y - X$. $F \circ K$ ist diffbar in $(0,1)$
 \begin{align*}
  D(F \circ K)_t = DF_{K(t)} \circ DK_t
 \end{align*}
 \begin{align*}
  (F \circ K)'(t) = DF_{K(t)}(K'(t)) = DF_{K(t)}(Y-X)
 \end{align*}
 \begin{align*}
  \norm{(F \circ K)'(t)} = \norm{DF_{K(t)}(Y - X)} \leq \norm{DF_{K(t)}}\norm{Y - X} \leq c \norm{Y - X}
 \end{align*}
 Mit $G := F \circ K$ und $c := c\norm{Y - X}$
 \begin{align*}
  \norm{F(Y) - F(X)} \leq c\norm{Y - X}
 \end{align*}

 \end{proof}
\end{theorem}
[missing stuff - gradients]
\begin{definition}
 Eine Funktion $f$ heißt \tbf{partiell differenzierbar}, wenn für jede Koordinatenachse $i$ die Partielle Ableitung $\forall i \in \{0, \hdots, n\} : \partial_i f : M \subseteq \bR \to \bR : X \to \delta_i f(X)$ existiert.
\end{definition}
\begin{theorem}
 Ist $f: M \to \bR$ in einer Umgebung von $X_0$ partiell differenzierbar und sind die partiellen Ableitungen in $X_0$ stetig, so ist $f$ in $X_0$ differenzierbar.
\end{theorem}
\begin{proof}
 Sei $U$ ein offener Ball um $X_0$, welcher vollständig in $M$ enthalten ist. Sei $H \in \bR^n$, sodass $X_0 + H \in U$. Nun gilt:
 \begin{align*}
  f(X_0 + H) - f(X_0) &= \sum_{i=1}^n (f(x_1, \hdots, x_{i-1}, \hdots, x_i + h_i, x_{i+1} + h_{i+1} , \hdots, x_n + h_n))\\
  &- \sum_{i=1}^n (f(x_1, \hdots, x_{i-1}, \hdots, x_i, x_{i+1} + h_{i+1} , \hdots, x_n + h_n))\\
 \end{align*}
 Die Summenglieder sind partielle Ableitung. Nach Mittelwertsatz erhalten wir:
 \begin{align*}
  \sum_{i=1}^n h_1 \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1}, \hdots, x_n)\ c_i \in (0,1)\\
 \end{align*}
 Nun gilt:
 \begin{align*}
  &\frac{1}{\norm{H}} \abs{f(X_0 + H) - f(X_0) - \scalar{\grad f(X_0)}{H}}\\
  =&\frac{1}{\norm{H}} \abs{\sum_{i=1}^n h_1 \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1} + h_{i+1}, \hdots, x_n + h_n) - \partial_i(f(x_0, \hdots, x_n))} \\
  \leq&\abs{\sum_{i=1}^n \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1} + h_{i+1}, \hdots, x_n + h_n) - \partial_i(f(x_0, \hdots, x_n))} \to 0\\
 \end{align*}
\end{proof}
 Sei $M \subseteq \bR^n$ offen, $X_0 \in M$, $F: M \to \bR^k$. Seien $\forall i \in \{1, \hdots, n\} f: M \to \bR^n \to \bR$ Koordinatenfunktionen.
 \begin{align*}
  F(X) = (f_1(X), \hdots, f_k(X)) = \sum_{i=1}^k f_i(X)E_i'
 \end{align*}
 \begin{align}
  Y = F(X) \Leftrightarrow \forall i : y_i = f_i(x_1, \hdots, x_n)
 \end{align}
\begin{theorem}
Die Abbildung $F$ ist genau dann differenzierbar in $X_0$, wenn alle Koordinatenfunktionen $f_i$ in $X_0$ differenzierbar sind. Ist das der Fall, gilt:
\begin{align*}
 DF_{X_0}(H) = \sum_{i=1}^k (Df_i)_{X_0}(H)E_i' \forall H \in \bR^n 
\end{align*}
\end{theorem}
\begin{proof}
 $L : \bR^n \to \bR^n$ linear. Dann
 \begin{align*}
  \lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - L(H)}{\norm{H}} = 0 \Leftrightarrow \lim_{H \to 0} \frac{f_i(X_0 + H) - f_i(X_0) - (D_i \circ L)(H)}{\norm{H}} = 0
 \end{align*}
 \end{proof}
 Wir wollen nun das Differential bezüglich der Standardbasis übersichtlich darstellen. Es gilt:
 \begin{align*}
  L(E_j) = \sum_{i=1}^k a_{ij} E_i'\\
  DF_{X_0} = \sum_{i=1}^k \partial_j f_i(X_0) E_j'
 \end{align*}
 Die Koeffizienten der Darstellenden Matrix sind also identisch mit den Partiellen Ableitungen.
\begin{theorem}
 Sei $F : M \to \bR^k$ differenzierbar in $X_0 \in M$. Dann wir das Differential $DF_{X_0}$ bezüglich der Standardbasis in $\bR^n$ und $\bR^k$ beschrieben als die $k \times n$-Matrix
 \begin{align*}
  JF(X_0) = (\delta_j f_i(X_0))_{1 \leq i \leq n, 1 \leq j \leq k}
 \end{align*}
 Sie heißt die Funktionalmatrix oder Jacobimatrix von $F$ in $X_0$. Falls $k = n$ wird die Determinante dieser Matrix als Funktionaldeterminante oder Jacobideterminante von $F$ in $X_0$ bezeichnet.
\end{theorem}
\newpar
[missing stuff]
\newpar
\begin{theorem}
 Ist $r \geq 2$ und $f \in C^r(M)$, so sind die partiellen Ableitungen von $f$ bis zur Ordnung $r$ unabhängig von der Reihenfolgen es gilt also:
 \begin{align*}
  \partial_1 \hdots \partial_r f = \partial_{\sigma(1)} \hdots \delta_\sigma(r) f
 \end{align*}
\end{theorem}
\begin{theorem}
 \emph{\tbf{Taylor-Formel}}: Sei $g: [-\epsilon, h] \to \bR$ $\epsilon, h > 0$.
 Sei $g$ $(k+1)$- mal differenzierbar. Dann gilt:
 \begin{align*}
  \exists c \in (0,h) : g(h) = \sum_{j = 0}^k \frac{1}{j!} g^{(j)} (0) h^j + \frac{1}{(k+1)!} g^{(k+1)}(c) h^{k+1}
 \end{align*}
 Sei $f : M \to \bR$ $M \subseteq \bR^n$ offen, $x_0 \in M$. Sei $k \in \bN$, $f \in C^k(M)$ mit partielle differenzierbaren partiellen Ableitungen $k$-ter Ordnung, $H \in \bR^n : [x_0, x_0+H] \subseteq M$. Sei $g(t) := f(x_0 + tH)$. Dann gilt für $r \in \{1, \hdots, k+1\}$:
 \begin{align*}
  g^{(r)}(t) = \sum_{i_1, \hdots i_r}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + tH) h_{i_1} \hdots h_{i_r}
 \end{align*}
 \begin{align*}
  g(1) = \sum_{r=0}^k \frac{1}{r!} g^{(r)}(0) + \frac{1}{(k+1)!} g^{(k+1)}(c)
 \end{align*}
\end{theorem}
\begin{theorem}
 \emph{\tbf{Mehrdimensionale Taylorformel:}} Sei $M \subseteq \bR^n$ offen, $X_0 \in M$, $H \in \bR^n$ mit $[X_0, X_0 + h] \subseteq M$, $k \in \bN$, $f \in C^k(M)$, sodass die partiellen Ableitungen der Ordnung $k$ in $M$ differenzierbar sind. Dann $\exists c \in (0,1)$, sodass:
 \begin{align*}
  f(X_0 + h) = f(X_0) + \sum_{r=1}^k \frac{1}{r!} \sum_{i_1, \hdots i_r = 1}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + tH) h_{i_1} \hdots h_{i_r} + \frac{1}{(k+1)!}  \sum_{i_1, \hdots i_r = 1}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + cH) h_{i_1} \hdots h_{i_r}
 \end{align*}
 Kompakter für $k = 2$:
 \begin{align*}
  f(X_0 + H) = f(X_0) + \scalar{\grad f(X_0)}{H} + \frac{1}{2} \sum_{i,j=1}^n \partial_i \partial_j f(X_0) h_i h_h + R(X_0, h)
 \end{align*}
 \begin{align*}
  R(X_0, H) = \frac{1}{6}\sum_{i,j,k=1}^n \delta_i \delta_j \delta_k f(Y) h_i h_j h_k \quad Y \in [X_0, X_0 + H]
 \end{align*}
 Falls die dritten Ableitungen auf der Verbindungslinie beschränkt sind gilt:
 \begin{align*}
  \lim_{H \to 0} \frac{R(X_0; H)}{\norm{H}^2} = 0
 \end{align*}
\end{theorem}
\begin{theorem}
 Sei $f : M \to \bR$ zweimal partiell differenzierbar in $X_0$. Dann heißt die durch
 \begin{align*}
  Q(f,X_0;H) := \sum_{i,j=1}^n \delta_i \delta_j f(X_0) h_i h_j
 \end{align*}
 definierte Funktion $Q(f,X_0;H$ die \tbf{Hesse-Form} von $f$ im Punkt $X_0$ und die dadurch definierte Matrix
 \begin{align*}
  \Hess{f, X_0}_{ij} = (\partial_i \partial_j f(X_0))
 \end{align*}
 heißt die \tbf{Hesse-Matrix} von $f$ in $X_0$.
\end{theorem}
\clearpage
[...]
\clearpage
\begin{lemma}
 Sei $M \subseteq \bR^n$ offen, $F : M \to \bR^n$ eine $C^1$-Abbildung, sei $L : \bR^n \to \bR^n$ eine lineare Abbildung, sei $X, Y \in M$ mit $[X, Y] \subseteq M$. Dann gilt:
 \begin{align*}
  \norm{F(X) - F(Y) - L(X-Y)} \leq \norm{X - Y} \cdot \max_{Z \in [X,Y]} \norm{DF_Z - L}
 \end{align*}
\end{lemma}
\begin{proof}
 \begin{align*}
  G(X) := F(X) - L(X) \quad X \in M
 \end{align*}
 \begin{align*}
  DG_Z = DF_Z - L
 \end{align*}
 Dann muss für $F \in C^1$ folgende Funktion stetig sein:
 \begin{align*}
  Z \to \norm{DF_Z - L}
 \end{align*}
 Zusätzlich ist $[X,Y]$ kompakt, also existiert das Maximum
 \begin{align*}
  \max_{Z \in [X,Y]} \norm{DF_Z - L} := c
 \end{align*}
 Gemäß Mittelwertsatz ist nun
 \begin{align*}
  \norm{F(X) - F(Y) - L(X-Y)} \leq c \norm{X - Y}
 \end{align*}
\end{proof}
\begin{theorem}
 Sei $M \subseteq R^n$ offen. Sei $\vx_0 \in M$. Sei $F : M \to \bR^n$ eine $C^r$-Abbildung ($r \in \bN_1$). Sei das Differential $DF_{\vx_0}$ regulär, also $\det JF(\vx_0) \neq 0$. Dann existiert eine offene Umgebung $U \subseteq M$ von $ \vx_0$, sodass folgendes gilt:
 \begin{enumerate}
  \item die Einschränkung $F|_U$ ist injektiv
  \item die Bildmenge $F(U) := V$ ist offen
  \item die Umkehrabbildung $(F|_U)^{-1} : V \to U$ ist $C^r$. 
 \end{enumerate}
\end{theorem}
\begin{proof}
 Sei $I$ die Identitätsabbildung des $\bR^n$. Sei $U(0, \alpha) := \{\vx \in \bR^n \mid \vx < \alpha\}$.
 \newpar
 \ul{Annahmen}: $\vx_0 = 0$, $F(0) = 0$ (Erfüllbar durch Verschieben), $DF_0 = I$ (Erfüllbar durch invertierbare Lineare Abbildung der Funktion?)
 \newpar
 $\vx \to \norm{DF_{\vx} - I}$ ist stetig mit $\norm{DF_0 - I} = 0$. Also gilt
 \begin{align*}
  \forall \epsilon > 0 : \exists \alpha > 0 : \forall \vx \in U_\alpha : \norm{DF_{\vx} - I} \leq \epsilon
 \end{align*}
 Nach 4.3 folgt:
 \begin{align*}
  \forall \vx, \vy \in U_\alpha : \norm{F(\vx) - F(\vy) - (\vx-\vy)} \leq \epsilon \norm{\vx - \vy}
 \end{align*}
 \begin{align*}
  \norm{\vx - \vy} &\leq \norm{\vx - \vy - (F(\vx) - F(\vy))} + \norm{F(\vx) - F(\vy)}\\
  &\leq \epsilon \norm{\vx - \vy} + \norm{F(\vx) - F(\vy)}\\
 \end{align*}
 also:
 \begin{align*}
  (1- \epsilon)\norm{\vx - \vy} \leq \norm{F(\vx) - F(\vy)}
 \end{align*}
 Also ist $\norm{F(\vx) - F(\vy)} = 0$ gdw. $\vx = \vy$, also folgt Injektivität.
 \newpar
 \begin{lemma}
  $U_{(1-\epsilon)\alpha} \subseteq F(U_\alpha)$
 \end{lemma}
 \begin{proof}
  Sei $\vy \in U_{(1-\epsilon)\alpha}$. Wir suchen $\vx \in U_\alpha : \vy = F(\vx)$. Wir wollen den Banchschen Fixpunktsatz anwenden. Dafür definieren wir $\phi : \overline{U_\alpha} \to \bR^n$ als:
  \begin{align*}
   \phi(\vx) := \vy - F(\vx) + \vx
  \end{align*}
  Sei nun $X \in \overline{U_\alpha}$. Dann gilt:
  \begin{align*}
   \norm{\phi(\vx)} &\leq \norm{\vy} + \norm{F(\vx) - \vx}\\
                    &\leq \norm{\vy} + \epsilon \norm{\vx}\\
                    &< (1-\epsilon)\alpha + \epsilon \alpha\\
                    &= \alpha\\
  \end{align*}
  Sei $X,Z \in \overline{U_\alpha}$. Nun gilt:
  \begin{align*}
   \norm{\phi(\vx) - \phi(\zz)} &= \norm{F(\vx) - \vx - (F(\zz) - \zz)}\\
                                &\leq \epsilon \norm{\vx - \zz}
  \end{align*}
  Gemäß Banachschem Fixpunktsatz existert also genau ein $X \in \overline{U_\alpha}$, sodass $\phi(\vx) = \vx$, also $F(\vx) = \vy$. Da $\phi(\vx) < \alpha$ gilt auch $\vx \in U_\alpha$.
 \end{proof}
 Sei nun $V : U_{(1 - \epsilon) \alpha}$ und $U := F^{-1}(V)$. Gemäß Lemma ist $U$ eine Obermenge von $V$, also ist $U$ eine offene Umgebung von $0$. Wir wissen bereits, dass $F|_U$ injektiv ist. Sei also nun $G : V \to U$ die Umkehrabbildung von $F|_U$.
 \begin{lemma}
  $G$ ist in $0$ differenzierbar.
 \end{lemma}
 \begin{proof}
 Sei $\epsilon' \in \bR^+$. So existiert ein $\alpha' \in \bR^+$, sodass $U_{\alpha'} \in M$ und
 \begin{align*}
  \norm{F(\vx) - \vx} \leq \frac{\epsilon'}{1 + \epsilon'} \norm{\vx} \quad \forall \vx \in U_{\alpha'}
 \end{align*}
 \begin{align*}
  \norm{\vx} &\leq \norm{\vx - F(\vx)} + \norm{F(\vx)}\\
             &\leq \frac{\epsilon'}{1 + \epsilon'} \norm{\vx} + \norm{F(\vx)}
 \end{align*}
 also:
 \begin{align*}
  \norm{\vx} \leq (1 + \epsilon') \norm{F(\vx)} \quad \forall \vx \in U_{\alpha'}
 \end{align*}
 Sei nun $\vh \in V$ mit $\norm{\vh} < \alpha'(1 - \epsilon)$. Sei $\vx := G(\vh)$. Gemäß Lemma ist $V \subseteq F(U_\alpha)$, also $G(V) \subseteq G(F(U_\alpha))$, also $U \subseteq U_\alpha$, also $\vx \in U$ (?)
 \newpar
 Gemäß vorheriger Überlegungen haben wir
 \begin{align*}
  \norm{X} \leq \frac{1}{1 - \epsilon} \norm{F(\vx)} = \frac{1}{1 - \epsilon} \norm{\vh} < \alpha' 
 \end{align*}
 Wir betrachten nun endlich den Differentialquotienten:
 \begin{align*}
  \norm{G(\vh) - \vh} &= \norm{\vx - F(\vx)}\\
                      &\overset{(*)}{\leq} \frac{\epsilon*}{1 + \epsilon'}\norm{\vx}\\
                      &\leq \epsilon'\norm{F(\vx)}\\
                      &\leq \epsilon' \norm{\vh}
 \end{align*}
 also:
 \begin{align*}
  \frac{\norm{G(\vh) - \vh}}{\norm{\vh}} \leq \epsilon'
 \end{align*}
 für alle $0 < \norm{\vh} < \min \{\alpha(1 - \epsilon), \alpha' (1 - \epsilon)\}$, also ist $G$ in $0$ differenzierbar mit $DG_0 = I$.
 \end{proof}
 Was ist nun, wenn die Vorraussetzungen $\vx = 0$, $F(0) = 0$, $DF_0 = I$ nicht gelten?
 \newpar
 Wir definieren lineare Translationsabbildungen $T_{\zz} : \bR^n \to \bR^n$ $\vx \to \vx + \zz$. Sei nun:
 \begin{itemize}
  \item $L : DF_{\vx_0}$,
  \item $M' := (L \circ T_{-\vx_0})(M)$,
  \item $F'(\vx) := T_{-F(\vx_0} \circ F \circ T_{\vx_0} \circ L^{-1}(\vx)$
 \end{itemize}
 Die Differentiale sind $DL = L$ und $DT_Z = I$. Nun gilt:
 \begin{align*}
  DF'_{0} = I \circ DF_{\vx_0} \circ I \circ (DF_{\vx_0})^{-1} = I
 \end{align*}
 Also $F'(0) = 0$, $0 \in M'$. $F'$ ist also umkehrbar und die Umkehrabbildung ist differenzierbar in $0$. Für die Ursprüungliche Abbildung gilt $F = T_{T_{\vx_0}} \circ F' \circ L \circ T_{-\vx_0}$.
\end{proof}
\begin{definition}
 Sei $F : M \subseteq \bR^n \to \bR^n$ eine $C^r$-Funktion mit regulären Differentialen. Eine solche Abbildung nennt man einen $C^r$-Diffeomorphismus.
\end{definition}
\clearpage
[...]
\clearpage
\begin{theorem}
 \emph{\tbf{(Implizite Funktion):}}  Sei $k < n$, $M \subseteq \bR^n$ offen, $F \in C^r : M \to \bR^k$, sei $N = \{\vx \in M \mid F(\vx) = 0\}$. Sei $\vx_0 \in N$ und $DF_{\vx_0}$ vom Rang $k$. 
 
 \noindent Dann gibt es nach passender Identifizierung von $\bR^n$ mit $\bR^{n-k} \times \bR^k$ eine offene Umgebung $U \subseteq M$ von $\vx_0$, eine offene Menge $V \subseteq \bR^{n-k}$ und eine Abbildung $G \in C^r : V \to \bR^k$, sodass $N \cap U$ der Graph von $G$ ist.
\end{theorem}
\begin{proof}
 $DF_{\vx_0}$ hat Rang $k$. Es gilt also $k$ linear unabhängige Spalten. OBDA seien dies die letzten $k$ Spalten. Die ersten $(n-k)$ Basisvektoren bilden eine Basis des $\bR^{n-k}$, ebenso bilden die letzten $k$ Vektoren eine Basis des $\bR^k$. Wir haben somit eine Identifikation $\bR^n \simeq \bR^{n-k} \times \bR^k$ erhalten, sodass wir $\vx \in \bR^n$ abbilden auf $\vx = (\vx',\vx'')$. Wir definieren folgende Funktion:
 \begin{align*}
  \phi : M \times \bR^k &\to \bR^{n-k} \times \bR^k\\
  (\vx', \vx'') &\to (\vx', F(\vx', \vx''))
 \end{align*}
 Da $F, \times \in C^r$ ist $\phi$ ebenfalls in $C^r$. Für die Jakobimatrix gilt:
 \begin{align*}
  J \phi = 
  \begin{pmatrix}
    1 & \hdots & 0 & 0 & \hdots & 0\\
    \vdots & \ddots & \vdots\\
  \end{pmatrix}
 \end{align*}
 
 [WIP]
 
 Nach dem Satz der Inversen Funktion existiert eine Umgebung $U_0$ von $\vx_0$, sodass auf dieser Umgebung eine Umkehrabbildung $\psi$ existiert. Und so weiter :)
\end{proof}
\begin{anmerkung}
 Seien $A$, $B$ Mengen. Es existieren folgende Funktionen: 
 \begin{itemize}
  \item Das Produkt $A \times B$
  \item Die Projektion $\pi_1 : A \times B \to A$ auf die erste Komponente
  \item Die Projektion $\pi_2 : A \times B \to B$ auf die zweite Komponente
  \item Die kanonische Injektion $i : A \to A \times B : a \to (a,0)$
 \end{itemize}
\end{anmerkung}
\begin{theorem}
 \emph{\tbf{Über lokal surjektive Abbildungen:}} Sei $k < n$. Sei $M \subseteq \bR^n$ offen, $F : M \to \bR^k$ eine Abbildung der Klasse $C^r$, $r \in \bN_1$. Sei $X_0 \in M$ und $F$ in $X_0$ vom Rang $k$, also $DF_{X_0}$ surjektiv. Dann gibt es eine offene Umgebung $U$ von $X_0$ in $M$, eine offene Menge $V$ in $\bR^{n-k}$, und einen $C^r$-Diffeomorphismus $h : U \to V \times F(u)$, sodass das folgende Diagramm kommutiert:
 \begin{figure}[h!]
 \centering
    \begin{tikzcd}[row sep = huge, column sep = huge]
    U
    \arrow[r, "h"]
    \arrow[dr, "F"]
    & V \times F(u)
    \arrow[d, "\pi_2"] \\
    &  F(u)
    \end{tikzcd}
\end{figure}
\end{theorem}
\begin{proof}
Nach Vorraussetzung hat $JF(X_0)$ $k$ unabhängige Spalten. Seien dies OBdA die letzten Spalten. Wir interpretieren $\bR^n = \bR^k \times \bR^{n-k}$ und definieren $\pi_1 : \bR^n \to \bR^{n-k}$ und $\pi_2 : \bR^n \to \bR^k$ als die dazugehörigen kanonischen Projektionen. Sei $\varphi$ folgende Funktion:
\begin{align*}
 \varphi : M &\to \bR^{n-k} \times \bR^k\\
 X &\to (\pi_1(X), F(X))
\end{align*}
Es gilt $F \in C^r$ und $\pi_1 \in C^r$, also auch $\varphi \in C^r$. Gemäß des Satzes der inversen Funktion existiert also eine Umgebung $\varphi \in C^r$, auf der $\varphi$ ein $C^r$ Diffeomorphismus (also $C^r$ und invertierbar).
\newpar
Da $\varphi^{-1}$ stetig ist, ist das Urbild $(\varphi^{-1})^{-1}(U') = \varphi(U')$ offen. Es enthält also eine offene Umgebung von $\varphi(X_0) = (\pi_1(X_0), F(X_0))$ der Form $V \times X$, also ist $V$ offen in $\bR^{n-k}$. Wir setzen $U := \varphi^{-1}(V \times W)$ und $h = \varphi \mid_U$. Dann ist $F(U) = W$, und für $X \in U$ gilt $h(X) = (\pi_1(X), F(X))$, also $\pi_2 \circ h = F$.
\end{proof}
\noindent Im Fall $k > n$ erhalten wir lokale Injektivität statt lokaler Surjektivität:
\begin{theorem}
 \emph{\tbf{Über lokal injektive Abbildungen:}} Sei $k > n$. Sei $M \subseteq \bR^n$ offen, sei $F : M \to \bR^k$ eine $C^r$-Abbildung. Sei $X_0 \in M$ und $F$ in $X_0$ vom Rang $n$ (und damit das Differential $DF_{X_0}$ injektiv). Sei $i$ die kanonische Injektion.
 \newpar
 Dann gibt es eine offene Umgebung $U$ von $X_0$ in $M$, eine offene Umgebung $V$ von $0$ in $\bR^{k-n}$, eine offene Umgebung $W$ von $F(X_0)$ in $\bR^k$, und einen $\bC^r$-Diffeomorphismus $h : U \times V \to W$, sodass das folgende Diagramm kommutiert:
 \FloatBarrier
 \begin{figure}[h!]
 \centering
    \begin{tikzcd}[row sep = huge, column sep = huge]
    U
    \arrow[r, "i"]
    \arrow[dr, "F"]
    & V \times F(u)
    \arrow[d, "h"] \\
    &  F(u)
    \end{tikzcd}
\end{figure}
\end{theorem}
\FloatBarrier
\begin{proof}
 hi :D
\end{proof}
\clearpage
[...]
\clearpage
%
%
%
%
%
%
%
%
%
%
\chapter{Gewöhnliche Differentialgleichungen}
Gewöhnliche Differentialgleichungen, auf englisch \textit{ordinary differential equations} (\textit{ODEs}) beschreiben zeitabhngige Prozesse. Sie sind nützlich für die modellierung zahlreicher Prozesse in verschiedenen Gebieten der Wissenschaft.
\section{Existenztheorie}
\begin{definition}
	Für $f : \bR^{n+1} \to \bR$ nennen wir eine Gleichung der Form
	\begin{align*}
		y^{(n)}(t) = f(t, y(t), y'(t), \hdots, y^{(n-1)}(t))
	\end{align*}
eine \textit{explizite Differntialgleichung n-ter Ordnung}. Ist zusätzlich
\[y^{(i)}(t_0) = y_{i-1}\]
für $(t_0, y_0, y_1, \hdots y_{n-1}) \in \bR^{n+1}$ gegeben, spricht man von einem \textit{Anfangswertproblem.}
\end{definition}
\begin{definition}
	Sei $I$ ein Intervall. Eine Funktion $y : I \to \bR$ heißt \textit{Lösung} von (2.1) im Intervall $I$, falls $y$ in $I$ $n$-mal differenzierbar ist und
	\[y^{(n)}(t) = f(t, y(t), y'(t), \hdots, y^{(n-1)}(t))\]
	für alle $t \in I$ erfüllt ist.
\end{definition}
Wir erlauben hier jede Art von Intervall, egal ob offen, halboffen, oder geschlossen.
\begin{definition}
	Sei $F : \omega \subseteq \bR^{n+1} \to \bR^n$. Wir nennen das Gleichungssystem
	\[Y'(t) = F(t, Y(t))\]
	ein \textit{System von Differentialgleichungen 1. Ordnung}. Für $F = (f_1, \hdots, f_n)$ und $Y = (y_1, \hdots, y_n)$ lässt sich das System komponentenweise schreiben als:
	\begin{align*}
		y_1'(t) &= f_1(t, y_1(t), \hdots, y_n(t))\\
		&\hspace{6pt}\vdots\\
		y_n'(t) &= f_n(t, y_1(t), \hdots, y_n(t))\\
	\end{align*}
	Ergänzt man das System durch die Bedingung $Y(t_0) = Y_0$ erhalten wir komponentenweise:
	\begin{align*}
		y_1(t_0) &= y_1^0\\
		&\hspace{6pt}\vdots\\
		y_n(t_0) &= y_n^0\\
	\end{align*}
	Für ein $(t_0, y_1^0, \hdots, y_n^0) \in \Omega$ und sprechen wieder von einem \textit{Anfangswertproblem}.
\end{definition}
\begin{lemma}
 Erfülle $F$ die Vorraussetzung $(S)$, sei außerdem $y_i (a,b) \to \bR^n$ eine Lösung des Gleichungssystems $Y'(t) = F(t,Y(t))$. Weiter existiere der Grenzwert $\lim_{t \to^+ b} Y(t) := y_1$ und es gelte $(b,y_1) \in \Omega$. Dann existiert $\delta > 0$, sodass man die Lösung $y$ zu einer Lösung auf dem Intervall $(a, b+\delta]$ fortsetzen kann.
\end{lemma}
\begin{definition}
 Seien $A,B \in \bR^{n+1}$. Wir definieren zwischen den beiden Mengen folgendermaßen eine Abstandsfunktion: \begin{align*}
  \text{dist}(A,B) := \inf_{x \in A, y \in B} \norm{X - Y}                                                                                                   
 \end{align*}
\clearpage
[big gap here oops]
\clearpage
\begin{lemma}
	Seien $I,J$ offene Intervalle, sei $f : J \times I \to \bR$ durch $f(t,y) = h(t)g(y)$ gegeben, wobei $g : I \to \bR$, $g \neq 0$, $h : J \to \bR$ stetig sind. Falls $\varphi : (\alpha, \beta) \subseteq J \to I$ eine Lösung von (3.1) ist, existiert $c \in \bR : \forall t \in (\alpha, \beta)$
	\[\varphi(t) = G^{-1}(H(t) + c)\]
\end{lemma}
\begin{theorem}
	Erfülle $f$ die Vorraussetzung des letzten Lemmas. Dann existiert $\forall (t_0, y_0) \in J \times I$ eine eindeutige maximale Lösung $y : J_0 \to I$ von (3.1) mit $y(t_0) = y_0$. Diese Lösung ist von der Form
	\[y(t) = G^{-1}(H(t)),\]
	wobei \[G(y) = \int_{y_0}^y \frac{1}{g(x)} dx, y \in I,\] \[H(t) = \int_{t_0}^{t}h(s) ds, t \in J\] ist. 
\end{theorem}
\end{definition}
\begin{beispiel}
	Sei $y'(t) = 2t(1 + y^2)$ mit Anfangsbedingung $y(t_0) = y_0$, wobei $(t_0, y_0 \in \bR^2)$.
	Dann gilt:
	\[
		g(y) = 1 + y^2, \quad h(t) = 2t
	\]
	\begin{align*}
		G(y) &= \int_{y_0}^y \frac{1}{1+x^2} dx \\
			 &= \arctan y - \arctan y_0\\
			 &:= \arctan y - c_0
	\end{align*}
	\begin{align*}
		H(t) &= \int_{t_0}^t 2s\ ds\\
			 &= t^2 - t_0^2
	\end{align*}
	\begin{align*}
		\text{Bild}(G) = \left(-\frac{\pi}{2} - c_0, \frac{\pi}{2} - c_0\right), \quad c_0 \in \left(-\frac{\pi}{2},\frac{\pi}{2}\right)
	\end{align*}
	\begin{enumerate}
		\item $-t_0^2 > -\frac{\pi}{2} - c_0$
		\[\alpha := \sqrt{\frac{\pi}{2} - c_0 + t_0^2}\]
		\[H^{-1}(\text{Bild}(G)) = (-\alpha, \alpha)\]
		\[y(t) = \tan(t^2 - t_0^2 + \arctan(y_0))\]
		\item $-t_0^2 \leq -\frac{\pi}{2} - c_0$
		\[\alpha = \sqrt{-\frac{\pi}{2} - c_0 + t_0^2}\]
		\[\beta := \sqrt{\frac{\pi}{2} - c_0 + t_0^2}\]
		\[H^{-1}(\text{Bild}(G)) = (-\beta, -\alpha) \cup (\alpha, \beta)\]
		OBdA $t_0 \in (\alpha, \beta)$, dann
		\[y(t) = \tan(t^2 - t_0^2 - \arctan(y_0))\]
	\end{enumerate}
\end{beispiel}
\clearpage
\section{Lineare Gleichungen}
Angenommen, wir haben eine Funktion $f(t, y)$ der Form
\[f(t,y) = h(t)y + p(t)\]
Es gilt:
\[h, p : I = (a,b) \to \bR\]
Wobei nach Annahme $h, p$ stetig sind und $f : I \times \bR \to \bR$ die Vorraussetzung (S) erfüllt.
\newpar
Wenn $p(t) = 0$ nennen wir die Differentialgleichung $y' = f(t,y)$ \textbf{homogen}, ansonsten nennen wir sie \textbf{inhomogen}.
\newpar
\[y' = h(t)y + p(t) \Leftrightarrow y'(t) - h(t)y(t) = p(t)\]
Wir suchen nun das Urbild von $p \in C^0(I)$ bezüglich
\[L : C^1(I) \to C^0(U) : y \mapsto y' - hy\]
\[(L(y))(t) := y'(t) - h(t)y(t)\]
Es gilt:
\[L(\alpha y + \beta z) = \alpha L(y) + \beta(L(z))\]
Also ist $L$ ein linearer Operator!
\newpar
[...]
\clearpage
\begin{lemma}
	Sei $I = (a,b)$ und für $f : I \times \bR \to \bR$ gelte
	\[f(t,y) = h(t)y + p(t)\]
	Wobei $p, h$ auf $I$ stetig sind. Seien Lösungen $y_1, y_2$ der inhomogenen Gleichung und $y_0$ Lösung der homogenen Gleichung. Dann gilt:
	\begin{enumerate}
		\item[(i)] $y_1 - y_2$ ist Lösung der homogenen Gleichung
		\item[(ii)] $y_1 + y_0$ ist Lösung der inhomogenen Gleichung
	\end{enumerate}
\end{lemma}
\begin{lemma}
	Sei $y_2$ eine Lösung der inhomogenen Gleichung und $y_0$ eine Lösung der homogenen Gleichung. So existiert eine Lösung $y_1$ der inhomogenen Gleichung, sodass
	\[y_2 = y_0 + y_1\]
\end{lemma}
	\textbf{Variation der Konstanten:} 
	\[y(t) = c(t) e^{H(t)}\]
	\[y'(t) = c'(t) e^{H(t)} + c(t)e^{H(t)}h(t)\]
	Damit $y$ eine Lösung von (3.1) ist muss gelten:
	\[y'(t) = p(t) + h(t)y(t)\]
	Also:
	\[c'(t) = p(t)e^{-H(t)}\]
	Also ist $c$ eine Stammfunktion von $pe^{-H}$.
\begin{theorem}
	Sei $I = (a,b)$, für $f : I \times \bR \to \bR$ gelte
	\[f(t,y) = h(t)y + p(t)\]
	$p,h$ stetig. Dann existert für alle $(t_0, y_0) \in I \times \bR$ eine eindeutige, maximale Lösung
	\[y : I \to \bR\]
	von (3.1) mit $y(t_0) = y_0$. Diese Lösung hat folgende Form:
	\[y(t) = e^{[H(t)]}\left(y_0 + \int_{t_0}^t p(s) e^{-H(s)} ds\right)\]
	Wobei
	\[H(t) = \int_{t_0}^t h(s) ds\]
\end{theorem}
\begin{proof}
	\begin{align*}
		y(t_0) = e^{H(t_0)} (y_0 + 0) = y_0
	\end{align*}
	Rechnungen liefern, dass $y : I \to \bR$ maximal ist. Seien $y_1, y_2$ eine maximale Lösung mit $y_i(t_0) = y_0$. $\overline{y} := y_1 - y_2$ ist eine Lösung der homogenen Gleichung mit $y(t_0) = 0$, also $\overline{y}(t) = 0$, also $y_1 = y_2$.
\end{proof}
%
%
%
%
%
%
%
\clearpage
\chapter{Systeme Linearer Differentalgleichungen}
Wir suchen nach Lösungen von Gleichungssystemen der Form:
\[Y'(t) = \tbf{A}(t) Y(t) + \tbf{B}(t)\]
Wobei $\tbf{A}, \tbf{B}$ Matrizen sind.
\newpar
Sei $I = (a,b)$ und sei $F(t,Y) := \tbA(t) Y + \tbB(t)$. $F$ ist bezüglich $Y$ lokal Lipschitzstetig, also existiert eine eindeutige maximale Lösung mit $Y(t_0) = Y_0$.
\begin{lemma}
	\tbf{Lemma von Gronwall:}\\
	Sei $J$ ein Intervall, $t_0 \in J$, $\alpha, \beta \in [0, \infty)$. Ferner sei $x : J \to [0, \infty)$ stetig und erfülle
	\[x(t) \leq \alpha + \beta \abs{\int_{t_0}^t x(s) ds}\]
	Für $t \in J$. Dann gilt
	\[x(t) \leq \alpha e^{\beta \abs{t - t_0}}\]
\end{lemma}
\begin{proof}
	Sei $t \geq t_0$, $t \in J$. Sei
	\[h(s) := \beta e^{\beta(t_0 - s)}\int_{t_0}^s x(\tau) d\tau\]
	mit $s \in [t_0, t]$. Sei
	\begin{align*}
		h'(s) &= \beta e^{\beta(t_0 - s)} (-1)\beta \int_{t_0}^s x(\tau) d\tau + \beta e^{\beta(t_0 - s)} x(s)\\
			  &= - \beta h(s) + \beta e^{\beta(t_0 - s)} x(s)\\
			  &\leq - \beta h(s) + \beta e^{\beta(t_0 - s)} \left(\alpha + \beta \abs{\int_{t_0}^s x(\tau) d\tau}\right)\\
			  &= - \beta h(s) + \alpha \beta e^{\beta(t_0 - s)} + \beta h(s)\\
			  &= \alpha \beta e^{\beta(t_0 - s)}
	\end{align*}
	\begin{align*}
		\int_{t_0}^t h'(s) ds = h(t) - h(t_0) = h(t) = \beta e^{\beta(t_0 - s)}\int_{t_0}^s x(\tau) d\tau
	\end{align*}
	\begin{align*}
		\int_{t_0}^t \frac{d}{ds}(-\alpha e^{B(t_0 - s)}) ds &= -\alpha e^{\beta(t_0 - t)} + \alpha\\
		\implies \beta e^{\beta(t_0 - s)}\int_{t_0}^t x(\tau) &\leq \alpha -\alpha e^{\beta(t_0 - t)}
	\end{align*}
	\begin{align*}
		\beta \int_{t_0}^t x(s) ds &\leq \alpha e^{\beta(t - t_0)} - \alpha\\
		\implies \alpha + \beta \int_{t_0}^t x(s) ds &\leq \alpha e^{\beta(t - t_0)}\\
		\implies x(t) &\leq \alpha e^{\beta(t - t_0)}
	\end{align*}
\end{proof}
\begin{theorem}
	Seien $\tbA, \tbB$ stetig Funktionen auf $I = (a,b)$. Dann ist jede maximale Lösung des dazugehörigen Differentialsystems auf ganz $(a,b)$ definiert.
\end{theorem}
\begin{proof}
	Sei eine maximale Lösung gegeben durch:
	\begin{align*}
		Y : (\alpha, \beta) \to \bR^n
	\end{align*}
	Wobei $Y(t_0) = Y_0, t_0 \in (\alpha, \beta)$. Da $Y$ eine maximale Lösung ist existiert der Limes
	\[\lim_{t \to^+ \beta} Y(t)\] 
	nicht. Sei $(\alpha, \beta) \subsetneq (a,b)$, OBdA $\beta < b$. Sei für $n \in \bN$:
	\[K_n := \{(t,Y) \in \bR^{n+1} \mid t \in [t_0, \beta], \norm{Y} \leq n\}\]
	Da diese Menge kompakt ist und $Y$ eine maximale Lösung ist, ist
	\[G^+ = \{(t,z) \in \overline{\text{graph}(Y)} \mid t \geq t_0\}\]
	keine kompakte Teilmenge.
	\begin{align*}
		\exists \tau_n \in [t_0, \beta) : \norm{Y(\tau_n)} = n\\
		\implies \lim_{n \to \infty} \norm{Y(\tau_n)} = \infty
	\end{align*}
	\begin{align*}
		n(T) &:= \norm{Y(t)} t \in (\alpha, \beta)\\
		\delta &:= \max_{t \in [t_0, \beta]} \norm{\tbA(t)}_{\bR^{n \times m}} < \infty\\
		\gamma &:= \max_{t \in [t_0, \beta]} \norm{\tbB(t)}_{\bR^{n}} < \infty\\
	\end{align*}
	Da $Y$ eine Lösung ist, ist:
	\begin{align*}
		Y(t) = Y(t_0) + \int_{t_0}^t \tbA(s)Y(s) + \tbB(s) ds t \in [t_0, \beta)
	\end{align*}
\end{proof}
[...]
\clearpage
\begin{definition}
	Wir nennen die Matrix $\tbY$, welche das System linearer Differentialgleichungen beschreibt, die \textbf{Fundamentalmatrix} des Systems.
\end{definition}
\begin{lemma}
	Es gilt $\tbY' = \tbA \tbY$.
\end{lemma}
\begin{definition}
	Wir definieren $\tbZ(t) = \tbY(t) \tbY^{-1}(t_0)$.
\end{definition}
\begin{definition}
	Sei $I = (a,b), A : I \to \bR^{n \times n}$ stetig. Sei $\tbY$ eine Fundamentalmatrix eines homogenen Systems linearer Differentialgleichungen. Wir nennen
	\[W(t) := \det{\tbY(t)}\]
	die \tbf{Wronski-Determinante}.
\end{definition}
\begin{theorem}
	Sei $I = (a,b), \tbA : I \to \bR^{n \times n}$ stetig. Sei $\tbY$ eine Fundamentalmatrix eines homogenen Systems linearer Differentialgleichungen. Dann gilt
	\[W(t) = W(t_0) e^{\int_{t_0}^t \tr(\tbA(s)) ds}\]
	für $t \in I$, wobei die Spur $\tr(\tbA)$ einer quadratischen Matrix als die Summe der Diagonaleinträge definiert ist.
\end{theorem}
\begin{proof}
	Es gilt $\tr(A) \in \bR$, $W(t) \in \bR$. Wir betrachten also eine skalare Gleichung. Gemäß Satz (3.8?) aus Kapitel 12 gilt diese Formel genau dann, wenn
	\[W'(t) = \tr(\tbA(t))\]
	Wir benötigen also eine Formel für die Ableitung der Determinante. Für $\tbB = (b_{ij})$ gilt
	\[\det \tbB = \sum_{\sigma \in S_n} \text{sgn}(\sigma) b_{1\sigma(1)} \cdot \hdots \cdot b_{n\sigma(n)}\]
	Somit gilt:
	\[(\det \tbB)' = \sum_{i=1}^n \sum_{\sigma \in S_n} \text{sgn}(\sigma) b_{1\sigma(1)} \cdot \hdots \cdot b_{i \sigma(i)}' \cdot \hdots \cdot b_{n\sigma(n)}\]
	(insert black magic here)
	\newpar
	Es folgt:
	\[W(t) = \det \tbY(t) = \det \tbZ(t) \det \tbY(t_1) = \det \tbZ(t) W(t_1)\]
	und somit:
	\[W'(t) = (\det \tbZ(t))' W(t_1)\]
	Mit der Formel der Ableitung der Determinante gilt:
	\begin{align*}
		(\det \tbZ(t_1))' &= \sum_{i = 1}^n \det (Z_1(t_1), \hdots, Z_i'(t_1), \hdots, Z_n(t_1))\\
						  &= \sum_{i = 1}^n \det (E_1(t_1), \hdots, \tbA(t_1)E_i, \hdots, E_n(t_1))\\
						  &= \sum_{i = 1}^n a_{ii}(t_1)\\
						  &= \tr \tbA(t_1)
	\end{align*}
	Es gilt folglich:
	\[W'(t) = \tr \tbA(t_1)W(t_1)\]
\end{proof}
\begin{corollary}
	Die Wronski-Determinante einer Fundamentalmatrix ist überall ungleich Null.
\end{corollary}
\section{Systeme mit konstanten A}
Wir betrachten Systeme der Form:
\[Y'(t) = \tbA Y(t)\]
Wir betrachten dabei die lineare Abbildung:
\[A : \bR^n \to \bR^n : Y \mapsto \tbA Y\]
Also $\tbA = M_{E}^{E}(A)$. Aus der Linearen Algebra ist bekannt, dass:
\[M^{B}_B(A) = M_B^E(id) M_E^E(A) M_E^B(id)\]
Wobei $M_E^B(id) := \tbB$ und $M_B^E := \tbB^{-1}$. Also:
\[M_B^B(A) = \tbB^{-1} \tbA \tbB := \tbD\]
\[Z(t) := \tbB^{-1}Y(T) \Leftrightarrow \tbB Z(t) = Y(t)\]
Falls $Y$ eine Lösung des Systems mit Konstante ist, gilt:
\[Z'(t) = \tbB^{-1}Y'(t) = \tbB^{-1}\tbA Y(t) = \tbB^{-1}\tbA\tbB Z(t) = \tbD Z(t)\]
\begin{theorem}
	Wenn $\tbA$ symmetrisch ist, können wir die Matrix diagonalisieren und erhalten eine besonders simple Lösung der Form:
	\[\tbY = \tbB \tbZ = (B_1 e^{\lambda_1 t}, \hdots, B_n e^{\lambda_n t})\]
\end{theorem}
%
%
%
%
%
%
%
%
%
%
\end{document}
