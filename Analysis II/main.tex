\documentclass{report}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{tikz-cd}
\usepackage{physics}
%\usepackage{bbm}
%\usepackage{biblatex}
%\addbibresource{bib.bib}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\Hess}[1]{\text{Hess}(#1)}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\ve}{\vec{e}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{0}}

\newcommand{\Mat}[3]{\text{Mat}^{#1}_{#2}\left(#3\right)}
\newcommand{\scalar}[2]{\left\langle #1, #2 \right\rangle}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Korollar}



\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{beispiel}[lemma]{Beispiel}
\newtheorem{beobachtung}[lemma]{Beobachtung}
\newtheorem{anmerkung}[lemma]{Anmerkung}
\newtheorem{question}[lemma]{Frage}
\newtheorem{application}[lemma]{Anwendung}
\newtheorem{konsequenz}[lemma]{Konsequenz}
%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
%
%
%
%
%
%
%\chapter{Wiederholung}
%
%
%
%
%
%
%
%
%
\chapter{Der Euklidische Raum}
\begin{lemma}
 Sei $(V, \scalar{\_}{\_})$ ein euklidischer Vektorraum. Dann wird durch
 \begin{align*}
  \norm{u} = \sqrt{\scalar{u}{u}}
 \end{align*}
auf $V$ eine Norm erklärt. Diese bezeichnet man als die durch das Skalarprodukt induzierte Norm.
\end{lemma}
\begin{definition}
 Seu $(V, \scalar{\_}{\_})$ ein euklidischer Vektorraum, Die Vektoren $u, v \in V$ heißen \tbf{orthogonal}, wenn \begin{align*}
   \scalar{u}{v} = 0                                                                                                                 
 \end{align*}
 ist. Für $u, v \in V \setminus \{0\}$ Wird die reelle Zahl
 \begin{align*}
  \phi = \arccos \frac{\scalar{u}{v}}{\norm{u}\ \norm{v}}
 \end{align*}
 als der Winkel zwischen $u$ und $v$ bezeichnet.
\end{definition}
\begin{anmerkung}
 Es gilt
 \begin{align*}
  \frac{\abs{\scalar{u}{v}}}{\norm{u}\ \norm{v}} \leq 1
 \end{align*}
\end{anmerkung}
\begin{lemma}
\label{lemma:normequiv}
 Für $X = (x_1, \hdots, x_n) \in \bR^n$ sei
 \begin{align*}
  \norm{X}_{\max} := \max\{\abs{x_1}, \hdots, \abs{x_n}\}
 \end{align*}
 Dann ist $||\_||_{\max}$ eine Norm auf $\bR^n$ und es gilt
 \begin{align*}
  \norm{X}_{\max} \leq \norm{X} \leq \sqrt{n}\norm{X}_{\max}
 \end{align*}
\end{lemma}
\begin{theorem}
Die Menge $\bQ^n$ der Punkte mit rational Koordinaten ist dicht in $\bR^n$.
\end{theorem}
\begin{proof}
Sei $X \in \bR^n$ und $\epsilon \in \bR^+$. Da $\bQ$ dicht in $\bR$ ist gilt
\begin{align*}
 \forall i \in \{1, \hdots, n\} : \exists y_i \in \bQ : \abs{x_i - y_i} \leq \frac{\epsilon}{\sqrt{n}}
\end{align*}
Durch Lemma \ref{lemma:normequiv} folgt:
\begin{align*}
 \norm{x-y} \leq \sqrt{n}\norm{X - Y} < \epsilon
\end{align*}
\end{proof}
\begin{theorem}
\label{theorem:componentcauchy}
 Sei $(X_k)_{k \in \bN}$ eine Folge aus $\bR^n$. Sei $X_k = (x_1^{(k)}, \hdots, x_n^{(k)})$. Dann gilt:
 \begin{align*}
  \lim_{k \to \infty} X_k = X \Leftrightarrow \forall i : \lim_{k \to \infty} x_i^{(k)} = x_i
 \end{align*}
 Insbesondere ist $X_k$ eine Cauchyfolge, wenn die Komponenten Cauchyfolgen sind.
\end{theorem}
\begin{proof}
 $X_k \to X$, $i \in \{1, \hdots, n\}$, $\epsilon \in \bR^+$. Dann gilt
 \begin{align*}
  \exists k_o \in \bN : \forall k \geq k_0 : \norm{X_k - X} \leq \epsilon \implies \forall i : \abs{x_i^{(k)} - x_i} < \epsilon \implies \lim_{k \to \infty} x_i^{(k)} = x_i
 \end{align*}
 Und umgekehrt:
 \begin{align*}
  \forall i : x_i^{(k)} \to x_i, \epsilon \in \bR^+ \implies \exists k_0^i \in \bN : \forall k \geq k_0^i \abs{x_i^{(k)} - x_i} \leq \frac{\epsilon}{\sqrt{n}}
 \end{align*}
 \[
  k_0 := \max\{k_0^n, \hdots, k_0^n\} \implies \forall k \geq k_0 : \abs{x_i^{(k)} - x_i} < \frac{\epsilon}{\sqrt{n}} \implies \norm{X_k - X} \leq \sqrt{n}\norm{X_k - X} < \epsilon
 \]
\end{proof}
\begin{theorem}
 Für konvergente Folgen $(X_k),(Y_k) \in \bR^n$, $(\lambda_k) \in \bR$ gilt:
\begin{align}
 \lim_{k \to \infty} (X_k + Y_k) = \lim_{k \to \infty} X_k + \lim_{k \to \infty} Y_k\\
 \lim_{k \to \infty} \lambda_k X_k = \left(\lim_{k \to \infty} \lambda_k\right)\left(\lim_{k \to \infty}
 X_k\right)\\
 \lim_{k \to \infty} \scalar{X_k}{Y_k} = \scalar{\lim_{k \to \infty} X_k}{\lim_{k \to \infty} Y_k}
\end{align}
\end{theorem}
\begin{theorem}
 $\bR^n$ ist vollständig.
\end{theorem}
\begin{proof}
Ist $X_k$ eine Cauchyfolge in $\bR^n$, so sind nach Satz \ref{theorem:componentcauchy} alle Teilfolgen Cauchy in $\bR$. Also:
\begin{align*}
 \exists x_i \in \bR : x_i^{(k)} \to x_i \implies \exists X \in \bR^n : X_k \to X
\end{align*}
\end{proof}
\begin{theorem}
\emph{\textbf{(Bolzano-Weierstrass:)}} Jede beschränkte Folge in $\bR^n$ besitzt eine konvergente Teilfolge.
\end{theorem}
\begin{proof}
 Sei $(X_k)$ eine beschränkte Folge in $\bR^n$. Nach \ref{lemma:normequiv} müssen die Komponentenfolgen ebenfalls beschränkt sein. Nach dem eindimensionalen Fall des Satzes von Bolzano-Weierstrass existieren also konvergente Teilfolgen der Koordinatenfolgen. Angenommen, die konvergente Teilfolge der ersten Komponente ist gegeben durch $x_1^{(k_n)} \to x_1$. So ist $x_2^{(k_n)}$ ebenfalls eine beschränkte Teilfolge, also existiert eine Teilfolge $x_2^{(k_n)_m}$ welche in den ersten beiden Komponenten konvergiert. Führt man dieses Verfahren induktiv fort, erhält man eine konvergente Teilfolge von $(X_k)$.
\end{proof}
\begin{theorem}
 Sei $(A_i)_{i \in \bN}$ eine Folge abgeschlossener beschränkter nichtleerer Teilmengen des $\bR^n$, sodass $A_1 \supseteq A_2 \supseteq \hdots$ Dann ist $\bigcap_{i \in \bN} \neq \emptyset$
\end{theorem}
\begin{proof}
 $A_i \neq \emptyset \implies \exists X_i \in A$ sd. $(X_i)_{i \in \bN}$ eine Folge ist. Da $A_i$ beschränkt ist ist $(X_i)_{i \in \bN}$ beschränkt, also hat $X_i$ eine konvergente Teilfolge $X_{i_k}$ mit Limes $X$. Es gilt $X_{i_k} \in A_{i_k} \subseteq A_i$, also ist $X$ ein Berührpunkt von $A_i$, also $X \in A_i$.
\end{proof}
\begin{theorem}
 Jede abgeschlossene beschränkte Teilmenge des $\bR^n$ ist kompakt.
\end{theorem}
\begin{proof}
 Analog zur eindimensionalen Version, wobei statt Intervallen $[a_i,b_i]$ Hyperwürfel $[a_i^{(1)}, b_i^{(1)}] \times \hdots \times [a_i^{(n)}, b_i^{(n)}]$ genutzt werden müssen.
\end{proof}
\begin{theorem}
\label{theorem:allnormsequiv}
 Seien $\norm{\_}_1$ und $\norm{\_}_2$ Normen auf $\bR^n$. So existieren $k, K \in \bR^+$ mit
 \begin{align*}
  \forall X \in \bR^n : k\norm{X}_1 \leq \norm{X}_2 \leq K\norm{X}_1
 \end{align*}
\end{theorem}
\begin{proof}
Diese Normenäquivalenz bildet eine Äquivalenzrelation. Es reicht also, zu zeigen, dass jede Norm $||\_||_2$ äquivalent zu einer spezifischen Norm $\norm{\_}_1$ ist. Wir wählen $\norm{\_}_{\max}$.\\
Sei $(E_i)$ die Standardbasis des $\bR^n$. Wir definieren:
\begin{align*}
 K := \norm{E_1}_2 + \hdots + \norm{E_n}_2
\end{align*}
Dann gilt:
\begin{align*}
 \norm{X}_2 &= \norm{x_1 E_1 + \hdots + x_n E_n}\\
         &\leq \abs{x_1}\norm{E_1}_2 + \hdots + \abs{x_n} \norm{E_n}_2\\
         &\leq \norm{X}_{\max} K \quad ^\text{[citation needed]}
\end{align*}
Es bleibt die Rückrichtung zu zeigen. 
\begin{lemma}
 $f(X) := \norm{X}_2$ ist stetig.
\end{lemma}
\begin{proof}
\begin{align*}
 \abs{\norm{X}_2 - \norm{Y}_2} \leq \norm{X - Y}_2 \leq K\norm{X - Y}_{\max} \leq K \norm{X - Y}
\end{align*}
Also ist $\norm{\_}_2$ stetig bezüglich der euklidischen Norm $\norm{\_}$.
\end{proof}
Wir definieren nun:
\begin{align*}
 A := \{X \in \bR^n \mid ||X||_{\max} = 1\}
\end{align*}
Diese Menge ist beschränkt. Wir wollen Zeigen, dass sie außerdem abgeschlossen ist. Sei $X_i \to X$, $X_i \in A$. Es gilt:
\begin{align*}
  \abs{\norm{X_i}_{\max} - \norm{X}_{\max}} \leq \norm{X_i - X}_{\max} \leq \norm{X_i - X}
\end{align*}
Also konvergiert jede Menge, also ist $A$ kompakt, also auch abgeschlossen. Dementsprechend muss $f$ auf $A$ ein Minimum $k$ annehmen. Wir wissen $f \geq 0$, also ist$k \geq 0$. Es gilt sogar $k > 0$, da keiner der Vektoren in $A$ der Nullvektor ist. Nun gilt also $\forall X \in A : ||X||_2 \geq k$. Wir definieren:
\begin{align*}
 \lambda := \frac{1}{\norm{X}_{\max}}
\end{align*}
\begin{align*}
 \norm{\lambda X}_{\max} = \abs{\lambda} \norm{X}_{\max} = 1 
\end{align*}
\begin{align*}
 |\lambda| \norm{X}_2 = \norm{\lambda X}_2 \geq k \implies \norm{X_2} \geq k \norm{X}_{\max}
\end{align*}
\end{proof}
\begin{anmerkung}
 Im unendlichdimensionalen Fall gilt Satz \ref{theorem:allnormsequiv} nicht.
\end{anmerkung}
%
%
%
%
%
%
%
%

%
%
%
%
\section{Abbildungen und Koordinatenfunktionen auf $\bR^n$}
In diesem Abschnitt betrachten wir Funktionen $F: \bR^n \to \bR^k$. Betrachten wir zuerst den Spezialfall Linearer Funktionen, also $\forall X,Y \in \bR^n : \forall \lambda, \mu \in \bR : F(\lambda X + \mu Y) = \lambda F(X) + \mu F(Y)$.
\newpar
Sei $(E_i)$ die Standardbasis des $\bR^n$ und sei $(E_i')$ die Standardbasis des $\bR^k$. Nun gilt:
\begin{align*}
 F(E_j) = \sum_{i=1}^k a_{ij} E_i'
\end{align*}
Daraus erhalten wir Koeffizienten $a_{ij}$, welche eine Matrix bilden. Umgekehrt können wir aus den Koeffizienten die Abbildung $F$ rekonstruieren, indem wir definieren:
\begin{align*}
 F(X) &= F\left(\sum_{j=1}^n x_j E_j\right) \\
 &= \sum x_j F(E_j) \\
 &= \sum_{j=1}^n x_j \sum_{i=1}^k a_{ij} E_i'\\
 &= \sum_{i=1}^k \left(\sum_{j=1}^n a_{ij} x_j\right) E_i'
\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\newpage
[missing stuff here]
\newpage
\begin{definition}
 Wir bezeichnen als $p_i : M \to k$ die Projektion eines Vektors auf die $i$-te Komponente.
\end{definition}
\begin{theorem}
\label{theorem:componentwisecontinuous}
 Sei $M$ ein metrischer Raum, $F : M \to \bR^n$ eine Abbildung und $x \in M$. Dann ist $F$ stetig in $x$ genau dann, wenn $p_i \circ F$ stetig für alle $i$ ist.
\end{theorem}
\begin{proof}
\begin{enumerate}
 \item $p_i$ ist stetig. Ist also $F$ stetig folgt direkt, dass auch $p_i \circ F$ stetig ist.
 \item Angenommen, $p_i \circ F$ ist stetig $\forall i$, $\epsilon \in \bR^+$. Da $p_i \circ F$ stetig ist existiert eine Umgebung $U_i$ von $x$, sodass $|f_i(x) - f_i(y)| < \frac{\epsilon}{\sqrt{n}} \forall y \in U_i$. Ebenso für die anderen Komponenten. Nun gilt:
 \begin{align*}
  \norm{F(y) - F(x)} \leq \sqrt{n} \norm{F(x) - F(y)}_{\max} \leq \epsilon
 \end{align*}
\end{enumerate} 
\end{proof}
Analog gilt das Selbe für Stetigkeit auf $M$, gleichmäßige Stetigkeit, etc.
\begin{definition}
 Sei $M \subseteq \bR^n$, $F : M \to \bR^k$ eine Abbildung, $x_0$ ein Häufungspunkt, $y \in \bR^k$. Dann definieren wir:
 \begin{align*}
  \lim_{x \to x_0} F(x) = y \Leftrightarrow \forall \epsilon \in \bR^+ : \exists \delta \in \bR^+ : \forall x \in M \setminus \{x_0\} : \norm{x - x_0} \leq \delta \implies \norm{F(x) - y} < \epsilon
 \end{align*}
 $F$ ist stetig in $x_0$ genau dann, wenn $\lim_{x \to x_0} F(x) = F(x_0)$.
\end{definition}
\begin{theorem}
 Sei $M \subseteq \bR^n$, $F : M \to \bR^k$ eine Abbildung, $X_0 \in M$ ein Häufungspunkt, $Y \in \bR^k$ und $f_i = p_i \circ F$. Dann gilt:
 \begin{align*}
  \lim_{X \to X_0} F(X) = Y \Leftrightarrow \forall i : \lim_{X \to X_0} f_i(X) = y_i
 \end{align*}
\end{theorem}
\begin{proof}
 Analog zu Beweis \ref{theorem:componentwisecontinuous}.
\end{proof}
\begin{corollary}
 \begin{align*}
  F(X) \to Y, G(X) \to Z \implies F(X) + G(X) \to Y + Z
 \end{align*}
\end{corollary}
\section{Mehrdimensionale Ableitungen}
\begin{beispiel}
 Sei $f : M \to \bR$ definiert auf einer offenen Menge $M \subseteq \bR^n$.
 \begin{align*}
  f(X) = f(x_1, \hdots, x_n) \text{ bzgl. der Standardbasis }
 \end{align*}
 Wir können aber auch $X = \sum x_i' E_i'$ bezüglich einer beliebigen anderen Basis darstellen. Also:
 \begin{align*}
  f(X) = f(x_1, \hdots, x_n) = g(x_1', \hdots, x_n')
 \end{align*}
 Da $f$ in der Regel nicht linear ist, ist ein solcher Basiswechsel sehr viel komplizierter als in der Linearen Algebra! Wo möglich ist es also besser, über $f(X)$ zu reden.
\end{beispiel}
\begin{definition}
 Sei $f : \bR^n \to \bR$, $\overline{X} \in M$. Betrachte die Abbildung \[t \to f(\overline{x}_1, \hdots \overline{x}_{i-1}, t, \overline{x}_{i+1}, \hdots, \overline{x}_n),\] welche eine Mehrdimensionale Funktion $f(x_1, \hdots x_n)$ auf eine eindimensionale Funktion $f(t)$ abbildet. \ul{Achtung:} Wir nehmen hier implizit eine Darstellung bezüglich der Standardbasis an!
\end{definition}
\begin{beispiel}
 Betrachte folgende Funktion:
 \begin{align*}
  f(x,y) = \begin{cases}
            \frac{xy}{x^2 + y^2} & (x,y) \neq (0,0)\\
            0 & (x,y) = (0,0)\\
           \end{cases}
 \end{align*}
 $f$ ist an $(0,0)$ partiellen differenzierbar, die Partiellen Ableitungen sind $0$. Allerdings gilt \begin{align*}
  \forall x : f(x,x) = \frac{1}{2}
 \end{align*}
 Also ist $f$ an $0$ nicht stetig! Es existieren also Funktionen, die an einem Punkt partiell Differenzierbar sind, an dem sie nicht stetig sind.
\end{beispiel}
\ul{Idee:} Fordere partielle Differenzierbarkeit bezüglich jeder möglichen Basis, also partielle Differenzierbarkeit in jedem Vektor.
\begin{beispiel}
 \begin{align*}
  f(x,y) = \begin{cases}
            \frac{x^2y}{x^4 + y^2} & (x,y) \neq (0,0)\\
            0 & (x,y) = (0,0)\\
           \end{cases}
 \end{align*}
 Wir betrachten die ``Linearisierung'' $t \to f(t, \alpha t)$. Einsetzen liefert:
 \begin{align*}
  f(t, \alpha t) = \frac{\alpha t}{t^2 + a^2}
 \end{align*}
 Diese Funktion ist differenzierbar, also ist $f$ differenzierbar bezüglich beliebiger Basen. Das reicht jedoch immer noch nicht:
 \begin{align*}
  f(a, a^2) = \frac{a^2a^2}{a^4+a^4} = \frac{1}{2}
 \end{align*}
 Also ist $f$ immer noch nicht stetig - es ist stetig für Folgen, welche den Nullpunkt durch Geraden erreichen, aber nicht, wenn wir durch kompliziertere Pfade gegen den Nullpunkt gehen.
\end{beispiel}
\newpar
Wir wollen die Begriffe aus der Analysis I über Stetigkeit und Ableitbarkeit retten, also brauchen wir einen komplizierteren Ableitungsbegriff.
\section{Differenzierbarkeit}
Sei $f$ eine beliebige Funktion $\bR \to \bR$. Die Ableitung gibt uns die Tangente der Funktion an einem beliebigen Punkt, also die beste affine Approximation der Funktion an diesem Punkt.
\begin{definition}
 Eine Funktion $F : \bR^n \to \bR^k$ heißt \tbf{affin}, wenn es eine Lineare Funktion $L : \bR^n \to \bR^k$ und eine Konstante $Z \in \bR^k$ gibt, sodass:
 \begin{align*}
  F(X) = L(X) + Z
 \end{align*}
\end{definition}
\newpar
Sei $g : \bR \to \bR$ affin, also $g(x) = cx + t$ für $c,t \in \bR$. Sei $f : \bR \to \bR$. Wir wollen eine beliebige Funktion $f$ an der Stelle $x_0$ approximieren. Für eine gute Approximation wollen wir $f(x_0) = g(x_0)$, also erhalten wir:
\begin{align*}
 g(x) = c(x - x_0) + f(x_0).
\end{align*}
Schreibe $x = x_0 + h$ und lasse $h$ gegen $0$ gehen.
\begin{align*}
 h \to f(x_0 + h) - g(x_0 + h) = f(x_0 + h) - f(x_0) - ch
\end{align*}
Wir sagen, die Approximation ist gut, wenn $f(x_0 + h) - f(x_0) - ch$ schneller gegen $0$ geht als $h$ selbst, also:
\begin{align}
\label{eq:goodapprox}
 \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0) - ch}{h} = 0
\end{align}
Was äquivalent ist zu:
\begin{align*}
 c = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}\\
\end{align*}
Wir sagen also, $f$ ist in $x_0$ differenzierbar, genau dann, wenn eine lineare Abbildung $L$ existiert, sodass:
\begin{align*}
 \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0) - L(h)}{h} = 0
\end{align*}
Diese geometrische Intuition, nach der die Ableitung die beste affine Approximation der Funktion an einem gegebenen Punkt ist, können wir auf den $\bR^n$ übertragen. Analog zu der Interpretation affiner Funktionen als Geraden in $\bR$, also der Ableitung als das Finden einer Tangentengeraden auf dem Funktionengraph, sucht man beim Ableiten einer Mehrdimensionalen Funktion eine Tangenten(hyper-)ebene auf dem Funktionengraph.
\begin{definition}
 Sei $M \subset \bR^n$ offen, $F : M \to \bR^k$ eine Abbildung, sei $X_0 \in M$. Die Abbildung $F$ heißt \tbf{differenzierbar} am Punkt $X_0$, wenn es eine Lineare Abbildung $L : \bR^n \to \bR^k$ gibt, sodass:
 \begin{align*}
  \lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - L(H)}{\norm{H}} = 0.
 \end{align*}
 Wir nennen sie das \tbf{Differenzial von $F$ im Punkt $X_0$} und notieren sie als $DF_{X_0}$. $F$ heißt differenzierbar, wenn sie differenzierbar an jedem Punkt $X \in M$ ist.
\end{definition}
\begin{anmerkung}
 Differenzierbarkeit kann analog über die Eigenschaften des Restglieds $R(X, X_0)$ definiert werden: Sei
 \begin{align*}
  f(X) = f(X_0) + D f_{X_0}(X - X_0) + R(X, X_0).
 \end{align*}
 Dann ist $f$ genau dann differenzierbar, wenn:
 \begin{align*}
  \lim_{X \to X_0} \frac{R(X, X_0)}{\norm{X - X_0}} = 0
 \end{align*}

\end{anmerkung}

\begin{theorem}
 Gibt es ein Differential, ist es eindeutig bestimmt.
\end{theorem}
\begin{proof}
 Seien $L_1, L_2$ Differentiale. Es folgt:
 \begin{align*}
  \lim_{H \to 0} \frac{L_1(H) - L_2(H)}{\norm{H}} = 0
 \end{align*}
 Sei $X \in \bR^n \setminus \{0\}$. Dann gilt:
 \begin{align*}
  \lim_{t \to 0} \frac{L_1(tX) - L_2(tX)}{\norm{tX}} = 0
 \end{align*}
 %also:
 \begin{align*}
  \implies \frac{L_1(X) - L_2(X)}{\norm{X}} = 0
 \end{align*}
 %also:
 \begin{align*}
  \implies L_1(X) - L_2(X) = 0
 \end{align*}
 also sind die beiden Differentiale identisch.
\end{proof}
\begin{anmerkung}
 Unserer Differenzierbarkeitsbegriff wird insbesonders in der älteren Literatur oft als \tbf{totale Differenzierbarkeit} bezeichnet.
\end{anmerkung}
\begin{theorem}
 Ist $F: M \to \bR^k$ an einem Punkt $X_0$ differenzierbar, so ist $F$ an diesem Punkt stetig.
\end{theorem}
\begin{proof}
 Sei $F$ differenzierbar. Da die Differenzierbarkeit über den Limes des Differentialquotienten definiert ist folgt direkt:
 \begin{align*}
  \forall \epsilon \in \bR^+ : \exists \delta_1 \in \bR^+ : \forall H \in M : (X_0 + H \in M) \wedge (0 \leq \norm{H} \leq \delta_1)\\ 
  \implies \frac{\norm{F(X_0 + M) - F(X_0) - DF_{X_0}(H)}}{\norm{H}} \leq \frac{\epsilon}{2}\\
  \implies \norm{F(X_0 + M) - F(X_0) - DF_{X_0}(H)} \leq \frac{\epsilon}{2}\norm{H}\\
 \end{align*}
 Da $DF_{X_0}$ eine lineare Abbildung ist ist $DF_{X_0}$ gleichmäßig stetig, also gilt:
 \begin{align*}
  \exists \delta_2 \in \bR^+ : \norm{H} < \delta_2 \implies \norm{DF_{X_0}(H)} \leq \frac{\epsilon}{2} 
 \end{align*}
 Also gilt für $\norm{H} \leq \delta := \min\{\delta_1, \delta_2, 1\}$
 \begin{align*}
  &\norm{F(X_0 + H) - F(X_0)}\\
  = &\norm{F(X_0 + H) - F(X_0) - DF_{X_0}(H) + DF_{X_0}(H)}\\
  \leq &\norm{F(X_0 + H) - F(X_0) - DF_{X_0}(H)} + \norm{DF_{X_0}(H)}\\
  \leq &\frac{\epsilon}{2}\norm{H} + \frac{\epsilon}{2}\\
  \leq &\epsilon
 \end{align*}

\end{proof}

\begin{theorem}
 Sind $F$ und $G$ differenzierbar, so auch $F + G$, und es gilt
 \begin{align*}
  D(F + G)_{X_0} = DF_{X_0} + DG_{X_0}
 \end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
 &\lim_{H \to 0} \frac{(F+G)(X_0 + H) - (F + G)(X_0) - (DF_{X_0} + DG_{X_0})}{\norm{H}}\\
 = &\lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - DF_{X_0}}{\norm{H}} + \lim_{H \to 0} \frac{G(X_0 + H) - G(X_0) - DG_{X_0}}{\norm{H}}\\
 = &0\\
\end{align*}
\end{proof}

\begin{theorem}
\ul{\tbf{Kettenregel:}} Seien $M \subseteq \bR^n$, $N \subseteq \bR^n$ offen, seien $F: M \to N$, $G : N \to \bR^m$ Abbildungen, sei $X_0$
\end{theorem}
\begin{proof}
 Sei $F(X_0) = Y_0, F(X_0 + H) - F(X_0) = Z$, $H \in \bR^n \setminus \{0\}$, $X_0 + H \in M$.
 \begin{align*}
    &\frac{1}{\norm{H}} ((G \circ F)(X_0 + H) - (G \circ F)(X_0) - DG_{F(X_0)} \circ DF_{X_0}(H))\\
    =&\frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H))\\
    =&\frac{1}{\norm{H}}(DG_{Y_0}(F(X_0 + H) - F(X_0)) - DG_{Y_0}(DF_{X_0}(H)))\\
    =&\frac{1}{\norm{H}}DG_{Y_0}((F(X_0 + H) - F(X_0)) - DF_{X_0}(H))\\
 \end{align*}
 \begin{align*}
  \lim_{H \to 0}\frac{1}{\norm{H}}DG_{Y_0}((F(X_0 + H) - F(X_0)) - DF_{X_0}(H)) = DG_{Y_0}(0) = 0\\
 \end{align*}
 \begin{align*}
  \frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H)) = 
  \begin{cases}
    0 & Z_H = 0\\
    \frac{1}{\norm{H}}(G(Y_0 + Z_H) - G(Y_0) - DG_{Y_0}(Z_H)) & Z_H \neq 0 
  \end{cases}
 \end{align*}
 Der Term zweite Term in $Z_H \neq 0$ geht gegen $0$ für $H \to 0 \implies Z_H = F(X_0 + H) - F(X_0) \to 0$
 \begin{align*}
  \frac{\norm{Z_H}}{\norm{H}} &= \frac{\norm{F(X_0 + H) - F(X_0)}}{\norm{H}} \\
  &= \frac{\norm{DF_{X_0}(H) - R(X_0, H)}}{\norm{H}}\\ 
  &\leq \frac{\norm{DF_{X_0}(H)}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\leq \frac{\norm{DF_{X_0}(H)}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\overset{???}{\leq} \frac{\norm{DF_{X_0}}\norm{H}}{\norm{H}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &= \norm{DF_{X_0}} + \frac{\norm{R(X_0, H)}}{\norm{H}}\\
  &\leq c\\
 \end{align*}
\end{proof}
\begin{theorem}
 Seien $I \subseteq \bR$, $N \subseteq \bR^k$ offen, seien $F : I \to N$, $G : N \to \bR^n$ Abbildungen.
 \newpar
 Ist $F$ differenzierbar in $t_0 \in I$ und $G$ differenzierbar in $F(t_0)$, so gilt:
 \begin{align*}
  (G \circ F)'(t_0) = DG_{F(t_0)} (F'(t_0))
 \end{align*}
\end{theorem}
\begin{proof}
 Gemät Kettenregel gilt $D(G \circ F) = DG_{F(t_0)} \circ DF_{t_0}$. Nun gilt:
 \begin{align*}
  h(G \circ F)'(t_0) &= h D(G \circ F)_{t_0} (1)\\ &= D(G \circ F)_{t_0}(h)\\ &= DG_{F(t_0)}(DF_{t_0}(h))\\
  &= h DG_{F(t_0)}(F'(t_0))
 \end{align*}
\end{proof}
\newpar
Mittelwertsatz: $f : [x,y] \to \bR$, dann $\exists y : f(y) - f(x) = f'(z)(y-x)$. Im Allgemeinen ist dieser im Mehrdimensionalen Fall leider falsch.
\newpar
Betrachte allerdings die folgende Ungleichung, welche die Wichtigste Konsequenz des Mittelwertsatzes ist: $\abs{f(y) - f(x)} \leq \abs{f'(z)}\abs{y-x} \leq c \abs{y-x}$. Diese kann im Allgemeinen erhalten werden.
\newpar
$F : M \subset \bR^n \to \bR^k$ $X,Y \in M$. Sei $[X,Y] = \{(1- \lambda)X + \lambda Y\}$ die Verbindungslinie zwischen den beiden Vektoren.
\begin{theorem}
 Sei $M \subseteq \bR^n$ offen, $X,Y \in M$ mit $[X,Y] \subseteq M$. Die Abbildung $F : M \to \bR^k$ sei stetig in $M$ und differenzierbar in den Punkten $(1-\lambda)X + \lambda Y$ mit $\lambda \in (0,1)$. Gilt
 \begin{align*}
  \forall \lambda \in (0,1) : \forall (1-\lambda)X + \lambda Y : \norm{DF_Z} \leq c 
 \end{align*}
 so gilt auch
 \begin{align*}
  \norm{F(Y) - F(X)} \leq c\norm{Y - X}
 \end{align*}
 \begin{proof}
  Angenommen $G : [0,1] \to \bR^k$ ist stetig auf $[0,1]$ und differenzierbar auf $(0,1)$. So gilt 
  \begin{align*}
  \forall t \in (0,1) : \norm{G'(t)} \leq c
  \end{align*}
 \newpar
 Sei $\epsilon \in \bR^+$ und 
 \begin{align*}
  A := \{t \in [0,1] \mid \norm{G(t) - G(0)} \leq (c + \epsilon) t + \epsilon\}
 \end{align*}
 Da $G$ stetig in $0$ ist gilt $[0, \tau] \subseteq A$.
 \newpar
 Sei $s = \sup A$. Es gilt $0 < s \leq 1$, also ist $G$ stetig in $s$.
 \newpar
 Da $t \in A \implies t \leq s$ 
 \begin{align*}
  \norm{G(t) - G(0)} \leq (c + \epsilon) t + \epsilon \to s
  \implies \norm{G(s) - G(0)} \leq (c + \epsilon) s + \epsilon
 \end{align*}
 also $s \in A$. Angenommen, $s < 1$. Dann gilt $\exists h > 0 : s + h < 1$.
 \begin{align*}
  \norm{\frac{G(s + h) - G(s)}{h} - G'(s)} \leq \epsilon
 \end{align*}
 \begin{align*}
  \implies \norm{\frac{G(s + h) - G(s)}{h}} \leq \epsilon +  G'(s) \leq c + \epsilon
 \end{align*}
 \begin{align*}
  \norm{G(s + h) - G(0)} &\leq \norm{G(s + h) - G(s)} + \norm{G(s) - G(0)}\\
                         &\leq (c + \epsilon) h + (c + \epsilon) s + \epsilon\\
                         &\leq (c + \epsilon)(s + h) + \epsilon\\
 \end{align*}
 Daraus folgt $s + h \in A$. Da $s$ das Supremum ist ist dies ein Widerspruch. Also gilt $h = 1$.
 \begin{align*}
  \forall \epsilon \in \bR^+ : \norm{G(1) - G(0)} \leq &c + \epsilon + \epsilon = c + 2 \epsilon\\
  \implies &\norm{G(1) - G(0)} \leq c
 \end{align*}
 Sei $F$ wie im Satz. Sei $K : [0,1] \to \bR^n : t \to (1-t)X + tY$. Diese Abbildung ist affin, also differenzierbar. Es gilt $K'(t) = Y - X$. $F \circ K$ ist diffbar in $(0,1)$
 \begin{align*}
  D(F \circ K)_t = DF_{K(t)} \circ DK_t
 \end{align*}
 \begin{align*}
  (F \circ K)'(t) = DF_{K(t)}(K'(t)) = DF_{K(t)}(Y-X)
 \end{align*}
 \begin{align*}
  \norm{(F \circ K)'(t)} = \norm{DF_{K(t)}(Y - X)} \leq \norm{DF_{K(t)}}\norm{Y - X} \leq c \norm{Y - X}
 \end{align*}
 Mit $G := F \circ K$ und $c := c\norm{Y - X}$
 \begin{align*}
  \norm{F(Y) - F(X)} \leq c\norm{Y - X}
 \end{align*}

 \end{proof}
\end{theorem}
[missing stuff - gradients]
\begin{definition}
 Eine Funktion $f$ heißt \tbf{partiell differenzierbar}, wenn für jede Koordinatenachse $i$ die Partielle Ableitung $\forall i \in \{0, \hdots, n\} : \partial_i f : M \subseteq \bR \to \bR : X \to \delta_i f(X)$ existiert.
\end{definition}
\begin{theorem}
 Ist $f: M \to \bR$ in einer Umgebung von $X_0$ partiell differenzierbar und sind die partiellen Ableitungen in $X_0$ stetig, so ist $f$ in $X_0$ differenzierbar.
\end{theorem}
\begin{proof}
 Sei $U$ ein offener Ball um $X_0$, welcher vollständig in $M$ enthalten ist. Sei $H \in \bR^n$, sodass $X_0 + H \in U$. Nun gilt:
 \begin{align*}
  f(X_0 + H) - f(X_0) &= \sum_{i=1}^n (f(x_1, \hdots, x_{i-1}, \hdots, x_i + h_i, x_{i+1} + h_{i+1} , \hdots, x_n + h_n))\\
  &- \sum_{i=1}^n (f(x_1, \hdots, x_{i-1}, \hdots, x_i, x_{i+1} + h_{i+1} , \hdots, x_n + h_n))\\
 \end{align*}
 Die Summenglieder sind partielle Ableitung. Nach Mittelwertsatz erhalten wir:
 \begin{align*}
  \sum_{i=1}^n h_1 \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1}, \hdots, x_n)\ c_i \in (0,1)\\
 \end{align*}
 Nun gilt:
 \begin{align*}
  &\frac{1}{\norm{H}} \abs{f(X_0 + H) - f(X_0) - \scalar{\grad f(X_0)}{H}}\\
  =&\frac{1}{\norm{H}} \abs{\sum_{i=1}^n h_1 \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1} + h_{i+1}, \hdots, x_n + h_n) - \partial_i(f(x_0, \hdots, x_n))} \\
  \leq&\abs{\sum_{i=1}^n \partial_i f(x_1, \hdots x_{i-1}, x_i + c_ih_i, x_{i+1} + h_{i+1}, \hdots, x_n + h_n) - \partial_i(f(x_0, \hdots, x_n))} \to 0\\
 \end{align*}
\end{proof}
 Sei $M \subseteq \bR^n$ offen, $X_0 \in M$, $F: M \to \bR^k$. Seien $\forall i \in \{1, \hdots, n\} f: M \to \bR^n \to \bR$ Koordinatenfunktionen.
 \begin{align*}
  F(X) = (f_1(X), \hdots, f_k(X)) = \sum_{i=1}^k f_i(X)E_i'
 \end{align*}
 \begin{align}
  Y = F(X) \Leftrightarrow \forall i : y_i = f_i(x_1, \hdots, x_n)
 \end{align}
\begin{theorem}
Die Abbildung $F$ ist genau dann differenzierbar in $X_0$, wenn alle Koordinatenfunktionen $f_i$ in $X_0$ differenzierbar sind. Ist das der Fall, gilt:
\begin{align*}
 DF_{X_0}(H) = \sum_{i=1}^k (Df_i)_{X_0}(H)E_i' \forall H \in \bR^n 
\end{align*}
\end{theorem}
\begin{proof}
 $L : \bR^n \to \bR^n$ linear. Dann
 \begin{align*}
  \lim_{H \to 0} \frac{F(X_0 + H) - F(X_0) - L(H)}{\norm{H}} = 0 \Leftrightarrow \lim_{H \to 0} \frac{f_i(X_0 + H) - f_i(X_0) - (D_i \circ L)(H)}{\norm{H}} = 0
 \end{align*}
 \end{proof}
 Wir wollen nun das Differential bezüglich der Standardbasis übersichtlich darstellen. Es gilt:
 \begin{align*}
  L(E_j) = \sum_{i=1}^k a_{ij} E_i'\\
  DF_{X_0} = \sum_{i=1}^k \partial_j f_i(X_0) E_j'
 \end{align*}
 Die Koeffizienten der Darstellenden Matrix sind also identisch mit den Partiellen Ableitungen.
\begin{theorem}
 Sei $F : M \to \bR^k$ differenzierbar in $X_0 \in M$. Dann wir das Differential $DF_{X_0}$ bezüglich der Standardbasis in $\bR^n$ und $\bR^k$ beschrieben als die $k \times n$-Matrix
 \begin{align*}
  JF(X_0) = (\delta_j f_i(X_0))_{1 \leq i \leq n, 1 \leq j \leq k}
 \end{align*}
 Sie heißt die Funktionalmatrix oder Jacobimatrix von $F$ in $X_0$. Falls $k = n$ wird die Determinante dieser Matrix als Funktionaldeterminante oder Jacobideterminante von $F$ in $X_0$ bezeichnet.
\end{theorem}
\newpar
[missing stuff]
\newpar
\begin{theorem}
 Ist $r \geq 2$ und $f \in C^r(M)$, so sind die partiellen Ableitungen von $f$ bis zur Ordnung $r$ unabhängig von der Reihenfolgen es gilt also:
 \begin{align*}
  \partial_1 \hdots \partial_r f = \partial_{\sigma(1)} \hdots \delta_\sigma(r) f
 \end{align*}
\end{theorem}
\begin{theorem}
 \emph{\tbf{Taylor-Formel}}: Sei $g: [-\epsilon, h] \to \bR$ $\epsilon, h > 0$.
 Sei $g$ $(k+1)$- mal differenzierbar. Dann gilt:
 \begin{align*}
  \exists c \in (0,h) : g(h) = \sum_{j = 0}^k \frac{1}{j!} g^{(j)} (0) h^j + \frac{1}{(k+1)!} g^{(k+1)}(c) h^{k+1}
 \end{align*}
 Sei $f : M \to \bR$ $M \subseteq \bR^n$ offen, $x_0 \in M$. Sei $k \in \bN$, $f \in C^k(M)$ mit partielle differenzierbaren partiellen Ableitungen $k$-ter Ordnung, $H \in \bR^n : [x_0, x_0+H] \subseteq M$. Sei $g(t) := f(x_0 + tH)$. Dann gilt für $r \in \{1, \hdots, k+1\}$:
 \begin{align*}
  g^{(r)}(t) = \sum_{i_1, \hdots i_r}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + tH) h_{i_1} \hdots h_{i_r}
 \end{align*}
 \begin{align*}
  g(1) = \sum_{r=0}^k \frac{1}{r!} g^{(r)}(0) + \frac{1}{(k+1)!} g^{(k+1)}(c)
 \end{align*}
\end{theorem}
\begin{theorem}
 \emph{\tbf{Mehrdimensionale Taylorformel:}} Sei $M \subseteq \bR^n$ offen, $X_0 \in M$, $H \in \bR^n$ mit $[X_0, X_0 + h] \subseteq M$, $k \in \bN$, $f \in C^k(M)$, sodass die partiellen Ableitungen der Ordnung $k$ in $M$ differenzierbar sind. Dann $\exists c \in (0,1)$, sodass:
 \begin{align*}
  f(X_0 + h) = f(X_0) + \sum_{r=1}^k \frac{1}{r!} \sum_{i_1, \hdots i_r = 1}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + tH) h_{i_1} \hdots h_{i_r} + \frac{1}{(k+1)!}  \sum_{i_1, \hdots i_r = 1}^n \delta{i_1} \hdots \delta{i_r} f(X_0 + cH) h_{i_1} \hdots h_{i_r}
 \end{align*}
 Kompakter für $k = 2$:
 \begin{align*}
  f(X_0 + H) = f(X_0) + \scalar{\grad f(X_0)}{H} + \frac{1}{2} \sum_{i,j=1}^n \partial_i \partial_j f(X_0) h_i h_h + R(X_0, h)
 \end{align*}
 \begin{align*}
  R(X_0, H) = \frac{1}{6}\sum_{i,j,k=1}^n \delta_i \delta_j \delta_k f(Y) h_i h_j h_k \quad Y \in [X_0, X_0 + H]
 \end{align*}
 Falls die dritten Ableitungen auf der Verbindungslinie beschränkt sind gilt:
 \begin{align*}
  \lim_{H \to 0} \frac{R(X_0; H)}{\norm{H}^2} = 0
 \end{align*}
\end{theorem}
\begin{theorem}
 Sei $f : M \to \bR$ zweimal partiell differenzierbar in $X_0$. Dann heißt die durch
 \begin{align*}
  Q(f,X_0;H) := \sum_{i,j=1}^n \delta_i \delta_j f(X_0) h_i h_j
 \end{align*}
 definierte Funktion $Q(f,X_0;H$ die \tbf{Hesse-Form} von $f$ im Punkt $X_0$ und die dadurch definierte Matrix
 \begin{align*}
  \Hess{f, X_0}_{ij} = (\partial_i \partial_j f(X_0))
 \end{align*}
 heißt die \tbf{Hesse-Matrix} von $f$ in $X_0$.
\end{theorem}
%
%
%
%
%
%
%
%
%
%
%
\end{document}
