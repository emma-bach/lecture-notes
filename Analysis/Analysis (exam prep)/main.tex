\documentclass{report}
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage[titles]{tocloft}
\usepackage[titletoc]{appendix}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{tikz-cd}
\usepackage{physics}
\usepackage{placeins}

%hyperref should be last apparently
\usepackage{hyperref}

\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\epsilon{\varepsilon}
\renewcommand\phi{\varphi}

% Starts a new paragraph without indentation
% and with an empty line between paragraphs
\newcommand*{\newpar}{\par\vspace{\baselineskip}\noindent}
\newcommand{\trans}{\twoheadrightarrow}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\Hess}[1]{\text{Hess}(#1)}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\ve}{\vec{e}}
\newcommand{\vh}{\vec{h}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{0}}
\newcommand{\zz}{\vec{z}}

\newcommand{\tbA}{\mathbf{A}}
\newcommand{\tbB}{\mathbf{B}}
\newcommand{\tbC}{\mathbf{C}}
\newcommand{\tbD}{\mathbf{D}}
\newcommand{\tbE}{\mathbf{E}}
\newcommand{\tbY}{\mathbf{Y}}
\newcommand{\tbZ}{\mathbf{Z}}

\newcommand{\an}{(a_n)_{n \in \bN}}
\newcommand{\bn}{(b_n)_{n \in \bN}}
\newcommand{\sn}{(s_n)_{n \in \bN}}

\renewcommand{\tr}{\text{tr}\ }
\newcommand{\rang}{\text{rang}\ }

\newcommand{\Mat}[3]{\text{Mat}^{#1}_{#2}\left(#3\right)}
\newcommand{\scalar}[2]{\left\langle #1, #2 \right\rangle}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}

\NewDocumentEnvironment{nalign}{}{\equation\aligned}{\endaligned\endequation}

\begin{document}
% \newtheorem{codename}{printedname}[countedwith]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}[lemma]{Satz}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Korollar}

\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{beispiel}[lemma]{Beispiel}
\newtheorem{beobachtung}[lemma]{Beobachtung}
\newtheorem{anmerkung}[lemma]{Anmerkung}
\newtheorem{question}[lemma]{Frage}
\newtheorem{application}[lemma]{Anwendung}
\newtheorem{konsequenz}[lemma]{Konsequenz}

%
%
%
\include{title}
\tableofcontents
\thispagestyle{fancy}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Folgen}
\begin{definition}
	Sei $(a_n)_{n \in \bN}$ eine Folge. $a \in \bR$ heißt \textbf{Grenzwert} von $(a_n)_{n \in \bN}$, falls
	\begin{align*}
		\forall \epsilon \in \bR^+ : \exists n_0 \in \bN : \forall n \geq n_0 : \abs{a_n - a} \leq \epsilon
	\end{align*}
\end{definition}
\begin{definition}
	Eine Folge heißt \textbf{konvergent}, falls die Folge einen Grenzwert hat. Sie heißt \textit{Nullfolge}, falls der Grenzwert $0$ ist.
\end{definition}
\begin{theorem}
	Jede konvergente Folge ist beschränkt.
\end{theorem}
\begin{theorem}
	\label{theorem:rechenregelnfürfolgen}
	Seien $\an$ und $\bn$ konvergent mit Grenzwerten $a$ und $b$. So gilt:
	\begin{itemize}
		\item $(a_n + b_n)_{n \in \bN}$ ist konvergent mit Grenzwert $a + b$
		\item $(a_n b_n)_{n \in \bN}$ ist konvergent mit Grenzwert $ab$
		\item Für $\lambda \in \bR$ ist $(\lambda a_n)_{n \in \bN}$ konvergent mit Grenzwert $\lambda a$.
		\item Falls $a \neq 0$ ist $(\frac{1}{a_n})_{n \in \bN}$ konvergent mit Grenzwert $\frac{1}{a}$.
	\end{itemize}
\end{theorem}
\begin{theorem}
	Seien $\an$ und $\bn$ konvergent mit Grenzwerten $a$ und $b$. Sei außerdem $\forall n \in \bN : a_n \leq b_n$. So ist auch $a \leq b$.
\end{theorem}
\begin{theorem}
	Seien $\an$ und $\bn$ konvergent mit Grenzwerten $a$ und $b$. Sei außerdem $\forall n \in \bN : a_n < b_n$. So ist immer noch $a \leq b$.
\end{theorem}
\begin{definition}
	Eine Folge $\an$ heißt \textbf{bestimmt divergent gegen $\infty$}, geschrieben
	\begin{align*}
		\lim_{n \to \infty} a_n = \infty
	\end{align*}
	Falls
	\begin{align*}
		\forall c \in \bR : \exists n_0 \in \bN : \forall n \geq n_0 : a_n > c
	\end{align*}
\end{definition}
\begin{definition}
	Eine Zahl heißt \textbf{Häufungspunkt} oder \textbf{Häufungswert} einer Folge, wenn sie der Grenzwert einer Teilfolge ist.
\end{definition}
\begin{theorem}
	\emph{\textbf{Bolzano-Weierstraß:}} Jede beschränkte Folge in $\bR$ besitzt eine konvergente Teilfolge.
\end{theorem}
\begin{theorem}
	Hat eine beschränkte Folge genau einen Häufungswert, so konvergiert sie gegen diesen Häufungswert.
\end{theorem}
\begin{definition}
	Eine Folge heißt \textbf{Cauchy-Folge}, falls 
	\begin{align*}
		\forall \epsilon \in \bR : \exists n_0 \in \bN : \forall m, n \geq n_0 : \abs{a_m - a_n} < \epsilon
	\end{align*}
\end{definition}
\begin{theorem}
	Jede Cauchyfolge ist konvergent.
\end{theorem}
\begin{theorem}
	Jede konvergente Folge in $\bR$ ist Cauchy.
\end{theorem}
\begin{theorem}
	Jede monoton wachsende, beschränkte Folge ist Cauchy.
\end{theorem}
\begin{definition}
	Gegeben eine beschränkte Folge $\an$ nennen wir den Grenzwert der oberen Schranken der Folge den \textbf{Limes superior}:
	\begin{align*}
		\limsup_{n \to \infty} a_n = \lim_{n \to \infty} \left(\sup_{m \geq n} a_m\right)
	\end{align*}
	Analog definieren wir für untere Schranken den \textbf{Limes inferior}:
	\begin{align*}
		\liminf_{n \to \infty} a_n = \lim_{n \to \infty} \left(\inf_{m \geq n} a_m\right)
	\end{align*}
\end{definition}
\begin{theorem}
	Es gilt $\limsup_{n \to \infty} a_n \leq a$ gdw. für alle $\epsilon \in \bR$ die Menge $\{k \in \bN \mid a_k \geq a + \epsilon\}$ endlich ist.
\end{theorem}
\begin{theorem}
	Es gilt $\limsup_{n \to \infty} a_n \geq a$ gdw. für alle $\epsilon \in \bR$ die Menge $\{k \in \bN \mid a_k \geq a - \epsilon\}$ unendlich ist.
\end{theorem}
\begin{theorem}
	Sei $\an$ eine beschränkte Folge. So ist $\limsup a_n$ der größte und $\liminf a_n$ der kleinste Häufungswert der Folge.
\end{theorem}
\begin{corollary}
	Die Folge $\an$ konvergiert genau dann, wenn sie beschränkt ist mit $\limsup a_n = \liminf a_n$.
\end{corollary}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Reihen}
\begin{definition}
	Sei $\an$ eine Folge. Wir nennen die Zahl
	\begin{align*}
		s_n := \sum_{k = 1}^n a_k
	\end{align*}
	Die \textbf{$n$-te Partialsumme} der Folge. Wir nennen Folgen der Form $\sn$ unendliche Reihen und schreiben solche auch als
	\begin{align*}
		\sum_{k = 1}^\infty a_k.
	\end{align*}
	Ist die Folge $\sn$ konvergent, bezeichnet dieser Ausdruck außerdem den Grenzwert der Folge. Per Definition ist
	\begin{align*}
		\sum_{k = 1}^\infty a_k = s \Leftrightarrow \lim_{n \to \infty} \sum_{k = 1}^n a_k = s
	\end{align*}
\end{definition}
\newpar
Gemäß Satz \ref{theorem:rechenregelnfürfolgen} können \textbf{konvergente} Reihen wie intuitiv erwartet addiert, skalarmultipliziert etc. werden.
\section{Konvergenzkriterien}
\begin{theorem} \textbf{\emph{Nullfolgentest:}} Ist eine Reihe $\displaystyle \sum_{k = 1}^\infty a_k$ konvergent, so folgt $\displaystyle \lim_{k \to \infty} a_k = 0$.
\end{theorem}
\begin{theorem} \textbf{\emph{Konvergenzkriterium von Cauchy:}} Eine Reihe $\displaystyle \sum_{n = 1}^\infty a_k$ ist genau dann konvergent, wenn 
	\begin{align*}
		\forall \epsilon \in \bR^+ : \exists n_0 \in \bN : \forall n,m > n_0 : \abs{\sum^m_{k = n} a_k} < \epsilon
	\end{align*}	
\end{theorem}
\begin{proof}
	Dies ist genau die Bedingung, dass die Reihe eine Cauchyfolge ist.
\end{proof}
\begin{theorem} 
	\textbf{\emph{Reihen mit positiven Gliedern:}} Falls $\forall k \in \bN : a_k > 0$, so konvergiert die Reihe $\sum_{k = 1}^\infty a_k$ genau dann, wenn die Folge ihrer Partialsummen nach oben beschränkt ist.
\end{theorem}
\begin{theorem} \textbf{\emph{Leibnizkriterium:}}
	Sei $a_k$ eine reelle, monoton fallende Nullfolge. So ist die Reihe 
	$\sum_{k = 1}^\infty (-1)^k a_k$ konvergent. Es gilt außerdem:
	\begin{align*}
		0 \leq (-1)^n \sum_{k = n}^\infty (-1)^k a_k \leq a_n
	\end{align*}
\end{theorem}
\begin{theorem}
	\emph{\textbf{Majorantenkriterium:}} Gilt $\abs{a_k} \leq c_k \in [0, \infty)$ und $\sum_{k=0}^\infty c_k < \infty$, so konvergiert die Reihe $\sum_{k=0}^\infty a_k$ absolut.
\end{theorem}
\begin{theorem}
	\emph{\textbf{Quotientenkriterium:}} Gibt es ein $\theta \in [0,1)$ und ein $n \in \bN$ sodass 
	\begin{align*}
		\forall k \geq n : \frac{\abs{a_{k+1}}}{\abs{a_k}} \leq \theta,
	\end{align*}
	so konvergiert die Reihe $\sum_{k=0}^\infty a_k$ absolut.
\end{theorem}
\begin{theorem}
	Eine äquivalente Bedingung ist $\limsup \abs{\frac{a_{n+1}}{a_n}} < 1$.
\end{theorem}
\begin{beispiel}
	Wir betrachten die Reihe
	\begin{align*}
		\sum_{k=1}^\infty a_k := \sum_{k=1}^\infty \frac{k}{e^k}
	\end{align*}
	Der Quotiententest liefert für $k \geq 10$:
	\begin{align*}
		\abs{\frac{a_{k+1}}{a_k}} = \abs{\frac{(k + 1)e^k}{ke^{k+1}}} = \abs{\frac{k + 1}{ke}} < \frac{1}{2}
	\end{align*}
	Also konvergiert die Reihe absolut. (Die Abschätzung ist hier natürlich sehr grob.)
\end{beispiel}
\begin{theorem}
	\emph{\textbf{Wurzelkriterium:}} Gibt es ein $\theta \in [0,1)$ und ein $n \in \bN$ sodass 
	\begin{align*}
		\forall k \geq n : \sqrt[k]{\abs{a_k}} \leq \theta,
	\end{align*}
	so konvergiert die Reihe $\sum_{k=0}^\infty a_k$ absolut.
\end{theorem}
\begin{anmerkung}
	Existiert ein $n \in \bN$, sodass
	\begin{align*}
		\forall k \geq n : \frac{\abs{a_{k+1}}}{\abs{a_k}} \geq 1
	\end{align*}
	oder
	\begin{align*}
		\forall k \geq n : \sqrt[k]{\abs{a_k}} \geq 1,
	\end{align*}
	so ist die Reihe offensichtlich nicht Cauchy, divergiert also.
\end{anmerkung}
\begin{theorem}
	Eine absolut konvergente Reihe kann beliebig umgeordnet werden, ohne den Grenzwert zu verändern.
\end{theorem}
%
%
%
%
%
%
%
%
\chapter{Stetigkeit}
\begin{definition}
	Sei $M \subset \bR$. Eine Zahl $a \in \bR \cup -\infty$ heißt \tbf{Infimum} der Menge $M$, wenn:
	\begin{enumerate}
		\item $\forall x \in M : a \leq x$
		\item $\forall a < a' : \exists x \in M : x < a'$
	\end{enumerate}
\end{definition}
\begin{definition}
	Sei $M \subset \bR$. Eine Zahl $a \in \bR \cup \infty$ heißt \tbf{Supremum} der Menge $M$, wenn:
	\begin{enumerate}
		\item $\forall x \in M : x \leq a$
		\item $\forall a' < a : \exists x \in M : a' < x$
	\end{enumerate}
\end{definition}
\begin{theorem}
	Sei $M$ nichtleer. So gibt es Folgen $x_n, y_n \in M$, sodass
	\begin{align*}
		\lim_{n \to \infty} x_n = \inf M
	\end{align*}
	und 
	\begin{align*}
		\lim_{n \to \infty} y_n = \sup M
	\end{align*}
\end{theorem}
\begin{proof}
	Im Fall einer nach oben unbeschränkten Menge lässt sich trivial eine Folge finden, welche gegen $\sup M = \infty$ konvergiert, eben so für das Infimum im Fall einer nach unten unbeschränkten Menge.
	\newpar
	Sei nun $a_1 \in M$ und $b_1 \geq M$. Wir konstruieren nun durch sukzessives Halbieren des Intervalls $[a_1, b_1]$ eine Intervallschachtelung, welche gegen das Supremum konvergiert:
	\begin{align*}
		I_{n+1} = [a_{n+1}, b_{n+1}] = 
		\begin{cases}
			[(a_n + b_n)/2, b_n] & [(a_n + b_n)/2, b_n] \cap M \neq \emptyset\\
			[a_n, (a_n + b_n)/2] &\\
		\end{cases}
	\end{align*}
\end{proof}
\begin{corollary}
	Jede Menge $M \subset \bR$ hat genau ein Supremum und genau ein Infimum.
\end{corollary}
\begin{proof}
	In der vorherigen Intervallschachtelung ist jedes Intervall Obermenge eines Intervalls in $M$.
\end{proof}
\begin{definition}
	\ul{\textbf{$\epsilon$-$\delta$-Kriterium:}} Seien $M, N$ Metrische Räume mit Metriken $d_M$ und $d_N$. Eine Funktion $f : M \to N$ heißt \textbf{stetig} an einem Punkt $p \in M$, wenn:
	\begin{align*}
		\forall \epsilon \in \bR^+ : \exists \delta \in \bR^+ : d_M(x,p) < \delta \implies d_N(f(x), f(p)) < \epsilon
	\end{align*}
\end{definition}
\begin{definition}
	\ul{\textbf{Allgemeine Topologische Stetigkeit:}} Seien $X, Y$ Topologische Räume. Eine Funktion $f : X \to Y$ heißt \textbf{stetig}, wenn das Urbild jeder offener Menge offen ist.
\end{definition}
\begin{theorem}
	\emph{\textbf{Zwischenwertsatz:}} Gegeben $a < b$ in $\bR$ nimmt eine stetige Funktion $f : [a,b] \to \bR$ jeden Wert zwischen $f(a)$ und $f(b)$ an.
\end{theorem}
\begin{proof}
	Sei $z \in [f(a), f(b)]$ und $p = \sup \{x \in [a,b] : f(p) \leq z\}$. Zu zeigen ist $f(p) = z$. Angenommen, $f(p) < z$. Da $z \leq f(b)$ folgt $p < b$. Sei nun $\epsilon = z - f(p)$. Aufgrund der Stetigkeit von $f$ existiert ein $\delta$, sodass für $q := p + \delta$ sowohl $p < q$ als auch $f(p) \leq z$ gelten. Somit war $p$ kein Supremum, was ein Widerspruch ist.
\end{proof}
\begin{corollary}
	Das Bild eines Intervalles unter einer Stetigen Funktion ist ebenfalls ein Intervall.
\end{corollary}
\begin{theorem}
	Ist das Bild einer monotonen Abbildung $f$ : $\bR \subset D \to \bR$ ein Intervall in $\bR$, so ist $f$ stetig.
\end{theorem}
\begin{corollary}
	Ist eine Funktion $f : [a,b] \to \bR$ streng monoton und stetig, so ist auch die Umkehrfunktion streng monoton und stetig.
\end{corollary}
\begin{proof}
	Die Umkehrfunktion ist klar monoton (sonst könnte man sie nicht erneut invertieren, um die ursprüngliche Funktion zu erhalten). Außerdem ist ihr Bild das Intervall $[a,b]$. Somit ist die Umkehrfunktion stetig.
\end{proof}
\begin{theorem}
	\emph{\textbf{Existenz von Extremalstellen auf Kompakta:}} Jede stetige Funktion auf einem nichtleeren Kompaktum hat ein Maximum und ein Minimum.\\
	\textbf{Alternativ:} Jede stetige Funktion auf einem nichtleeren Kompaktum nimmt das Supremum und Infimum ihrer Funktionswerte als Funktionswert an.\\
	\textbf{Also:}
	\begin{align*}
		\forall f : D \to \bR : \exists x_0, x_1 \in D : f(x_0) = \inf_{x \in D} f(x), \quad f(x_1) = \sup_{x \in D} f(x)
	\end{align*}
\end{theorem}
\begin{proof}
	Wir zeigen die Aussage über das Infimum, das Supremum folgt dann analog. Setze 
	\begin{align*}
		\alpha = \inf_{x \in D} f(x) \in [-\infty,\infty)
	\end{align*}
	Nach Definition des Infimums gibt es eine Folge $x_k \in D$ mit $f(x_k) \to a$. Da $D$ beschränkt ist, ist die Folge beschränkt. Nach Satz von Bolzano-Weierstraß gibt es eine Teilfolge $x_{k_j}$, welche gegen einen Wert $x_0 \in D$ konvergiert.
	\newpar
	Wir wissen, dass $f(x_{k_j})$ gegen $f(x_0)$ konvergiert (dies ist genau die Folgenstetigkeit von $f$). Aus der Eindeutigkeit des Grenzwerts folgt nun $f(x_0) = \alpha$.
\end{proof}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Differentiation}
\section{Differenzierbarkeitsbegriffe im Mehrdimensionalen}
\subsection{Totale Differenzierbarkeit}
\begin{definition}
	Eine Abbildung $f : \bR^n \supset \Omega \to \bR^m$ heißt \tbf{total differenzierbar} oder einfach \tbf{differenzierbar} am Punkt $p \in \Omega$, falls eine lineare Abbildung $Df(p) : \bR^n \to \bR^n$ gibt, sodass
	\begin{align*}
		\lim_{h \to 0} \frac{f(p + h) - f(p) - Df(p)(h)}{\abs{h}} = 0											
	\end{align*}
	Existiert ein solches $Df(p)$, nennen wir es das \tbf{Differential} von $f$ am Punkt $p$. Existiert eine solche Abbildung für alle Punkte $p \in \Omega$, so nennen wir die Abbildung $Df : \bR^n \supset \Omega \to L(\bR^n, \bR^m)$ \tbf{*das* Differential} von $f$.
\end{definition}
\begin{anmerkung}
	Im Fall $n =m = 1$ (also bei eindimensionaler Urbild- und Bildmenge) sind lineare Abbildungen $\bR \to \bR$ genau durch Multiplikation mit einer Konstanten $c \in \bR$ gegeben. In diesem Fall vereinfacht sich die Bedingung zu:
	\begin{align*}
		\exists c \in \bR : \lim_{h \to 0} \frac{f(p + h) - f(p) - ch}{\abs{h}} &= 0\\
		\implies \exists c \in \bR : \lim_{h \to 0} \frac{f(p + h) - f(p)}{\abs{h}} &= \lim_{h \to 0}-\frac{ch}{\abs{h}}\\
		\implies \exists c \in \bR : \lim_{h \to 0} \frac{f(p + h) - f(p)}{\abs{h}} &= c\\								
	\end{align*}
\end{anmerkung}
\begin{anmerkung}
	Ist die Funktion $f$ linear, so ist sie durch Matrixmultiplikation $f : x \to Ax$ gegeben und es gilt trivial $d(f) = A$.
\end{anmerkung}	
\subsection{Partielle Differenzierbarkeit}
\begin{definition}
	Die \tbf{partielle Ableitung} einer Funktion $f : \bR^n \supset \omega \to \bR^m$ ist die eindimensionale Ableitung entlang einer der Koordinatenachsen:
	\begin{align*}
		\partial_j f(x) &= \left(\frac{d}{dh} f(x + he_j)\right)(0)\\
		&= \lim_{h \to 0} \frac{f(x + he_j) - f(x)}{h}
	\end{align*}
\end{definition}
\section{Ableitungsregeln}
\subsection{Kettenregel}
\begin{theorem}
	Es gilt
	\begin{align*}
		D(g \circ f)(x_0) = Dg(f(x_0))Df(x_0)
	\end{align*}
\end{theorem}
\subsection{Produktregel}
\subsection{Quotientenregel}
\subsection{Ableitung von Umkehrfunktionen}
\begin{theorem}
	Sei $I \in \bR$ ein mehrpunktiges Intervall und $f : I \to \bR$ streng monoton, stetig auf $I$ und differenzierbar bei $p$. So ist die Umkehrfunktion $f^{-1} : f(I) \to \bR$ differenzierbar bei $q = f(p)$ mit:
	\begin{align*}
		(f^{-1})'(q) = \frac{1}{f'(f^{-1}(q))}
	\end{align*}
\end{theorem}
\begin{proof}
	Kettenregel:
	\begin{align*}
		f(f^{-1}(q)) &= q\\
		\implies (f(f^{-1}(q)))' &= 1\\
		\implies (f^{-1})'(q) \cdot f'(f^{-1}(q)) &= 1\\
		\implies (f^{-1})'(q) &= \frac{1}{f'(f^{-1}(q))}
	\end{align*}
\end{proof}
\section{Der Mittelwertsatz}
\begin{theorem}
	Nimmt eine Funktion $f : (a,b) \to \bR$ an einem Punkt $p$ ein Maximum oder Minimum an, so gilt $f'(p) = 0$.
\end{theorem}
\begin{theorem}
	\emph{\textbf{Satz von Rolle:}} Sei $f : [a,b] \to \bR$ stetig auf dem kompakten Intervall $[a,b]$ und differenzierbar auf dem offenen Intervall $(a,b)$. Gilt dann $f(a) = f(b)$, so gibt es $p \in (a,b)$ mit $f'(p) = 0$.
\end{theorem}
\begin{proof}
	Im Abschnitt über Stetigkeit haben wir bewiesen, dass es Punkte $p,q \in [a,b]$ gibt, an denen $f$ sein Maximum und Minimum annimmt. Ist einer dieser Punkte in $(a,b)$ folgt der Satz trivial. Liegen die Punkte $p$ und $q$ hingegen am Rand, so folgt aus $f(a) = f(b)$, dass $p = q$ ist, und die Funktion somit konstant.
\end{proof}
\begin{theorem}
	\emph{\textbf{Mittelwertsatz:}} Sei $f : [a,b] \to \bR$ stetig auf dem kompakten Intervall $[a,b]$ und differenzierbar auf dem offenen Intervall $(a,b)$. So existiert ein $p \in (a,b)$, sodass
	\begin{align*}
		f'(p) = \frac{f(b) - f(a)}{b - a}
	\end{align*}
\end{theorem}
\begin{proof}
	Man wende den Satz von Rolle auf die Funktion
	\begin{align*}
		g : [a,b] &\to \bR\\
		x &\mapsto f(x) - f(a) - (x - a)\frac{f(b) - f(a)}{b - a}
	\end{align*}
	an.
\end{proof}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Integration}
\section{Treppen- und Regelfunktionen}
\begin{definition}
	Eine Funktion $f \in B([a,b])$ (eine beschränkte Funktion mit Definitionsbereich $[a,b]$) heißt \textbf{Treppenfunktion}, falls es Punkte $a = x_0 < x_1 < \hdots < x_n = b$ gibt, sodass $f$ auf jedem offenen Teilintervall $(x_k, x_{k+1})$ konstant ist. Wir nennen dann $(x_0, \hdots, x_n)$ eine \tbf{zu $f$ gehörige Unterteilung von $[a,b]$}. Die Menge aller Treppenfunktionen auf dem Intervall $[a,b]$ schreiben wir $T([a,b])$.
\end{definition}
\begin{definition}
	Eine Funktion $f : [a,b] \to \bR$ heißt \textbf{Regelfunktion}, falls es eine Folge $f_n$ von Treppenfunktionen gibt, welche Gleichmäßig gegen $f$ konvergiert. Die Menge aller Treppenfunktionen auf dem Intervall $[a,b]$ schreiben wir $R([a,b])$. 
\end{definition}
\begin{theorem}
	Sei $f_n$ eine Funktionenfolge, welche gleichmäßig gegen eine Funktion $f$ konvergiert. Hat jedes $f_n$ überall einseitige Grenzwerte, so auch $f$.
\end{theorem}
\begin{theorem}
	Eine Funktion $f : [a,b] \to \bR$ ist genau dann eine Regelfunktion, wenn für alle $c \in [a,b]$ die einseitigen Grenzwerte
	\begin{align*}
		\liminf_{x \to c} f(x) \text{ und } \limsup_{x \to c} f(x)
	\end{align*}
	existieren.
\end{theorem}
\begin{corollary}
	\label{corollary:stetig->regel}
	Jede stetige Funktion ist eine Regelfunktion.
\end{corollary}
\section{Integration von Treppenfunktionen}
\begin{definition}
	Sei $f$ eine Regelfunktion. Sei eine Unterteilung $x_0 < \hdots < x_n$ gegeben, sodass $f$ auf dem offenen Intervall $(x_i, x_{i+1})$ den Wert $c_i$ annimmt. Wir nennen die Zahl
	\begin{align*}
		I(f) := \sum_{i=0}^n c_i(x_{i+1} - x_i)
	\end{align*}
	Das \textbf{Integral} von $f$ über $[a,b]$ und schreiben auch $I(f) = \int_a^b f(x) dx = \int_a^b f$
\end{definition}
\begin{lemma}
	Der Wert von $I(f)$ ist unabhängig von der Wahl der Unterteilung.
\end{lemma}
\noindent Betrachte hierfür die gemeinsame Verfeinerung, bei der alle Zwischenpunkte aus zwei verschiedenen Unterteilungen vorkommen.
\clearpage
\begin{theorem} Die Abbildung $I(f)$ ist ein \textbf{lineares Funktional}, es gilt also:
	\label{theorem:treppenintegrallinearesfunktional}
	\begin{enumerate}
		\item $\displaystyle \forall f,g \in T([a,b]) : \int_a^b f + g = \int_a^b f + \int_a^b g$
		\item $\displaystyle \forall f \in T([a,b]) : \forall \lambda \in \bR : \int_a^b (\lambda f) = \lambda \int_a^b f$
	\end{enumerate}
	Die Abbildung ist außerdem Monoton, es gilt also:
	\begin{enumerate}
		\item[3.] $\displaystyle \forall f,g \in T([a,b]) : \left(\forall x : f(x) \leq g(x) \implies \int_a^b f \leq \int_a^b g\right)$
	\end{enumerate}
	Ferner gelten folgende obere Schranken für den Betrag der Abbildung:
	\begin{enumerate}
		\item[4.] $\displaystyle \forall f,g \in T([a,b]) : \abs{\int_a^b f} \leq \int_a^b \abs{f} \leq (b-a)\norm{f}$
	\end{enumerate}
\end{theorem}
\section{Integration von Regelfunktionen}
\begin{definition}
Sei $f$ eine Regelfunktion auf $[a,b]$ und sei $(t_n)_{n \in \bN}$ eine Folge von Treppenfunktionen, welche gleichmäßig gegen $f$ konvergiert. Wir definieren das Integral von $f$ als:
\begin{align*}
	\int_a^b f(x) dx = \lim_{n \to \infty} \int_a^b t_n(x) dx
\end{align*}
\end{definition}
\begin{theorem}
	Der gesuchte Grenzwert existiert für jede Regelfunktion.
\end{theorem}
\begin{proof}
	Sei $\epsilon \in \bR^+$. Per Definition konvergiert die Folge $(t_n)_{n \in \bN}$ gleichmäßig gegen $f$, also existiert ein $n_0 \in \bN$, sodass
	\begin{align*}
		\forall n \geq n_0 : \norm{t_n - f} < \frac{\epsilon}{2(b-a)}
	\end{align*}
	Für $m, n \geq n_0$ gilt nun unter Anwendung der Dreiecksungleichung:
	\begin{align*}
		\norm{t_m - t_n} \leq \norm{t_m - f} + \norm{t_n - f} < \frac{\epsilon}{2(b-a)} + \frac{\epsilon}{2(b-a)} = \frac{\epsilon}{(b-a)}
	\end{align*}
	Gemäß \ref{theorem:treppenintegrallinearesfunktional} folgt:
	\begin{align*}
		\abs{\int_a^b t_m - \int_a^b t_n} 
		&= \abs{\int_a^b (t_m - t_n)} 
			& (\ref{theorem:treppenintegrallinearesfunktional}.1)\\
		&\leq (b-a)\norm{t_m - t_n} 
			& (\ref{theorem:treppenintegrallinearesfunktional}.4)\\ 
		&< (b-a)\frac{\epsilon}{(b-a)}\\
		&= \epsilon
	\end{align*} 
	Also ist 
	\begin{align*}
		I_t := \left(\int_a^b t_n(x) dx\right)_{n \in \bN}
	\end{align*}
	eine Cauchyfolge reeller Zahlen, also konvergent.
\end{proof}
\begin{theorem}
	Das Integral ist unabhängig von der Wahl der Folge $(t_n)_{n \in \bN}$ der Treppenfunktionen.
\end{theorem}
\begin{proof}
	Sei $(u_n)_{n \in \bN}$ eine weitere Folge in $T([a,b])$, welche gleichmäßig gegen $f$ konvergiert. Sei $\epsilon \in \bR^+$. Durch die gleichmäßige Konvergenz gegen $f$ existieren $n_1, n_2 \in \bN$, sodass:
	\begin{align*}
		\forall n \geq n_1 : \norm{f - t_n} < \frac{\epsilon}{2(a-b)}
	\end{align*}
	und 
	\begin{align*}
		\forall n \geq n_2 : \norm{f - u_n} < \frac{\epsilon}{2(a-b)}
	\end{align*}
	Für $n_0 = \max{n_1, n_2}$ gilt also
	\begin{align*}
		\forall n \geq n_0 : \norm{t_n - u_n} \leq \norm{t_n - f} + \norm{u_n - f} < \frac{\epsilon}{b-a}
	\end{align*}
	Analog zum vorherigen Beweis folgt durch \ref{theorem:treppenintegrallinearesfunktional}
	\begin{align*}
		\abs{\int_a^b t_n - \int_a^b u_n} \leq (a,b)\norm{t_n - u_n} < \epsilon
	\end{align*}
	Also konvergiert die Folge $\displaystyle \left(\int_a^b t_n - \int_a^b u_n \right)_{n \in \bN}$ gegen Null, also ist
	\begin{align*}
		\lim_{n \to \infty} \int_a^b t_n = \lim_{n \to \infty}\int_a^b u_n
	\end{align*}
\end{proof}
\begin{theorem}
	Genau wie im Fall der Treppenfunktionen ist das Integral einer Regelfunktion ebenfalls ein beschränktes monotones lineares Funktional.
\end{theorem}
\begin{anmerkung}
	Es gilt desweiteren:
	\begin{enumerate}
		\item $\displaystyle \int_a^b 1 = b - a$
		\item $\displaystyle \forall f \in T([a,b]) : \forall z \in [a,b] : \int_a^b f = \int_a^z f + \int_z^b f$
	\end{enumerate}
	Gemeinsam mit Linearität und Monotonizität, welche im Vorherigen Satz bewiesen wurden, definieren diese Vorschriften die Integralabbildung bereits eindeutig.
\end{anmerkung}
\begin{theorem}
	Sei $(f_n)_{n \in \bN}$ eine Folge von Regelfunktionen, welche gleichmäßig gegen eine Funktion $f$ konvergieren. Dann ist $f$ eine Regelfunktion und es gilt
	\begin{align*}
		\int_a^b f = \lim_{n \to \infty} \int_a^b f_n
	\end{align*}
\end{theorem}
\begin{theorem}
	\emph{\textbf{Mittelwertsatz der Integralrechnung:}} Seien $f,g \in R([a,b])$, sei $f$ stetig und $g(x) \geq 0$. Dann existiert ein $c \in [a,b]$, sodass:
	\begin{align*}
		\int_a^b fg = f(c) \int_a^b g
	\end{align*}
\end{theorem}
\begin{proof}
	Nach dem Satz von Weierstrass (Extremwertsatz) nimmt die stetige Funktion $f$ auf dem Intervall $[a,b]$ ein Minimum $m$ und ein Maximum $M$ an. Aus $m \leq f(x) \leq M$ und $g(x) \geq 0$ folgt $mg(x) \leq f(x)g(x) \leq Mg(x)$, also auch:
	\begin{align*}
		m \int_a^b g(x) dx \leq \int_a^b f(x)g(x) dx \leq M \int_a^b g(x) dx
	\end{align*}
	Also ist $\displaystyle \int_a^b f(x)g(x) dx = \alpha \int_a^b g(x) dx$ für eine Zahl $\alpha \in [m,M]$. Da $\alpha$ also zwischen den Extremwerten von $f$ liegt gibt es gemäß Zwischenwertsatz eine Zahl $c \in [a,b]$ mit $f(c) = \alpha$.
\end{proof}
\begin{corollary}
 	Sei $f \in R([a,b])$ und sei $f$ stetig Dann existiert ein $c \in [a,b]$, sodass:
 	\begin{align*}
 		\int_a^b f = f(c) (a-b)
 	\end{align*}
\end{corollary}
\begin{proof}
	Dies ist der vorherige Satz im Fall $g(x) = 1$.
\end{proof}
\section{Hauptsatz der Integralrechnung}
\begin{definition}
	Sei $f : [a,b] \to \bR$. Eine Funktion $F : [a,b] \to \bR$ heißt \textbf{Stammfunktion} von $f$, falls \begin{align*}
		\frac{d}{dx} F(x) = f(x)
	\end{align*}
\end{definition}
\begin{lemma}
	Seien $F$ und $G$ Stammfunktionen einer Funktion $f$. So gilt $\exists c \in \bR : G(x) = F(x) + c$.
\end{lemma}
\begin{proof}
	Es gilt $F' = G' = f$, also
	\begin{align*}
		(F - G)' = F' - G' = 0.
	\end{align*}
	Somit ist die Funktion $F - G$ konstant.
\end{proof}
\begin{theorem}
	\emph{\textbf{Hauptsatz der Integralrechnung:}} Sei $f : [a,b] \to \bR$ eine stetige Funktion, also insbesondere eine Regelfunktion. Definiere die Funktion
	\begin{align*}
		F(x) := \int_a^x f(t) dt
	\end{align*}
	So ist $F$ eine Stammfunktion von $f$.
	(Achtung: $a$ ist keine beliebige Konstante, sondern der kleinste Punkt im Definitionsbereich!)
\end{theorem}
\begin{proof}
	\textbf{Beweis nach Růžička:} Sei $x \in [a,b]$. Sei $h \in \bR$ mit $x + h \in [a,b]$. Dann gilt:
	\begin{align*}
		\abs{F(x+h) - F(x) - f(x)h} &= \abs{\int_a^{x+h}f(t)dt - \int_a^{x}f(t)dt - f(x)h}\\
		&= \abs{\int_x^{x+h}f(t)dt - f(x)h}\\
		&= \abs{\int_x^{x+h}f(t)dt - \int_x^{x+h} f(x) dt}\\
		&= \abs{\int_x^{x+h}(f(t) - f(x)) dt}\\
		&\leq \int_x^{x+h}\abs{f(t) - f(x)} dt &\ref{theorem:treppenintegrallinearesfunktional}\\
	\end{align*}
	Sei nun $\epsilon \in \bR^+$. Da $f$ stetig und auf dem Kompaktum $[a,b]$ definiert ist, ist $f$ gleichmäßig stetig. Also existiert ein $\delta \in \bR^+$, sodass
	\begin{align*}
		\forall x,t \in [a,b] : \abs{t - x} < \delta \implies \abs{f(t) - f(x)} < \epsilon.
	\end{align*}
	Ist nun $\abs{t-x} \leq \delta$, also $\abs{h} \leq \delta$, folgt
	\begin{align*}
		\int_x^{x+h}\abs{f(t) - f(x)} dt \leq \int_x^{x+h}\epsilon dt = \abs{h}\epsilon
	\end{align*}
	Also:
	\begin{align*}
		&\abs{F(x+h) - F(x) - f(x)h} \leq \abs{h}\epsilon\\
		\implies &\lim_{h \to 0} \abs{F(x+h) - F(x) - f(x)h} = 0\\
		\implies &\lim_{h \to 0} F(x+h) - F(x) - f(x)h = 0\\
		\implies &\lim_{h \to 0} \frac{F(x+h) - F(x)}{h} - f(x) = 0\\
		\implies &\lim_{h \to 0} \frac{F(x+h) - F(x)}{h} = f(x) \\
	\end{align*}
	Also ist $f$ die Ableitung von $F$.
\end{proof}
\begin{proof}
	\textbf{Beweis per Mittelwertsatz:} (Dieser Beweis ist übersichtlicher, aber nur, weil die ganze Arbeit an den Mittelwertsatz und den Satz von Rolle abgegeben wurden. Vermutlich also für die mündliche Prüfung also weniger gut geeignet.)
	Wir betrachten die Ableitung von $F$. Es gilt:
	\begin{align*}
		\frac{F(x+h) - F(x)}{h} &= \frac{1}{h} \left(\int_a^{x+h} f(t) dt - \int_a^{x} f(t) dt\right)\\
							    &= \frac{1}{h} \int_x^{x+h} f(t) dt\\ 
	\end{align*}
	Nach dem Mittelwertsatz der Integralrechnung existiert nun ein $c_h \in [x,x+h]$, sodass 
	\begin{align*}
		\int_x^{x+h} f(t) dt = (x + h - x) f(c) \implies \frac{1}{h} \int_x^{x+h} f(t) dt = f(c)
	\end{align*}
	Da $c \in [x,x+h]$ folgt $\displaystyle \lim_{h \to 0} c_h = x$, da $f$ stetig ist folgt $\displaystyle \lim_{h \to 0} f(c_h) = f(x)$. Also gilt:
	\begin{align*}
		\lim_{h \to 0} \frac{F(x+h) - F(x)}{h} = \lim_{h \to 0} f(c_h) = f(x)
	\end{align*}
\end{proof}
\subsection{Partielle Integration}
\begin{theorem}
	Es gilt
	\begin{align*}
		\int_a^b f'(x) g(x) dx = \left[f(x) g(x)\right]_a^b - \int_a^b f(x) g'(x) dx
	\end{align*}
\end{theorem}
\begin{proof}
Folgt direkt aus dem Hauptsatz der Integralrechnung und der Produktregel:
	\begin{align*}
		(f(x)g(x))' &= f'(x)g(x) + f(x)g'(x)\\
		\implies \int_a^b (f(x)g(x))' dx &= \int_a^b \left(f'(x)g(x) + f(x)g'(x)\right) dx\\
		\implies [f(x)g(x)]_a^b &= \int_a^b f'(x)g(x) dx + \int_a^b f(x)g'(x) dx\\
		\implies \int_a^b f'(x)g(x) dx &=  [f(x)g(x)]_a^b - \int_a^b f(x)g'(x) dx\\
	\end{align*}
\end{proof}
\begin{anmerkung}
	Ich persönlich finde folgende äquivalente Schreibweise einfacher anzuwenden:
	\begin{align*}
		\int_a^b f(x) g(x) dx = \left[F(x) g(x)\right]_a^b - \int_a^b F(x) g'(x) dx
	\end{align*}
\end{anmerkung}
\begin{beispiel}
	Ein sehr häufig anwendbarer Trick ist die partielle Integration einer Funktion durch Multiplikation mit der konstanten Einsfunktion, z.B:
	\begin{align*}
		\int \ln(x) dx &= \int 1 \cdot \ln(x) dx\\
					   &= x \ln(x) - \int x \cdot \frac{1}{x} dx\\ 
					   &= x \ln(x) - \int 1 dx\\
					   &= x \ln(x) - x\\ 
	\end{align*}
\end{beispiel}
\begin{beispiel}
	Im Fall $f = g$ erhält man durch partielle Integration das gleiche Integral ein zweites Mal und kann daraufhin die Gleichung durch Umstellen lösen:
	\begin{align*}
		\int f'(x) f(x) dx &= f(x)f(x) - \int f(x) f'(x) dx\\
		\implies 2 \int f'(x) f(x) dx &= f(x)^2\\
		\implies \int f'(x) f(x) dx &= \frac{1}{2} f(x)^2
	\end{align*}
	Diese Formel hat zahlreiche direkte Anwendungen:
	\begin{itemize}
		\item $\displaystyle \int \frac{\ln x}{x} dx = \frac{1}{2} \ln(x)^2$
		\item $\displaystyle \int \sin(x)\cos(x) dx = \frac{1}{2} \sin(x)^2 = -\frac{1}{2} \cos(x)^2$
	\end{itemize}
\end{beispiel}
\subsection{Integration durch Substitution}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Diffeomorphismen}
In diesem Kapitel geht es um die lokale Lösbarkeit von nichtlinearen Gleichungen. 
\newpar
Sei $\Omega \in \bR^n$ offen und $f \in \bC^1(\Omega, \bR^m)$. Sei $y \in \bR^m$ gegeben. Uns interessieren nun Lösungen der Gleichung $f(x) = y$. Intuitiv ist dann $m$ die Anzahl der Gleichungen und $n$ die Anzahl der Unbekannten.
\newpar
Angenommen, wir haben bereits eine Lösung $x_0$ der Gleichung $f(x) = y_0$. Uns interessiert nun:
\begin{enumerate}
	\item Hat die Gleichung $f(x) = y$ auch für andere Werte von $y$ nahe an $y_0$ Lösungen? Falls ja, sind diese nahe an $x_0$?
	\item Ist $x_0$ die einzige Lösung von $f(x) = y_0$ in einer Umgebung von $x_0$?
	\item Falls nein, wie sieht die Lösungsmenge $f^{-1}(y_0)$ nahe bei $x_0$ aus?
\end{enumerate}
Falls $f$ affin ist, gilt $f(x) = y \Leftrightarrow A(x - x_0) = y - y_0$, und die Lineare Algebra gibt uns folgende Antworten:
\begin{enumerate}
	\item Es gibt genau dann eine Lösung für alle $y \in \bR^m$, wenn 
	\begin{align*}
		\rang A = m.
	\end{align*}
	\item Es gibt höchstens eine Lösung $x \in \bR^n$, wenn 
	\begin{align*}
		\ker A = \{0\}.
	\end{align*} Dies ist äquivalent zu 
	\begin{align*}
		\rang A = n.
	\end{align*}
	\item $f^{-1}\{y_0\}$ ist ein affiner Unterraum des $\bR^n$ mit Dimension $n - \rang A$.
\end{enumerate}
Da das Differential $Df(x_0)$ einer Abbildung $f \in C^1(\Omega, \bR^m)$ an einem gegebenen Punkt $x_0 \in \Omega$ eine lineare Abbildung ist, hoffen wir nun, einige dieser Erkenntnisse über lineare Funktionen auf die allgemeinere Klasse der differenzierbaren Funktionen übertragen zu können.
\newpar
Wir wollen hierfür die Funktion durch das Differential linear approximieren. Das Restglied für eine solche Approximation mit Abstand $\xi \in \bR^n$ ist dann genau die Differenz zwischen dem Tatsächlichen Wert und dem approximierten Wert, also:
\begin{align*}
	R_f(\xi) = f(x_0 + \xi) - (f(x_0) + Df(x_0)\xi)
\end{align*}
Für unsere Gleichung gilt nun:
\begin{align*}
	f(x) = y \Leftrightarrow Df(x_0) \cdot (x - x_0) + R_f(\xi) \cdot (x - x_0) = y - y_0
\end{align*}
Es wurde also einfach ein Restglied zur Gleichung für affine Funktionen hinzugefügt. Es hilft, im Kopf zu behalten, dass der affine Fall genau der Fall ist, in dem die Approximation exakt und somit das Restglied $0$ ist. 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Implizite Funktionen}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\chapter{Anfangswertprobleme und Differentialgleichungen}
\section{Anfangswertprobleme}
\subsection{Natürliches Wachstum}
Wir sprechen von \tbf{natürlichem Wachstum} wenn die Wachstums- oder Zerfallgeschwindigkeit proportional zum Wert der Funktion ist, also:
\begin{align*}
	x(t_0) = x_0, \qquad x' = \alpha x
\end{align*}
Die Lösung ist
\begin{align*}
	x(t) &= x_0 e^{\alpha(t - t_0)}
\end{align*}	
was oft vereinfacht geschrieben wird als:
\begin{align*}
	x(t) &= x_0 e^{\alpha(t - t_0)}\\
	     &= \left(\frac{x_0}{e^{\alpha t_0}}\right) e^{\alpha t}\\
	     &:= ce^{\alpha t}
\end{align*}
Ntürliches Wachstum tritt z.B. beim radioaktiven Zerfall oder in der Zinsrechnung auf.
\subsection{Logistisches Wachstum}
Beim Logistischen Wachstum wird eine zusätzliche "Sterberate" hinzugefügt:
\begin{align*}
	x(t_0) = x_0, \qquad x' = \alpha x - \beta x^2
\end{align*}
Die Lösung hier ist bereits deutlich komplizierter:
\begin{align*}
	x(t) = \frac{1}{\frac{\beta}{\alpha} + \left( \frac{1}{x_0} - \frac{\beta}{\alpha}\right) e^{-\alpha(t - t_0)}}
\end{align*}
Durch den Faktor $e^{-\alpha(t - t_0)}$ konvergiert die Lösung für $t \to \infty$ gegen $\frac{\alpha}{\beta}$.
\subsection{Lotka-Volterra-Modell}
Ein bekanntest Modell für Systeme von Raub- und Beutetieren ist das Modell von Lotka-Volterra. Wir betrachten eine Population $x(t)$ an Beutetieren und $y(t)$ von Raubtieren, sodass bei einer zu großen Raubtierpopulation die Wachstumsrate der Beutetierpopulation sinkt und bei einer zu kleinen Beutetierpopulation die Raubtierpopulation sinkt:
\begin{align*}
	x(t_0) &= x_0, \qquad x' = (\alpha - \beta y)x\\
	y(t_0) &= y_0, \qquad y' = (-\gamma + \delta x)y
\end{align*}
Eine besonders simple Lösung ist $x_0 = \frac{\delta}{\gamma}$, $y_0 = \frac{\alpha}{\beta}$ - in diesem Fall bleiben beide Populationen konstant.
\newpar
Im allgemeinen sind die Lösungen dieses Modells sind periodisch, eine allgemeine Lösungsformel lässt sich aber bereits nicht mehr analytisch durch Elementarfunktionen darstellen. Immerhin sind sie ohne größere Probleme sehr genau numerisch approximierbar.
\newpar
Wir wollen nun den Begriff des Anfangswertproblems formalisieren und unsere Lösungsmethoden verallgemeinern.
\begin{definition}
	Sei $G \subset \bR \times \bR^n$ offen und $f : G \to \bR^n$ stetig (in kürzerer Notation: $f \in C^0(G, \bR^n)$
	\newpar
	Eine stetig differenzierbare Funktion $x : I \to \bR^n$ (kurz: $x \in C^1(I, \bR^n)$) ist eine \tbf{Lösung der Differentialgleichung} $x' = f(\cdot, x)$, falls
	\begin{align*}
		\forall t \in I : x'(t) = f(t, x(t))
	\end{align*}
	Gilt außerdem $x(t_0) = x_0$, so ist $x$ eine \tbf{Lösung des dazugehörigen Anfangswertproblems}.
\end{definition}
\begin{definition}
	Falls die Funktion $f$ zeitunabhängig ist (also unabhängig von ihrer ersten Komponente) nennen wir die zugehörige Differentialgleichung \tbf{autonom}.
\end{definition}
\newpar
Die drei zentralen Fragen sind nun:
\begin{enumerate}
	\item Existert eine Lösung des Anfangswertproblems?
	\item Falls eine Lösung existiert, ist sie eindeutig?
	\item Wie hängt die Lösung von $x_0$ und $f$ ab?
\end{enumerate}
Die dritte Frage sprengt leider den Rahmen einer Grundlagenvorlesung Analysis. Die ersten beiden Fragen können wir jedoch bald befriedigend beantworten.
\newpar
Es stellt sich zum Beispiel heraus, dass selbst bei simplen Anfangswertproblemen die Stetigkeit von $f$ nicht ausreicht, um die Eindeutigkeit der Lösungsmenge zu gewährleisten:
\begin{beispiel}
	Sei $f(t,x) = 2\sqrt{\abs{x}}$. Dann hat das Anfangsproblem
	\begin{align*}
		x(0) &= 0\\
		x' &= f(\cdot, x)
	\end{align*}
	unendlich viele Lösungen in $\bC^1(\bR, \bR)$, nämlich:
	\begin{align*}
		x_{\alpha, \beta}(t) = 
		\begin{cases}
			-(t - \alpha)^2 & t < \alpha\\
			0 & t \in [\alpha, \beta]\\
			(t - \beta)^2 & t > \beta\\
		\end{cases}
	\end{align*}
	für beliebige $\alpha \in \bR^-, \beta \in \bR^+$.
\end{beispiel}
\begin{theorem}
	Hiii!!! 'w'
\end{theorem}
\begin{lemma}
	Sei $f \in \bC^0(G, \bR^n)$ und $(t_0, x_0) \in G$. 
	\newpar
	Dann sind die folgenden Aussagen quivalent:
	\begin{enumerate}
		\item $x \in \bC^1(I, \bR^n)$ ist eine Lösung des Anfangswertproblems
		\begin{align*}
			x(t_0) &= x_0,\\ \forall t \in I : x'(t) &= f(t,x(t)),
		\end{align*}
		\item $x \in \bC^0(I, \bR^n)$ erfüllt die Gleichung
		\begin{align*}
			\forall t \in I : x(t) = x_0 + \int_{t_0}^t f(s, x(s)) ds.
		\end{align*}
	\end{enumerate} 
\end{lemma}
\noindent 
Das folgende Beispiel zeigt, dass wir im Allgemeinen nur eine zeitlich lokale Lösung erwarten können:
\begin{beispiel}
	Das Anfangswertproblem 
	\begin{align*}
		x(0) &= 1,\\
		x' &= x^2 
	\end{align*}
	Hat auf $(-\infty, 1)$ die Lösung $x(t) = \frac{1}{1-t}$. Diese Lösung hat jedoch bei $t = 1$ eine Singularität und ist somit nicht fortsetzbar.
\end{beispiel}
\begin{theorem}
	\emph{\tbf{Kurzzeitexistenzsatz von Picard-Lindelöf:}} \\\noindent Sei $f \in \bC^0(G, \bR^n)$ mit $D_x f \in \bC^0(G, \bR^{n \times n})$. Sei $(t_0, x_0) \in G$.
	\newpar
	Dann existiert ein $\delta > 0$, sodass das Anfangswertproblem
	\begin{align*}
		x(t_0) &= x_0,\\
		\forall t \in [t_0-\delta, t_0 + \delta] : x'(t) &= f(t, x(t))
	\end{align*}
	eine eindeutige Lösung besitzt.
\end{theorem}
\begin{proof}
	Banachscher Fixpunktsatz :)
\end{proof}
%
%
%
%
%
%
%
%
\appendix
\chapter{Sammlung von Reihen und ihren Grenzwerten}
\section{Geometrische Summe}
	\begin{align*}
		\sum_{k = 0}^n q^k = \frac{1 - q^{n+1}}{1 - q}
	\end{align*}
	Also für $|q| < 1$:
	\begin{align*}
		\sum_{k = 0}^\infty q^k = \frac{1}{1 - q}
	\end{align*}
\section{Standardbeispiel für Teleskopsummen}
	\begin{align*}
		\sum_{k = 1}^\infty \frac{1}{k(k+1)} &= \lim_{n \to \infty} \sum_{k = 1}^n\frac{1}{k(k+1)}\\
		&= \lim_{n \to \infty }\sum_{k = 1}^n \left(\frac{1}{k} - \frac{1}{k+1}\right) ^*\\
		&= \lim_{n \to \infty } 1 - \frac{1}{n + 1}\\
		&= 1
	\end{align*}
	$^*$ Die Umformung der Brüche funktioniert folgendermaßen:
	\begin{align*}
		\frac{1}{k(k+1)} = \frac{k+1}{k(k+1)} - \frac{k}{k(k+1)} = \frac{1}{k} - \frac{1}{k+1}
	\end{align*}
\section{Wichtige Taylorreihen}
\subsection{Exponentialfunktion}
\begin{align*}
	e^x = \sum_{k = 0}^\infty \frac{x^k}{k!} \quad \left( = \lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n \right)
\end{align*}
\subsection{Sinus und Kosinus}
\begin{align*}
	\cos(x) &= \sum_{k=0}^\infty \frac{(-1)^k}{(2k)!}x^{2k}\\
	\sin(x) &= \sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)!}x^{2k+1}\\
\end{align*}
Aus diesen Reihen folgt direkt die Eulersche Formel $e^{ix} = \cos(x) + i\sin(x)$:
\begin{align*}
	e^{ix} &= \sum_{k = 0}^\infty \frac{(ix)^k}{k!}\\
		   &= \sum_{k = 0}^\infty \frac{(ix)^{2k}}{(2k)!} + \sum_{k = 0}^\infty \frac{(ix)^{2k+1}}{(2k+1)!}\\
		   &= \sum_{k = 0}^\infty \frac{i^{2k}}{(2k)!}x^{2k} + \sum_{k = 0}^\infty \frac{i^{2k+1}}{(2k+1)!}x^{2k+1}\\
		   &= \sum_{k = 0}^\infty \frac{i^{2k}}{(2k)!}x^{2k} + i \sum_{k = 0}^\infty  \frac{i^{2k}}{(2k+1)!}x^{2k+1}\\
		   &= \sum_{k = 0}^\infty \frac{(i^2)^k}{(2k)!}x^{2k} + i \sum_{k = 0}^\infty \frac{(i^2)^k}{(2k+1)!}x^{2k+1}\\
		   &= \sum_{k = 0}^\infty \frac{(-1)^k}{(2k)!}x^{2k} + i \sum_{k = 0}^\infty \frac{(-1)^k}{(2k+1)!}x^{2k+1}\\
		   &= \cos(x) + i \sin(x)
\end{align*}
\subsection{Logarithmus}
Da $\ln(x)$ eine Singularität am Punkt $x = 0$ hat, Entwickeln wir stattdessen am Punkt $x = 1$ und erhalten:
\begin{align*}
	\ln(1+x) = \sum_{k=1}^\infty (-1)^{k+1} \frac{x^k}{k}
\end{align*}
\chapter{Sammlung von Stammfunktionen}
\section{Inverse Trigonometrie}
\begin{align*}
	\arcsin(x) &= \int \frac{1}{\sqrt{1-x^2}} dx\\
	\arccos(x) &= - \int \frac{1}{\sqrt{1-x^2}} dx\\
	\arctan(x) &= \int \frac{1}{1+x^2} dx\\
\end{align*}
Für die Herleitung sind die Ableitungsformel für Umkehrfunktionen und die Identität 
\begin{align*}
	\sin(x)^2 &+ \cos(x)^2 = 1\\
	 \implies \sin(x) = \sqrt{1 - \cos(x)^2}&, \qquad \cos(x) = \sqrt{1 - \sin(x)^2}
\end{align*} nötig.

\section{Hyperbolische Trigonometrie}
\begin{align*}
	\sinh(x) &= \frac{e^x - e^{-x}}{2}\\
	\cosh(x) &= \frac{e^x + e^{-x}}{2}\\
	\tanh(x) &= \frac{\sinh(x)}{\cosh(x)}\\
\end{align*}
\end{document}