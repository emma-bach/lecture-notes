\documentclass{report}

% custom margins
\usepackage[a4paper,margin=1.5in]{geometry}
\renewcommand{\baselinestretch}{1.2}

% emma's long list of custom macros and universally used packages
\include{../macros-and-packages.tex}

% fancy bar next to proofs
\tcolorboxenvironment{proof}{
	colback=white,
	boxrule=0pt,
	leftrule=0.5mm,
	before skip=0.75cm,
	after skip=0.75cm,
	sharp corners,
	breakable,
	enhanced,
}

\usepackage{algorithm}
\usepackage{algpseudocode}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
	\include{title}
	\tableofcontents
	\thispagestyle{fancy}
	\chapter{Aufgabenstellung}
	In der Numerik beschäftigt man sich mit der praktischen Berechnung von Lösungen mathematischer Probleme.
	\begin{example}
		Berechne $\displaystyle \int_0^1 e^{-x^2} dx$!
	\end{example}
	\begin{example}
		Berechne $\sin(20)$!
	\end{example}
	\begin{example}
		Berechne $\sqrt{753}$!
	\end{example}
	\begin{example}
		Berechne $\displaystyle \min_{x \in [0,1]} F(x)$, für eine geeignete Funktion $F$!
	\end{example}
	\begin{example}
		Berechne $x$, sodass $f(x) = 0$!
	\end{example}
	\begin{example}
		Berechne $x \in \bR^n$, sodass $Ax = b$!
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe in der Numerik besteht im Finden einer Lösung von
		\begin{align*}
			F(x,d) = 0
		\end{align*}
		für gegebenes Datum $d$ und gegebene Funktion $F$.
	\end{definition}
	\noindent
	Typischerweise können in akzeptabler Zeit keine exakten Lösungen gefunden werden, sondern nur Approximationen. Insbesondere stehen statt den vollen Mengen $\bQ$, $\bR$, $\bC$ etc. auch nur endlich viele \tbf{Maschinenzahlen} zur Verfügung - arbiträre reelle Zahlen benötigen unendlich viel Speicher! Rechenoperationen sind dementsprechend Fehlerbehaftet, es gibt Rundungsfehler. Außerdem gibt es in reellen Anwendungen oft \tbf{Modellfehler} und \tbf{Datenfehler}.
	\newpar
	Eine Grundlegende Idee in der Numerik ist es deshalb, eine gute Balance zwischen Exaktheit und Aufwand der Berechnung zu finden.
	\begin{example}
		Die Berechnung der Determinante einer Matrix mittels Laplaceschem Entwicklungssatz benötigt $O(n!)$ Rechenoperationen. Die Determinante mit diesem Verfahren zu berechnen, dauert sehr viel länger, als das Universum alt ist.
		\newpar
		Besser: Matrix (approximativ) auf Dreiecksgestalt bringen und die Diagonalelemente multiplizieren.
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe heißt \tbf{wohlgestellt}, wenn zu geeigneten Daten $d$ eindeutige Lösungen $x$ existieren, und diese stetig von $d$ abhängt. Andernfalls ergibt die Suche nach einer numerischen Lösung wenig Sinn. Für wohlgestellte Probleme existiert eine Lösungsfunktion $\phi$, sodass $x = \phi(d)$ das Problem löst, d.h. $f(\phi(d),d) = 0$.
	\end{definition}
	\begin{definition}
		Ein numerischer Algorithmus zur näherungsweisen Lösung einer wohlgestellten Aufgabe $\phi$ ist eine Abbildung $\tilde{\phi}$, die durch Hintereinanderausführung möglicherweise fehlerbehafteter elementarer Rechenoperationen definiert ist, also 
		\begin{align*}
			\tilde{\phi} = f_j \circ f_{j-1} \circ \hdots \circ f_1
		\end{align*}
	\end{definition}
	\begin{definition}
		Der \tbf{Aufwand} eines Verfahrens $\tilde \phi$ ist die Anzahl der benötigten elementaren Rechenschritte. Typischerweise interessiert uns nicht die exakte Anzahl an Schritten, sondern nur die Größenordnung.
	\end{definition}
	\begin{proposition}
		Das Gaußverfahren hat Aufwand $\cO(n^3)$.
	\end{proposition}
\chapter{Numerische Lineare Algebra}
\section{Matrixfaktorisierung}
\subsection{Dreiecksmatrizen}
\begin{definition}
	Eine Matrix $L \in \bR^{n \times n}$ heißt \tbf{untere Dreiecksmatrix}, falls $\forall i < j : l_{ij} = 0$.
\end{definition}
\begin{definition}
	Eine Matrix $U \in \bR^{n \times n}$ heißt \tbf{obere Dreiecksmatrix}, falls $U^\top$ eine untere Dreiecksmatrix ist.
\end{definition}
\begin{definition}
	Eine Dreiecksmatrix heißt \tbf{normalisiert}, falls alle ihre Diagonaleinträge $1$ sind.
\end{definition}
\begin{definition}
	Eine Matrix heißt \tbf{regulär}, wenn sie invertierbar ist.
\end{definition}
\begin{lemma}
	Die quadratischen oberen (bzw. unteren) Dreiecksmatrizen bilden unter Matrixmultiplikation eine Gruppe.
\end{lemma}
\noindent
Lineare Gleichungssysteme mit regulärer Dreiecksmatrix lassen sich leicht lösen. Sei $U \in \bR^{n \times n}$ eine reguläre obere Dreiecksmatrix und $b \in \bR^n$. Wir berechnen $x \in \bR^n$ folgendermaßen:
\begin{enumerate}
	\item for i = n : -1 : 1:
	\begin{enumerate}
		\item $\displaystyle x_i = \left(b_i - \sum_{j = i+1}^n u_{ij}x_j\right) \cdot \frac{1}{u_{ii}}$
	\end{enumerate}
	\item end.
\end{enumerate}
Der Aufwand dieses Verfahrens ist $\cO(n^2)$. Ein analoger Algorithmus existiert für untere Dreiecksmatrizen.
\subsection{$LU$-Zerlegung}
Falls für eine reguläre Matrix $A \in \bR^{n \times n}$ eine Zerlegung $A = LU$ in eine untere Dreiecksmatrix $U$ und eine obere Dreiecksmatrix $L$ gegeben ist, so lässt sich das lineare Gleichungssystem $Ax = b$ in zwei Schritten lösen:
\begin{enumerate}
	\item Löse $Ly = b$.
	\item Löse $Ux = y$.
\end{enumerate}
\begin{definition}
	Eine Faktorisierung $A = LU$ mit unterer Dreiecksmatrix $L \in \bR^{n \times n}$ und oberer Dreiecksmatrix $U \in \bR^{n \times n}$ heißt \tbf{$LU$-Zerlegung} von $A$. Die Zerlegung heißt \tbf{normalisiert}, falls $L$ normalisiert ist.
\end{definition}
\begin{theorem}
	Für jede reguläre Matrix $A \in \bR^{n \times n}$ sind äquivalent:
	\begin{enumerate}
		\item Es existiert eine eindeutige normalisierte $LU$-Zerlegung.
		\item Alle Untermatrizen $A_k = (a_{ij})_{(i,j) \in (1, \hdots, k)^2}$ sind regulär. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	\phantom{}
	\begin{itemize}
		\item[$\rightarrow$] Ist $A$ regulär, so sind auch $L$ und $U$ regulär. Damit sind von $L$ und $U$ alle Diagonaleinträge nicht null. Somit sind auch die Untermatrizen $L_k$ und $U_k$ regulär, somit auch die Untermatrizen $A_k = L_k U_k$.
		\item[$\leftarrow$] Für $n = 1$ ist die Aussage klar. Sei nun angenommen, die Aussage gelte für Matrizen der Größe $(n - 1) \times (n - 1)$. Damit existieren Matrizen $L_{n-1}, U_{n-1}$, sodass $A_{n-1} = L_{n-1}U_{n-1}$ eine normalisierte $LU$-Zerlegung ist. Seien nun $\binom{b}{a_{nn}}$ die letzte Spalte und $(c^\top, a_{nn})$ die letzte Zeile von $A$. Die Aussage ist bewiesen, wenn geeignete $l,u \in \bR^{n-1}$ und $r \in \bR$ existieren, sodass
		\begin{align*}
			\begin{pmatrix}
				A_{n-1} & b\\
				c^\top & a_{nn}
			\end{pmatrix}
			&=
			\begin{pmatrix}
				L_{n-1} & 0\\
				l^\top & 1
			\end{pmatrix}
			\begin{pmatrix}
				U_{n-1} & u\\
				0 & r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				L_{n-1}U_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				A_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}
		\end{align*}
		Wir brauchen also $L_{n-1}u = b$, $U_{n-1}^\top l = c$, und $a_{nn} = l^\top u + r$. Durch Regularität von $L_{n-1}$ und $U_{n-1}$ existieren eindeutige Lösungen $u, l$, der ersten beiden Gleichungen, somit ist auch $r$ festgelegt.
	\end{itemize}
\end{proof}
\begin{corollary}
	\phantom{}
	\begin{itemize}
		\item Jede positiv definite Matrix besitzt eine eindeutige $LU$-Zerlegung.
		\item Jede strikt diagonaldominante Matrix, also jede Matrix $A$ mit $\sum_{j \in 1, \hdots, n, i \neq j} \abs{a_ij} < \abs{a_{ii}}$ besitzt eine eindeutige $LU$-Zerlegung.
		\item Die Matrix $A = \begin{pmatrix}
			0 & 1\\
			1 & 0
		\end{pmatrix}$
		besitzt keine $LU$-Zerlegung.
		\item Die Nullmatrix besitzt zwar $LU$-Zerlegungen, diese sind aber nicht eindeutig.
	\end{itemize}
\end{corollary}
\begin{lemma}
	Falls $A = LU$ eine normalisierte $LU$-Zerlegung von $A$ ist, so gilt
	\begin{align*}
		a_{ik} = u_{ik} + \sum_{j = 1}^{i-1} l_{ij} u_{jk}
	\end{align*}
	und
	\begin{align*}
		a_{ki} = l_{ki} u_{ii} + \sum_{j = 1}^{i-1} l_{kj} u_{ji}
	\end{align*}
\end{lemma}
\begin{proof}
	Es gilt $l_{ij} = 0$ für $j > i$ und $l_{ii} = 1$. Es gilt außerdem $u_{ij} = 0$ für $j < i$. Die Formeln folgen direkt aus der Definition des Matrixprodukts.
\end{proof}
Diese Formeln lassen sich für $i \leq k$ nach $u_{ik}$ auflössen und für $k > i$ nach $l_{ki}$ auflösen. Wir erhalten folgenden Algorithmus:
\begin{algorithm}
\begin{algorithmic}[1]
	\For{$i = 1, i \leq n, i++$}
		\For{$k = i, k \leq n, k++$}
			\State $\displaystyle u_{ik} \gets a_{ik} - \sum_{j = 1}^{i-1}l_{ij}u_{jk}$
		\EndFor
		\For{$k = i+1, k \leq n, k++$}
		\State $\displaystyle l_{ki} \gets \frac{1}{u_{ii}} \cdot \left(a_{ki} - \sum_{j = 1}^{i-1}l_{kj}u_{ji}\right)$
		\EndFor
	\EndFor
\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Rechenauftrag dieses Algorithmus beträgt $O(n^3)$.
\end{proposition}
\begin{proposition}
	Es ist nicht mehr Speicher nötig, als sowieso für $A$ benötigt wird. Die Einträge von $A$ können im Speicher einfach sukzessiv durch die jeweiligen Einträge von $L$ bzw. $U$ ersetzt werden.
\end{proposition}
\begin{definition}
	Ein numerisches Problem $\phi$ heißt \tbf{schlecht Konditioniert}, wenn kleine Unterschiede in der Eingabe zu großen Unterschieden in der korrekten Lösung führen, also wenn
	\begin{align*}
		\frac{\abs{\phi(\tilde{x}) - \phi(x)}}{\abs{\phi(x)}} >> \frac{\abs{\tilde{x} - x}}{\abs{x}}
	\end{align*}
	Ansonsten heißt die Aufgabe \tbf{gut konditioniert}.
\end{definition}
\begin{definition}
	Ein Verfahren $\tilde{\phi}$ heißt \tbf{numerisch instabil}, wenn eine Störung $\tilde{x}$ existiert, sodass der durch Rundungsfehler verursachte relative Fehler erheblich größer ist als der rein durch die Störung verursachte Fehler.
\end{definition}
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\ 
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Für $\epsilon \in \bR^+$. Es gilt
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			1 + \frac{1}{\epsilon} & -\frac{1}{\epsilon}\\ 
			- \frac{1}{\epsilon} & \frac{1}{\epsilon}
		\end{pmatrix}
	\end{align*}
	\begin{align*}
		A^{-1} \binom{1}{1} = \binom{1}{0}. \qquad \qquad A^{-1} \binom{1}{1 + \epsilon} = \binom{0}{1}.
	\end{align*}
\end{example}
\noindent Eine kleine Störung in den Daten $b$ des linearen Gleichungssystems $Ax = b$ führt zu Problemen der Größenordnung $1$!!! So können wir keine Numerik machen!!! Dieses Problem ist \tbf{schlecht konditioniert}.
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			\epsilon & 1\\ 
			1 & 0
		\end{pmatrix},
	\end{align*}
	also
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			0 & 1\\ 
			1 & -\epsilon
		\end{pmatrix}.
	\end{align*}
	So haben wir kein Problem bei der Berechnung von $A^{-1}b$ - die Aufgabe ist gut konditioniert. Sagen wir nun, wir versuchen, das Gleichungssystem effizient durch $LU$-Zerlegung zu lösen. Wir sehen, $LU$-Zerlegung von $A$ ist jedoch gegeben durch
	\begin{align*}
		A = 
		\begin{pmatrix}
			1 & 0\\ 
			\frac{1}{\epsilon} & 1
		\end{pmatrix}
		\begin{pmatrix}
			\epsilon & 1\\ 
			0 & \frac{1}{\epsilon}
		\end{pmatrix},
	\end{align*}
	und die Berechnung von $L^{-1}b$ und  $U^{-1}b$ führt nun wieder zu großen Rundungsfehlern. Aus unserer Idee entsteht also ein \tbf{instabiler Algorithmus.}
\end{example}
\begin{theorem}
	\theoremname{Cholesky-Zerlegung:} Sei $A \in \bR^{n \times n}$ symmetrisch und positiv definit. So existiert eine eindeutige untere Dreiecksmatrix $L$, sodass
	\begin{align*}
		A = LL^\top.
	\end{align*} 
	und $l_{ii} > 0$
\end{theorem}
\begin{proof}
	Für $n = 1$ ist die Suche durch $l_{11} = \sqrt{a_{11}}$ erledigt.
	\newpar
	Die Untermatrix $A_{n-1}$ ist immer ebenfalls positiv definit und symmetrisch. Sei also $A_{n-1} = L_{n-1}L_{n-1}^\top$. Wir setzen $\binom{b}{a_{nn}}^\top$ als die letzte Zeile von $A$. Dann müssen wir zum Beweis des Satzes einen Vektor $c \in \bR^{n-1}$ und ein $\alpha \geq 0$ finden, sodass
	\begin{align*}
		\begin{pmatrix}
			A_{n-1} & b\\
			b^\top & a_{nn}
		\end{pmatrix}
		=
		\begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_{n-1} & L_{n-1}c\\
			(L_{n-1} - c)^\top & \alpha^2 + c^\top c
		\end{pmatrix}
	\end{align*}
	Dies ist nach Annahme äquivalent zu $L_{n-1}c = b$ und $c^\top c + \alpha^2 = a_{nn}$
	\newpar
	Da $L$ regulär ist existiert ein eindeutiges $c$, welches die erste Gleichung erfüllt.
	Es gilt: 
	\begin{align*}
		\det A = \det \begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\cdot
		\det
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\alpha^2(\det L_{n-1})^2
	\end{align*}
	Da $\det A > 0$ und $\det L_{n-1} \geq 0$ bekommen wir $\alpha > 0$, sodass $c^\top c + \alpha^2 = a_{nn}$ ebenfalls eine eindeutige positive Lösung hat.
\end{proof}
\begin{lemma}
	Für $A = LL^\top$ gilt:
	\begin{align*}
		a_{ik} = 
		\begin{cases}
			l_{ik}l_{kk} + \sum_{j = i}^{k-1} l_{ij}l_{ki} & i > k\\
			l_{kk}^2 + \sum_{j = 1}^{k-1} l_{kj}^2 & i = k
		\end{cases} 
	\end{align*}
\end{lemma}
\begin{proof}
	Matrixmultiplikation ohne triviale Summanden.
\end{proof}
\begin{algorithm}
	\tbf{Cholesky-Zerlegung:}
	\begin{algorithmic}[1]
		\For{$k = 1, i \leq n, i++$}
			\State $l_{kk} = \sqrt{a_{kk} - \sum_{j = 1}^{k-1}l_{kj}^2}$
			\For{$i = k+1, i \leq n, i++$}
				\State $l_{ik} = (a_{kk} - \sum_{j = 1}^{k-1} l_{ij}l_{kj}) \frac{1}{l_{kk}}$
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Aufwand ist wieder $\cO(n^3)$, allerdings mit kleineren Konstanten.
\end{proposition}
\begin{proposition}
	Lösung von $Ax = b$ für $A = LL^\top$ wie gehabt durch $Ly = b$ und $L^\top x = y$.
\end{proposition}
\subsection{Matrixnormen}
Bekannt sind die üblichen Vektornormen auf $\bR^n$, insbesondere 
\begin{align*}
	\norm{\vv}_p = \left(\sum_{j = 1}^n \abs{v_j}^p\right)^{\frac{1}{p}}
\end{align*}
und
\begin{align*}
	\norm{\vv}_\infty = \max v_j
\end{align*} 
Für $1 \leq p,q \leq \infty$ existiert eine Konstante $c_{pqn}$, sodass
\begin{align*}
	\forall \vv \in \bR^n : \frac{1}{c_{pqn}}\norm{\vv}_p \leq \norm{\vv}_q \leq c_{pqn}\norm{\vv}_p 
\end{align*}
\begin{definition}
	Für Normen $\norm{-}_{\bR^n}$ und $\norm{-}_{\bR^m}$ auf $\bR^n$ und $\bR^m$ definieren wir die Operatornorm auf $\hom(\bR^n, \bR^m) = \bR^{m \times n}$ als
	\begin{align*}
		\norm{A}_{op} = \sup_{x \in \bR^n : \norm{x}_{\bR^n} = 1} \norm{Ax}_{\bR^m}
	\end{align*}
\end{definition}
\begin{lemma}
	Die Operatornorm ist eine Norm.
\end{lemma}
\begin{proof}
	\begin{enumerate}
		\item Skalare können aus der inneren Norm und dem Supremum wie nötig herausgezogen werden.
		\item Das Supremum ist über einer Menge positiver Zahlen, falls $x \neq \vzero$ gibt es mindestens einen Vektor größer $0$.
		\item Dreiecksungleichung folgt aus der Dreiecksungleichung für $\norm{-}_{\bR^m}$.
	\end{enumerate}
\end{proof}
\begin{lemma}
	\begin{align*}
		\norm{A}_{op} = \inf \{c > 0 : \forall x \in \bR^n \norm{Ax} \leq c\norm{x}\}
	\end{align*}
\end{lemma}
\begin{lemma}
	Für $A \neq 0$ und $x \in \bR^n$ mit $\norm{x} \leq 1$ und $\norm{Ax} = \norm{A}_{op}$ folgt $\norm{x} = 1$
\end{lemma}
\begin{corollary}
	Für alle $x \in \bR^n$ gilt $\norm{Ax} \leq \norm{A}_{op}\norm{x}$
\end{corollary}
\begin{lemma}
	Es gibt Vektoren, sodass die Matrixnorm ihr $\inf$ und ihren $\sup$ annimmt.
\end{lemma}
\begin{proof}
	Es handelt sich um eine stetige Funktion auf einem Kompaktum.
\end{proof}
\begin{example}
\begin{enumerate}
	\item Die \tbf{Spaltensummennorm} $\norm{-}_1$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{1} = \max_{j = 1, \hdots, n} \sum_{i = 1}^m \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Zeilensummennorm} $\norm{-}_\infty$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{\infty} = \max_{i = 1, \hdots, m} \sum_{j = 1}^n \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Spektralnorm} $\norm{-}_2$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_2 = \rho(A^\top A) = (\max \{\abs{\lambda} : \lambda \textnormal{ ist Eigenwert von $A^\top A$}\})^\frac{1}{2}
	\end{align*}
\end{enumerate}
\end{example}
\begin{lemma}
	Für $A \in \bR^{l \times m}$, $B \in \bR^{m \times n}$ und eine beliebige Operatornorm $\norm{-}$ gilt $\norm{AB} \leq \norm{A}\norm{B}$
\end{lemma}
\begin{proof}
	\begin{align*}
		&\norm{ABx} \leq \norm{A}\norm{Bx} \leq \norm{A}\norm{B}\norm{x}\\
		\implies &\norm{AB} \leq \norm{A}\norm{B}
	\end{align*}
\end{proof}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt
	\begin{align*}
		\norm{E_n} = 1
	\end{align*}
\end{lemma}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt für $A$ symmetrisch mit Eigenwert $\lambda$
	\begin{align*}
		\norm{A} \geq \abs{\lambda}
	\end{align*}
\end{lemma}
\begin{example}
	Die Frobeniusnorm $\norm{-}_\cF$ einer Matrix $A \in \bR^{m \times n}$ ist gegeben durch
	\begin{align*}
		\norm{A}_\cF = \left(\sum_{j = 1}^m \sum_{i = 1}^n a_{ij}^2\right)^\frac{1}{2}
	\end{align*}
\end{example}
\begin{lemma}
	Für $n > 1$ ist die Frobeniusnorm keine Operatornorm!
\end{lemma}
\begin{proof}
	\begin{align*}
			\norm{E_n} = \sqrt{n}
	\end{align*}
	Normieren wir die Norm, gilt
	\begin{align*}
		\frac{1}{\sqrt{2}}\norm{\begin{pmatrix}
				1 & 0\\
				0 & 0
		\end{pmatrix}}_{\cF}
		=
		\frac{1}{\sqrt{2}}
		< 
		1
	\end{align*}
	Wobei $1$ ein Eigenwert ist, was unseren vorherigen Lemmata widerspricht.
\end{proof}
\subsection{Konditionszahl}
\begin{theorem}
	Sei $\norm{-}$ eine Operatornorm auf $\bRnn$. Sei $A \in \bRnn$ regulär und seien $x, x', b, b' \in \bR^n$, sodass $Ax = b$, $Ax' = b'$. Dann gilt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\frac{\norm{b - b'}}{\norm{b}}
	\end{align*}
\end{theorem}
\begin{theorem}
	\begin{align*}
		\norm{x - x'} = \norm{A^{-1}(b - b')} \leq \norm{A^{-1}}\norm{b - b'}
	\end{align*}
	und
	\begin{align*}
		\norm{b} = \norm{Ax} \leq \norm{A}\norm{x}
	\end{align*}
	Es folgt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{A^{-1}}\norm{b}}
	\end{align*}
\end{theorem}
\begin{definition}
	Die \tbf{Konditionszahl} einer regulären Matrix $A \in \bRnn$ bezüglich der durch $\norm{-}$ auf $\bR^n$ induzierten Operatornorm ist gegeben durch:
	\begin{align*}
		\textnormal{cond}_{\norm{-}}(A) = \norm{A}\norm{A^{-1}}
	\end{align*}
\end{definition}
Wir schreiben oft $\textnormal{cond}_p$ statt $\textnormal{cond}_{\norm{-}_p}$.
\begin{lemma}
	$\textnormal{cond}(A) \geq 1$
\end{lemma}
\begin{lemma}
	Für $A$ symmetrisch mit Eigenwerten $\lambda_i$ gilt 
	\begin{align*}
		\cond_2(A) = \frac{\max \abs{\lambda_j}}{\min \abs{\lambda_j}}
	\end{align*}
\end{lemma}
\begin{example}
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Besitzt die Eigenwerte
	\begin{align*}
		\lambda_{1,2} = 1 + \frac{\epsilon}{2} \pm \left(1 + \frac{\epsilon^2}{4}\right)^\frac{1}{2}
	\end{align*}
	Also $\lambda_1 \approx 2 + \frac{\epsilon}{2}$ und $\lambda_2 \approx \frac{\epsilon}{2}$. Für $\epsilon \to 0$ geht also die Konditionszahl gegen unendlich.
\end{example}
\begin{theorem}
	Für $A$ symmetrisch und positiv definit mit Cholesky-Zerlegung $A = L L^\top$ gilt
	\begin{align*}
		\cond_2(L) = \cond_2(L^\top) = \sqrt{\cond(A)}
	\end{align*}
\end{theorem}
 \noindent Also kann das Problem, welches bei der $LU$-Zerlegung auftrat, bei der Cholesky-Zerlegung nicht vorkommen.
 \begin{proof}
 	Wir bemerken zunächst, dass $L^\top L$ und $LL^\top$ die selben Eigenwerte haben. Beide Matrizen sind symmetrisch und es gilt
 	\begin{align*}
 		\forall &x \in \bR^n, \lambda \in \bR : \\
 		&L^\top L x = \lambda x \Leftrightarrow  LL^\top Lx = \lambda(Lx) := \lambda y
 	\end{align*}
 	Somit folgt
 	\begin{align*}
 		\rho(LL^\top) = \rho(L^\top L)
 	\end{align*}
 	und somit auch
 	\begin{align*}
 		\norm{L}_2 = \norm{L^\top}_2
 	\end{align*}
 	analog gilt
 	\begin{align*}
 		\norm{L^{-1}}_2 = \norm{L^{-\top}}_2
 	\end{align*}
 	Also $\cond_2(L) = \cond_2(L^\top)$. Da $LL^\top = A$, und $A$ symmaterisch, folgt
 	\begin{align*}
 		\norm{L}_2^2 &= \norm{L^\top}_2^2\\
 					 &= \rho(LL^\top)\\
 					 &= \rho(A)\\ 
 					 &= \norm{A}_2
 	\end{align*}
 	und
 	\begin{align*}
 		\norm{L^{-1}}_2^2 &= \rho(L^{-\top}L^{-1})\\
 		 				  &= \rho(A^{-1})\\ 
 		 				  &= \norm{A^{-1}}_2,
 	\end{align*}
 	also insgesamt
 	\begin{align*}
 		\cond_2(L) &= \norm{L}_2\norm{L^{-1}}_2\\
 		           &= \norm{A}_2^{1/2} \norm{A^{-1}}_2^{1/2}\\ 
 		           &= \sqrt{\cond_2(A)}
 	\end{align*}
 \end{proof}
\chapter{Eliminationsverfahren}
 \section{Gauss-Jordan-Elimination}
 	\begin{definition}\theoremname{Gauss-Jordan-Elimination}
 		Sei $A \in \bRnn$, $b \in \bR^n$.
 		\begin{enumerate}
 			\item Setze $A^{(1)} = A$, $b^{(1)} = b$, $k = 1$.
 			\item Für $A^{(k)}$ gelte für $1 \leq j \leq k -1$ und $i \geq j + 1$ $a_{ij}^k = 0$, d.h. $A^{(k)}$ habe die Form
 			\begin{align*}
 				\begin{pmatrix}
 					a_{11}&\hdots&&&\hdots&a_{1n}\\
 					&a_{22}&&&&\vdots\\
 					&&\ddots\\
 					&&&a_{kk} & \hdots & a_{kn}\\
 					&&&\vdots & &\vdots\\
 					&&&a_{nk} & \hdots & a_{nn}\\
 				\end{pmatrix}
 			\end{align*}
 			mit Nullen im unteren linken Teil.
 			\item Wir setzen $\displaystyle l_{ik} = \frac{a_{ik}}{a_{kk}^{(k)}}$ und definieren $L \in \bRnn$ als
 			\begin{align*}
 				L =
 				\begin{pmatrix}
 					1&\hdots&&&\hdots&0\\
 					&1&&&&\vdots\\
 					&&\ddots\\
 					&&&1 & \hdots & 0\\
 					&&&-l_{k+1, k} & \hdots & 0\\
 					\vdots&&&\vdots & &\vdots\\
 					0&\hdots&&-l_{n,k} & \hdots 0 \hdots& 1\\
 				\end{pmatrix}
 			\end{align*}
 			\item Setze $A^{(k+1)} = L^{(k)}A^{(k)}$, $b^{(k+1)} = L^{(k)}b^{(k)}$
 			\item Stoppe, falls $k + 1 = n$, sonst erhöhe $k$ und gehe zu Schritt 2.
 		\end{enumerate}
 	\end{definition}
 	\begin{theorem}
 		Ist $A \in \bRnn$ regulär, so ist Gauß-Jordan-Elimination genau dann durchführbar, wenn $A$ eine $LU$-Zerlegung hat. Das Verfahren liefert dann die normierte $LU$-Zerlegung $U = A^{(n)}$ und $L = (L^{(n-1)} \cdot \hdots \cdot L^{(1)})^{-1}$. Die rechte Seite $y = b^{(n)}$ löst dann $y = L^{-1}b$ und die Lösung des Linearen Gleichungssystems $Ax = b$ ist gegeben durch die Lösung von $Ux = y$.
 	\end{theorem}
\section{Pivotsuche}
	Das Gaußverfahren ist für 
	\begin{align*}
	 	A = \begin{pmatrix}
	 		\epsilon & 1\\
	 		1 & 1\\
	 	\end{pmatrix}
	\end{align*}
	zwar durchführbar, aber instabil. Wir führen deswegen eine sogenannte Pivotsuche durch - im $k$-ten Schritt bestimmen wir $p \in \{k, \hdots, n\}$, sodass
	\begin{align*}
	 	\abs{a_{pk}^{(k)}} = \max a_{ik}^{(k)}
	\end{align*}
	und vertauschen dann die Zeilen $p$ und $k$ in $A^{(k)}$ und $b^{(k)}$. Wir müssen diese Vertauschung jedoch nicht im Speicher tatsächlich durchführen, es reicht, einen Permutationsvektor $\pi \in \bN^n$ vorzuschalten. Wir initialisieren $\pi$ als $(1,2,\hdots,n)^\top$, sollen daraufhin $k$ und $p$ vertauscht werden, vertauschen wir die jeweiligen Komponenten in $\pi$. Wollen wir daraufhin im Programm auf $a_{ij}$ Zugreifen, müssen wir stattdessen auf $a_{\pi(i) j}$ zugreifen.
	\begin{theorem}
		Ist $A \in \bRnn$ regulär, $b \in \bR^n$, so ist das Gaußverfahren mit Pivotsuche durchführbar und liefert die normalisierte $LU$-Zerlegung 
		\begin{align*}
			PA = LU
		\end{align*}
		mit
		$\abs{l_{ij}} \leq 1$
		für alle $i,j$, sowie die modifizierte rechte Seite $b^{(n)} = L^{-1}Pb$, wobei 
		\begin{align*}
			P = P^{(n-1)} P^{(n-2)} \hdots P^{(1)}
		\end{align*}
	\end{theorem}




\chapter{Ausgleichsprobleme}
\begin{example}
	Zu Messdaten $t_i, y_i, i = 1, \hdots, m$ wird die Ausgleichsgerade gesucht, also die Gerade definiert durch $c,b \in \bR$, sodass der Least-Squares Abstand
	\begin{align*}
		\sum_{i = 1}^n \lr((c \cdot t_i + b) - y_i)^2
	\end{align*}
	zu den Messdaten minimiert wird.
\end{example}
\noindent Im Allgemeinen haben solche Probleme die Form
\begin{align*}
	\min (x \mapsto \norm{Ax - b}_2^2).
\end{align*}
Ist $A$ eine reguläre $n \times n$-Matrix, so wird das Problem eindeutig durch $x = A^{-1}b$ gelöst. Typischerweise ist dies aber nicht der Fall - in der Praxis sind die meisten Gleichungssysteme überbestimmt.

\section{Die Gaußsche Normalengleichung}
\begin{definition}
	Durch $A \in \bR^{m \times n}, b \in \bR^m$ wird das Ausgleichsproblem 
	\begin{align*}
		\min (x \mapsto \norm{Ax - b}_2^2).
	\end{align*}
	definiert. Für $x \in \bR^n$ heißt
	\begin{align*}
		r= b - Ax
	\end{align*}
	das \tbf{Residuum} von $x$.
\end{definition}
\begin{theorem}
	Die Lösungen des Ausgleichsproblems sind genau die Lösung der \tbf{Gaußschen Normalengleichung}
	\begin{align*}
		A^\top Ax = A^\top b.
	\end{align*}
	Insbesondere existiert immer eine Lösung $x \in \bR^n$. Ist $z \in \bR^n$ eine weiter Lösung, so gilt $Ax = Az$ und die dazugehörigen Residuen stimmen überein.
\end{theorem}
\noindent Diese Normalengleichung erhält man durch Ableitung der Residuenfunktion $Ax-b$ nach $x$.
\newpar
\begin{proof}
	Aus der linearen Algebra ist bekannt, dass
	\begin{align*}
		\bR^m = \im(A) + \ker(A^\top),
	\end{align*}
	wobei diese Zerlegung direkt und orthogonal ist. Damit existieren zu $b \in \bR^m$ eindeutig bestimmte Vektoren $y \in \im(A)$, $r \in \ker(A^\top)$, sodass $y \cdot r = 0$ und $b = r + y$. Weiter existiert $x \in \bR^n$ mit $y = Ax$.
	\newpar
	Es folgt
	\begin{align*}
		A^\top b = A^\top y + A^\top r = A^\top Ax + 0 = A^\top Ax.
	\end{align*}
	Somit löst $x$ die Gaußsche Normalengleichung. Es bleibt zu zeigen, dass $x$ auch das Ausgleichsproblem löst. Sei $z \in \bR^n$. So rechnen wir
	\begin{align*}
		\norm{b-Az}_2^2 &= \norm{(b - Ax) + A(x-z)}_2^2\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2 + 2r \cdot (Ax - z)\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2 + \underbrace{2A^\top r}_{= 0} \cdot (x - z)\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2\\
						&\geq \norm{b - Ax}_2^2
	\end{align*}
	somit gilt insbesondere Gleichheit genau dann, wenn $Ax = Az$.
\end{proof}
\begin{lemma}
	Die Matrix $A^\top A$ ist symmetrisch und positiv semidefinit. Weiter ist $A$ genau dann positiv definit, wenn $\ker A = \{0\}$. In diesem Fall sind die Lösungen der Gaußschen Normalengleichung eindeutig.
\end{lemma}
\begin{proof}
	Symmetrie ist offensichtlich. Positive Semidefinitheit gilt, da
	\begin{align*}
		x(A^\top A) x = (Ax) \cdot (Ax) = \norm{Ax}_2^2 \geq 0.
	\end{align*}
	Insbesondere gilt also Gleichheit genau dann, wenn $Ax = 0$, also wenn $x \in \ker A$.
	\newpar
	Die Eindeutigkeit der Lösung der Gaußschen Normalengleichung folgt aus der Regularität positiv definiter Matrizen. 
\end{proof}
\noindent Für $m = n$, $A \in \bRnn$ gilt
\begin{align*}
	\cond_2 (A^\top A) = \frac{\lambda_{\max}(A^\top A)}{\lambda_{\min}(A^\top A)} = (\cond_2(A))^2
\end{align*}
da $\cond_2 (A) \geq 1$ ist somit $A^\top A$ immer schlechter konditioniert als $A$. Die Lösung eines Ausgleichsproblems durch die Gaußsche Normalengleichung ist somit instabil.

\section{Householder-Matrizen}
Sei $Q \in O(n)$ (also eine $n \times n$-Orthogonalmatrix). So gilt $\norm{Q(Ax - b)}_2^2 = \norm{Ax-b}$. Wir versuchen, eine orthogonale Matrix $Q$ so zu konstruieren, dass $QA$ Dreiecksgestalt hat.
\newpar
\begin{lemma}
	Für alle Orthogonalen Matrizen $Q$ gilt $\cond_2(Q) = 1$
\end{lemma}
\begin{proof}
	\begin{align*}
		\norm{Q}_2 \norm{Q^\top}_2 = \norm{Q}_2 \norm{Q^{-1}}_2 = 1
	\end{align*}
\end{proof}
\begin{definition}
	Für $v \in \bR^l$ mit $\norm{v}_2 = 1$ heißt die Matrix
	\begin{align*}
		P_v = E_l - 2vv^\top
	\end{align*}
	Die Householder-Transformation zu $v$.
\end{definition}
\noindent $(vv^\top)x$ entspricht der Projektion von $x$ auf den von $v$ aufgespannten Vektorraum. Insgesamt spiegelt die Householder-Transformation also $x$ an der Ursprungsebene orthogonal zu $v$.
\begin{lemma}
	$P_v$ ist symmetrisch und orthogonal. Außerdem gilt $P_vv = -v$ und
	\begin{align*}
		\forall w \in \bR^l : wv = 0 \implies P_v w = w
	\end{align*}
\end{lemma}
\begin{lemma}
	Sei $x \in \bR^l \neq 0, x \neq \lambda e_1$ und sei
	\begin{align*}
		\sigma = \begin{cases}
			\sgn(x_1) & x_1 \neq 0\\
			1 & \textnormal{sonst}
		\end{cases}
	\end{align*}  
	Setzen wir nun
	\begin{align*}
		v = \frac{x + \sigma \norm{x}_2 e_1}{\norm{x + \sigma \norm{x}_2 e_1}_2},
	\end{align*}
	so gilt
	\begin{align*}
		P_v x = (E_l x - 2vv^\top)x = -\sigma\norm{x}_2 e_1
	\end{align*}
\end{lemma}
\begin{proof}
	Da $x \neq \lambda e_1$ ist $v$ wohldefiniert mit $\norm{v}_2 = 1$. Weiter folgt
	\begin{align*}
		\norm{x + \sigma \norm{x}_2 e_1}_2^2 &= \norm{x}^2 + 2 \sigma\norm{x}_2 x \cdot e_1 + \sigma^2 \norm{x}_2^2 \norm{e_1}_2^2\\
		&= 2(x + \sigma \norm{x}_2 e_1)^\top x
	\end{align*}
	Mit $\tilde v = x + \sigma \norm{x}_2 e_1$ gilt
	\begin{align*}
		2\tilde v ^\top x &= 2(x + \sigma \norm{x}_2 e_1)^\top x\\
		&= \norm{x + \sigma \norm{x}_2 e_1}_2^2\\
		&= \norm{\tilde v}^2_2
	\end{align*}
	Es gilt $v = \frac{\tilde v}{\norm{v}_2}$, also
	\begin{align*}
		P_vx &= (E_l - 2vv^\top)x\\
		     &= x - 2v\frac{\tilde v^\top x}{\norm{\tilde v}_2}\\
		     &= x - v\frac{\norm{\tilde v}_2^2}{\norm{\tilde v}_2}\\
		     &= x - v\norm{\tilde v}_2\\
		     &= - \sigma\norm{x}_2 e_1
	\end{align*}
\end{proof}
\noindent $\sigma$ verhindert hier sogenannte Auslöschungseffekte, also schlechte Konditionierung der Subtraktion zweier fast identischer Zahlen.
\section{$QR$-Zerlegung}
\begin{theorem}
	Sei $A \in \bR^{m \times n}$, $m \geq n$, $\rang A = n$. So existiert $Q \in O(m)$ und eine verallgemeinerte obere Dreiecksmatrix $R \in \bR^{m \times n}$, sodass
	\begin{align*}
		A = QR
	\end{align*}
	Außerdem gilt $\forall i : \abs{r}_{ii} \geq 0$
\end{theorem}
\begin{proof}
	Wir setzen $A_1 = A$, und es sei $x = a_1 \in \bR^m$ die erste Spalte von $A_1$. Falls $x \in \bR{e_1}$, setzen wir $Q_1 = E_m$. Ansonsten sei
	\begin{align*}
		Q_1 = P_v
	\end{align*}
	mit $v$ wie im Lemma. Es folgt
	\begin{align*}
		Q_1 a_1 = r_{11}e_1
	\end{align*}
	mit $\abs{r_{11}} = \norm{a_1}_2 \geq 0$. Somit folgt
	\begin{align*}
		Q_1 A_1 = 
		\begin{pmatrix}
			r_{11} & r_1^\top\\
			\vzero & A_2
		\end{pmatrix}
	\end{align*}
	mit $A_2 \in \bR^{(m-1) \times (n-1)}$.
\end{proof}
\end{document}