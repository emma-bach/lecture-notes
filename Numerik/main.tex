\documentclass{report}

% custom margins
\usepackage[a4paper,margin=1.5in]{geometry}
\renewcommand{\baselinestretch}{1.2}

% emma's long list of custom macros and universally used packages
\include{../macros-and-packages.tex}

% fancy bar next to proofs
\tcolorboxenvironment{proof}{
	colback=white,
	boxrule=0pt,
	frame hidden,
	borderline west={1pt}{0pt}{black},
	before skip=0.75cm,
	after skip=0.75cm,
	sharp corners,
	breakable,
	enhanced,
}

\usepackage{algorithm}
\usepackage{algpseudocode}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
	\include{title}
	\tableofcontents
	\thispagestyle{fancy}
	\chapter{Aufgabenstellung}
	In der Numerik beschäftigt man sich mit der praktischen Berechnung von Lösungen mathematischer Probleme.
	\begin{example}
		Berechne $\displaystyle \int_0^1 e^{-x^2} dx$!
	\end{example}
	\begin{example}
		Berechne $\sin(20)$!
	\end{example}
	\begin{example}
		Berechne $\sqrt{753}$!
	\end{example}
	\begin{example}
		Berechne $\displaystyle \min_{x \in [0,1]} F(x)$, für eine geeignete Funktion $F$!
	\end{example}
	\begin{example}
		Berechne $x$, sodass $f(x) = 0$!
	\end{example}
	\begin{example}
		Berechne $x \in \bR^n$, sodass $Ax = b$!
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe in der Numerik besteht im Finden einer Lösung von
		\begin{align*}
			F(x,d) = 0
		\end{align*}
		für gegebenes Datum $d$ und gegebene Funktion $F$.
	\end{definition}
	\noindent
	Typischerweise können in akzeptabler Zeit keine exakten Lösungen gefunden werden, sondern nur Approximationen. Insbesondere stehen statt den vollen Mengen $\bQ$, $\bR$, $\bC$ etc. auch nur endlich viele \tbf{Maschinenzahlen} zur Verfügung - arbiträre reelle Zahlen benötigen unendlich viel Speicher! Rechenoperationen sind dementsprechend Fehlerbehaftet, es gibt Rundungsfehler. Außerdem gibt es in reellen Anwendungen oft \tbf{Modellfehler} und \tbf{Datenfehler}.
	\newpar
	Eine Grundlegende Idee in der Numerik ist es deshalb, eine gute Balance zwischen Exaktheit und Aufwand der Berechnung zu finden.
	\begin{example}
		Die Berechnung der Determinante einer Matrix mittels Laplaceschem Entwicklungssatz benötigt $O(n!)$ Rechenoperationen. Die Determinante mit diesem Verfahren zu berechnen, dauert sehr viel länger, als das Universum alt ist.
		\newpar
		Besser: Matrix (approximativ) auf Dreiecksgestalt bringen und die Diagonalelemente multiplizieren.
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe heißt \tbf{wohlgestellt}, wenn zu geeigneten Daten $d$ eindeutige Lösungen $x$ existieren, und diese stetig von $d$ abhängt. Andernfalls ergibt die Suche nach einer numerischen Lösung wenig Sinn. Für wohlgestellte Probleme existiert eine Lösungsfunktion $\phi$, sodass $x = \phi(d)$ das Problem löst, d.h. $f(\phi(d),d) = 0$.
	\end{definition}
	\begin{definition}
		Ein numerischer Algorithmus zur näherungsweisen Lösung einer wohlgestellten Aufgabe $\phi$ ist eine Abbildung $\tilde{\phi}$, die durch Hintereinanderausführung möglicherweise fehlerbehafteter elementarer Rechenoperationen definiert ist, also 
		\begin{align*}
			\tilde{\phi} = f_j \circ f_{j-1} \circ \hdots \circ f_1
		\end{align*}
	\end{definition}
	\begin{definition}
		Der \tbf{Aufwand} eines Verfahrens $\tilde \phi$ ist die Anzahl der benötigten elementaren Rechenschritte. Typischerweise interessiert uns nicht die exakte Anzahl an Schritten, sondern nur die Größenordnung.
	\end{definition}
	\begin{proposition}
		Das Gaußverfahren hat Aufwand $\cO(n^3)$.
	\end{proposition}
\chapter{Numerische Lineare Algebra}
\section{Matrixfaktorisierung}
\subsection{Dreiecksmatrizen}
\begin{definition}
	Eine Matrix $L \in \bR^{n \times n}$ heißt \tbf{untere Dreiecksmatrix}, falls $\forall i < j : l_{ij} = 0$.
\end{definition}
\begin{definition}
	Eine Matrix $U \in \bR^{n \times n}$ heißt \tbf{obere Dreiecksmatrix}, falls $U^\top$ eine untere Dreiecksmatrix ist.
\end{definition}
\begin{definition}
	Eine Dreiecksmatrix heißt \tbf{normalisiert}, falls alle ihre Diagonaleinträge $1$ sind.
\end{definition}
\begin{definition}
	Eine Matrix heißt \tbf{regulär}, wenn sie invertierbar ist.
\end{definition}
\begin{lemma}
	Die quadratischen oberen (bzw. unteren) Dreiecksmatrizen bilden unter Matrixmultiplikation eine Gruppe.
\end{lemma}
\noindent
Lineare Gleichungssysteme mit regulärer Dreiecksmatrix lassen sich leicht lösen. Sei $U \in \bR^{n \times n}$ eine reguläre obere Dreiecksmatrix und $b \in \bR^n$. Wir berechnen $x \in \bR^n$ folgendermaßen:
\begin{enumerate}
	\item for i = n : -1 : 1:
	\begin{enumerate}
		\item $\displaystyle x_i = \left(b_i - \sum_{j = i+1}^n u_{ij}x_j\right) \cdot \frac{1}{u_{ii}}$
	\end{enumerate}
	\item end.
\end{enumerate}
Der Aufwand dieses Verfahrens ist $\cO(n^2)$. Ein analoger Algorithmus existiert für untere Dreiecksmatrizen.
\subsection{$LU$-Zerlegung}
Falls für eine reguläre Matrix $A \in \bR^{n \times n}$ eine Zerlegung $A = LU$ in eine untere Dreiecksmatrix $U$ und eine obere Dreiecksmatrix $L$ gegeben ist, so lässt sich das lineare Gleichungssystem $Ax = b$ in zwei Schritten lösen:
\begin{enumerate}
	\item Löse $Ly = b$.
	\item Löse $Ux = y$.
\end{enumerate}
\begin{definition}
	Eine Faktorisierung $A = LU$ mit unterer Dreiecksmatrix $L \in \bR^{n \times n}$ und oberer Dreiecksmatrix $U \in \bR^{n \times n}$ heißt \tbf{$LU$-Zerlegung} von $A$. Die Zerlegung heißt \tbf{normalisiert}, falls $L$ normalisiert ist.
\end{definition}
\begin{theorem}
	Für jede reguläre Matrix $A \in \bR^{n \times n}$ sind äquivalent:
	\begin{enumerate}
		\item Es existiert eine eindeutige normalisierte $LU$-Zerlegung.
		\item Alle Untermatrizen $A_k = (a_{ij})_{(i,j) \in (1, \hdots, k)^2}$ sind regulär. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	\phantom{}
	\begin{itemize}
		\item[$\rightarrow$] Ist $A$ regulär, so sind auch $L$ und $U$ regulär. Damit sind von $L$ und $U$ alle Diagonaleinträge nicht null. Somit sind auch die Untermatrizen $L_k$ und $U_k$ regulär, somit auch die Untermatrizen $A_k = L_k U_k$.
		\item[$\leftarrow$] Für $n = 1$ ist die Aussage klar. Sei nun angenommen, die Aussage gelte für Matrizen der Größe $(n - 1) \times (n - 1)$. Damit existieren Matrizen $L_{n-1}, U_{n-1}$, sodass $A_{n-1} = L_{n-1}U_{n-1}$ eine normalisierte $LU$-Zerlegung ist. Seien nun $\binom{b}{a_{nn}}$ die letzte Spalte und $(c^\top, a_{nn})$ die letzte Zeile von $A$. Die Aussage ist bewiesen, wenn geeignete $l,u \in \bR^{n-1}$ und $r \in \bR$ existieren, sodass
		\begin{align*}
			\begin{pmatrix}
				A_{n-1} & b\\
				c^\top & a_{nn}
			\end{pmatrix}
			&=
			\begin{pmatrix}
				L_{n-1} & 0\\
				l^\top & 1
			\end{pmatrix}
			\begin{pmatrix}
				U_{n-1} & u\\
				0 & r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				L_{n-1}U_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				A_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}
		\end{align*}
		Wir brauchen also $L_{n-1}u = b$, $U_{n-1}^\top l = c$, und $a_{nn} = l^\top u + r$. Durch Regularität von $L_{n-1}$ und $U_{n-1}$ existieren eindeutige Lösungen $u, l$, der ersten beiden Gleichungen, somit ist auch $r$ festgelegt.
	\end{itemize}
\end{proof}
\begin{corollary}
	\phantom{}
	\begin{itemize}
		\item Jede positiv definite Matrix besitzt eine eindeutige $LU$-Zerlegung.
		\item Jede strikt diagonaldominante Matrix, also jede Matrix $A$ mit $\sum_{j \in 1, \hdots, n, i \neq j} \abs{a_ij} < \abs{a_{ii}}$ besitzt eine eindeutige $LU$-Zerlegung.
		\item Die Matrix $A = \begin{pmatrix}
			0 & 1\\
			1 & 0
		\end{pmatrix}$
		besitzt keine $LU$-Zerlegung.
		\item Die Nullmatrix besitzt zwar $LU$-Zerlegungen, diese sind aber nicht eindeutig.
	\end{itemize}
\end{corollary}
\begin{lemma}
	Falls $A = LU$ eine normalisierte $LU$-Zerlegung von $A$ ist, so gilt
	\begin{align*}
		a_{ik} = u_{ik} + \sum_{j = 1}^{i-1} l_{ij} u_{jk}
	\end{align*}
	und
	\begin{align*}
		a_{ki} = l_{ki} u_{ii} + \sum_{j = 1}^{i-1} l_{kj} u_{ji}
	\end{align*}
\end{lemma}
\begin{proof}
	Es gilt $l_{ij} = 0$ für $j > i$ und $l_{ii} = 1$. Es gilt außerdem $u_{ij} = 0$ für $j < i$. Die Formeln folgen direkt aus der Definition des Matrixprodukts.
\end{proof}
Diese Formeln lassen sich für $i \leq k$ nach $u_{ik}$ auflössen und für $k > i$ nach $l_{ki}$ auflösen. Wir erhalten folgenden Algorithmus:
\begin{algorithm}
\begin{algorithmic}[1]
	\For{$i = 1, i \leq n, i++$}
		\For{$k = i, k \leq n, k++$}
			\State $\displaystyle u_{ik} \gets a_{ik} - \sum_{j = 1}^{i-1}l_{ij}u_{jk}$
		\EndFor
		\For{$k = i+1, k \leq n, k++$}
		\State $\displaystyle l_{ki} \gets \frac{1}{u_{ii}} \cdot \left(a_{ki} - \sum_{j = 1}^{i-1}l_{kj}u_{ji}\right)$
		\EndFor
	\EndFor
\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Rechenauftrag dieses Algorithmus beträgt $O(n^3)$.
\end{proposition}
\begin{proposition}
	Es ist nicht mehr Speicher nötig, als sowieso für $A$ benötigt wird. Die Einträge von $A$ können im Speicher einfach sukzessiv durch die jeweiligen Einträge von $L$ bzw. $U$ ersetzt werden.
\end{proposition}
\begin{definition}
	Ein numerisches Problem $\phi$ heißt \tbf{schlecht Konditioniert}, wenn kleine Unterschiede in der Eingabe zu großen Unterschieden in der korrekten Lösung führen, also wenn
	\begin{align*}
		\frac{\abs{\phi(\tilde{x}) - \phi(x)}}{\abs{\phi(x)}} >> \frac{\abs{\tilde{x} - x}}{\abs{x}}
	\end{align*}
	Ansonsten heißt die Aufgabe \tbf{gut konditioniert}.
\end{definition}
\begin{definition}
	Ein Verfahren $\tilde{\phi}$ heißt \tbf{numerisch instabil}, wenn eine Störung $\tilde{x}$ existiert, sodass der durch Rundungsfehler verursachte relative Fehler erheblich größer ist als der rein durch die Störung verursachte Fehler.
\end{definition}
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\ 
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Für $\epsilon \in \bR^+$. Es gilt
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			1 + \frac{1}{\epsilon} & -\frac{1}{\epsilon}\\ 
			- \frac{1}{\epsilon} & \frac{1}{\epsilon}
		\end{pmatrix}
	\end{align*}
	\begin{align*}
		A^{-1} \binom{1}{1} = \binom{1}{0}. \qquad \qquad A^{-1} \binom{1}{1 + \epsilon} = \binom{0}{1}.
	\end{align*}
\end{example}
\noindent Eine kleine Störung in den Daten $b$ des linearen Gleichungssystems $Ax = b$ führt zu Problemen der Größenordnung $1$!!! So können wir keine Numerik machen!!! Dieses Problem ist \tbf{schlecht konditioniert}.
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			\epsilon & 1\\ 
			1 & 0
		\end{pmatrix},
	\end{align*}
	also
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			0 & 1\\ 
			1 & -\epsilon
		\end{pmatrix}.
	\end{align*}
	So haben wir kein Problem bei der Berechnung von $A^{-1}b$ - die Aufgabe ist gut konditioniert. Sagen wir nun, wir versuchen, das Gleichungssystem effizient durch $LU$-Zerlegung zu lösen. Wir sehen, $LU$-Zerlegung von $A$ ist jedoch gegeben durch
	\begin{align*}
		A = 
		\begin{pmatrix}
			1 & 0\\ 
			\frac{1}{\epsilon} & 1
		\end{pmatrix}
		\begin{pmatrix}
			\epsilon & 1\\ 
			0 & \frac{1}{\epsilon}
		\end{pmatrix},
	\end{align*}
	und die Berechnung von $L^{-1}b$ und  $U^{-1}b$ führt nun wieder zu großen Rundungsfehlern. Aus unserer Idee entsteht also ein \tbf{instabiler Algorithmus.}
\end{example}
\begin{theorem}
	\theoremname{Cholesky-Zerlegung:} Sei $A \in \bR^{n \times n}$ symmetrisch und positiv definit. So existiert eine eindeutige untere Dreiecksmatrix $L$, sodass
	\begin{align*}
		A = LL^\top.
	\end{align*} 
	und $l_{ii} > 0$
\end{theorem}
\begin{proof}
	Für $n = 1$ ist die Suche durch $l_{11} = \sqrt{a_{11}}$ erledigt.
	\newpar
	Die Untermatrix $A_{n-1}$ ist immer ebenfalls positiv definit und symmetrisch. Sei also $A_{n-1} = L_{n-1}L_{n-1}^\top$. Wir setzen $\binom{b}{a_{nn}}^\top$ als die letzte Zeile von $A$. Dann müssen wir zum Beweis des Satzes einen Vektor $c \in \bR^{n-1}$ und ein $\alpha \geq 0$ finden, sodass
	\begin{align*}
		\begin{pmatrix}
			A_{n-1} & b\\
			b^\top & a_{nn}
		\end{pmatrix}
		=
		\begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_{n-1} & L_{n-1}c\\
			(L_{n-1} - c)^\top & \alpha^2 + c^\top c
		\end{pmatrix}
	\end{align*}
	Dies ist nach Annahme äquivalent zu $L_{n-1}c = b$ und $c^\top c + \alpha^2 = a_{nn}$
	\newpar
	Da $L$ regulär ist existiert ein eindeutiges $c$, welches die erste Gleichung erfüllt.
	Es gilt: 
	\begin{align*}
		\det A = \det \begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\cdot
		\det
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\alpha^2(\det L_{n-1})^2
	\end{align*}
	Da $\det A > 0$ und $\det L_{n-1} \geq 0$ bekommen wir $\alpha > 0$, sodass $c^\top c + \alpha^2 = a_{nn}$ ebenfalls eine eindeutige positive Lösung hat.
\end{proof}
\begin{lemma}
	Für $A = LL^\top$ gilt:
	\begin{align*}
		a_{ik} = 
		\begin{cases}
			l_{ik}l_{kk} + \sum_{j = i}^{k-1} l_{ij}l_{ki} & i > k\\
			l_{kk}^2 + \sum_{j = 1}^{k-1} l_{kj}^2 & i = k
		\end{cases} 
	\end{align*}
\end{lemma}
\begin{proof}
	Matrixmultiplikation ohne triviale Summanden.
\end{proof}
\begin{algorithm}
	\tbf{Cholesky-Zerlegung:}
	\begin{algorithmic}[1]
		\For{$k = 1, i \leq n, i++$}
			\State $l_{kk} = \sqrt{a_{kk} - \sum_{j = 1}^{k-1}l_{kj}^2}$
			\For{$i = k+1, i \leq n, i++$}
				\State $l_{ik} = (a_{kk} - \sum_{j = 1}^{k-1} l_{ij}l_{kj}) \frac{1}{l_{kk}}$
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Aufwand ist wieder $\cO(n^3)$, allerdings mit kleineren Konstanten.
\end{proposition}
\begin{proposition}
	Lösung von $Ax = b$ für $A = LL^\top$ wie gehabt durch $Ly = b$ und $L^\top x = y$.
\end{proposition}
\subsection{Matrixnormen}
Bekannt sind die üblichen Vektornormen auf $\bR^n$, insbesondere 
\begin{align*}
	\norm{\vv}_p = \left(\sum_{j = 1}^n \abs{v_j}^p\right)^{\frac{1}{p}}
\end{align*}
und
\begin{align*}
	\norm{\vv}_\infty = \max v_j
\end{align*} 
Für $1 \leq p,q \leq \infty$ existiert eine Konstante $c_{pqn}$, sodass
\begin{align*}
	\forall \vv \in \bR^n : \frac{1}{c_{pqn}}\norm{\vv}_p \leq \norm{\vv}_q \leq c_{pqn}\norm{\vv}_p 
\end{align*}
\begin{definition}
	Für Normen $\norm{-}_{\bR^n}$ und $\norm{-}_{\bR^m}$ auf $\bR^n$ und $\bR^m$ definieren wir die Operatornorm auf $\hom(\bR^n, \bR^m) = \bR^{m \times n}$ als
	\begin{align*}
		\norm{A}_{op} = \sup_{x \in \bR^n : \norm{x}_{\bR^n} = 1} \norm{Ax}_{\bR^m}
	\end{align*}
\end{definition}
\begin{lemma}
	Die Operatornorm ist eine Norm.
\end{lemma}
\begin{proof}
	\begin{enumerate}
		\item Skalare können aus der inneren Norm und dem Supremum wie nötig herausgezogen werden.
		\item Das Supremum ist über einer Menge positiver Zahlen, falls $x \neq \vzero$ gibt es mindestens einen Vektor größer $0$.
		\item Dreiecksungleichung folgt aus der Dreiecksungleichung für $\norm{-}_{\bR^m}$.
	\end{enumerate}
\end{proof}
\begin{lemma}
	\begin{align*}
		\norm{A}_{op} = \inf \{c > 0 : \forall x \in \bR^n \norm{Ax} \leq c\norm{x}\}
	\end{align*}
\end{lemma}
\begin{lemma}
	Für $A \neq 0$ und $x \in \bR^n$ mit $\norm{x} \leq 1$ und $\norm{Ax} = \norm{A}_{op}$ folgt $\norm{x} = 1$
\end{lemma}
\begin{corollary}
	Für alle $x \in \bR^n$ gilt $\norm{Ax} \leq \norm{A}_{op}\norm{x}$
\end{corollary}
\begin{lemma}
	Es gibt Vektoren, sodass die Matrixnorm ihr $\inf$ und ihren $\sup$ annimmt.
\end{lemma}
\begin{proof}
	Es handelt sich um eine stetige Funktion auf einem Kompaktum.
\end{proof}
\begin{example}
\begin{enumerate}
	\item Die \tbf{Spaltensummennorm} $\norm{-}_1$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{1} = \max_{j = 1, \hdots, n} \sum_{i = 1}^m \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Zeilensummennorm} $\norm{-}_\infty$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{\infty} = \max_{i = 1, \hdots, m} \sum_{j = 1}^n \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Spektralnorm} $\norm{-}_2$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_2 = \rho(A^\top A) = (\max \{\abs{\lambda} : \lambda \textnormal{ ist Eigenwert von $A^\top A$}\})^\frac{1}{2}
	\end{align*}
\end{enumerate}
\end{example}
\begin{lemma}
	Für $A \in \bR^{l \times m}$, $B \in \bR^{m \times n}$ und eine beliebige Operatornorm $\norm{-}$ gilt $\norm{AB} \leq \norm{A}\norm{B}$
\end{lemma}
\begin{proof}
	\begin{align*}
		&\norm{ABx} \leq \norm{A}\norm{Bx} \leq \norm{A}\norm{B}\norm{x}\\
		\implies &\norm{AB} \leq \norm{A}\norm{B}
	\end{align*}
\end{proof}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt
	\begin{align*}
		\norm{E_n} = 1
	\end{align*}
\end{lemma}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt für $A$ symmetrisch mit Eigenwert $\lambda$
	\begin{align*}
		\norm{A} \geq \abs{\lambda}
	\end{align*}
\end{lemma}
\begin{example}
	Die Frobeniusnorm $\norm{-}_\cF$ einer Matrix $A \in \bR^{m \times n}$ ist gegeben durch
	\begin{align*}
		\norm{A}_\cF = \left(\sum_{j = 1}^m \sum_{i = 1}^n a_{ij}^2\right)^\frac{1}{2}
	\end{align*}
\end{example}
\begin{lemma}
	Für $n > 1$ ist die Frobeniusnorm keine Operatornorm!
\end{lemma}
\begin{proof}
	\begin{align*}
			\norm{E_n} = \sqrt{n}
	\end{align*}
	Normieren wir die Norm, gilt
	\begin{align*}
		\frac{1}{\sqrt{2}}\norm{\begin{pmatrix}
				1 & 0\\
				0 & 0
		\end{pmatrix}}_{\cF}
		=
		\frac{1}{\sqrt{2}}
		< 
		1
	\end{align*}
	Wobei $1$ ein Eigenwert ist, was unseren vorherigen Lemmata widerspricht.
\end{proof}
\subsection{Konditionszahl}
\begin{theorem}
	Sei $\norm{-}$ eine Operatornorm auf $\bRnn$. Sei $A \in \bRnn$ regulär und seien $x, x', b, b' \in \bR^n$, sodass $Ax = b$, $Ax' = b'$. Dann gilt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\frac{\norm{b - b'}}{\norm{b}}
	\end{align*}
\end{theorem}
\begin{theorem}
	\begin{align*}
		\norm{x - x'} = \norm{A^{-1}(b - b')} \leq \norm{A^{-1}}\norm{b - b'}
	\end{align*}
	und
	\begin{align*}
		\norm{b} = \norm{Ax} \leq \norm{A}\norm{x}
	\end{align*}
	Es folgt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{A^{-1}}\norm{b}}
	\end{align*}
\end{theorem}
\begin{definition}
	Die \tbf{Konditionszahl} einer regulären Matrix $A \in \bRnn$ bezüglich der durch $\norm{-}$ auf $\bR^n$ induzierten Operatornorm ist gegeben durch:
	\begin{align*}
		\textnormal{cond}_{\norm{-}}(A) = \norm{A}\norm{A^{-1}}
	\end{align*}
\end{definition}
Wir schreiben oft $\textnormal{cond}_p$ statt $\textnormal{cond}_{\norm{-}_p}$.
\begin{lemma}
	$\textnormal{cond}(A) \geq 1$
\end{lemma}
\begin{lemma}
	Für $A$ symmetrisch mit Eigenwerten $\lambda_i$ gilt 
	\begin{align*}
		\cond_2(A) = \frac{\max \abs{\lambda_j}}{\min \abs{\lambda_j}}
	\end{align*}
\end{lemma}
\begin{example}
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Besitzt die Eigenwerte
	\begin{align*}
		\lambda_{1,2} = 1 + \frac{\epsilon}{2} \pm \left(1 + \frac{\epsilon^2}{4}\right)^\frac{1}{2}
	\end{align*}
	Also $\lambda_1 \approx 2 + \frac{\epsilon}{2}$ und $\lambda_2 \approx \frac{\epsilon}{2}$. Für $\epsilon \to 0$ geht also die Konditionszahl gegen unendlich.
\end{example}
\begin{theorem}
	Für $A$ symmetrisch und positiv definit mit Cholesky-Zerlegung $A = L L^\top$ gilt
	\begin{align*}
		\cond_2(L) = \cond_2(L^\top) = \sqrt{\cond(A)}
	\end{align*}
\end{theorem}
 \noindent Also kann das Problem, welches bei der $LU$-Zerlegung auftrat, bei der Cholesky-Zerlegung nicht vorkommen.
 \begin{proof}
 	Wir bemerken zunächst, dass $L^\top L$ und $LL^\top$ die selben Eigenwerte haben. Beide Matrizen sind symmetrisch und es gilt
 	\begin{align*}
 		\forall &x \in \bR^n, \lambda \in \bR : \\
 		&L^\top L x = \lambda x \Leftrightarrow  LL^\top Lx = \lambda(Lx) := \lambda y
 	\end{align*}
 	Somit folgt
 	\begin{align*}
 		\rho(LL^\top) = \rho(L^\top L)
 	\end{align*}
 	und somit auch
 	\begin{align*}
 		\norm{L}_2 = \norm{L^\top}_2
 	\end{align*}
 	analog gilt
 	\begin{align*}
 		\norm{L^{-1}}_2 = \norm{L^{-\top}}_2
 	\end{align*}
 	Also $\cond_2(L) = \cond_2(L^\top)$. Da $LL^\top = A$, und $A$ symmaterisch, folgt
 	\begin{align*}
 		\norm{L}_2^2 &= \norm{L^\top}_2^2\\
 					 &= \rho(LL^\top)\\
 					 &= \rho(A)\\ 
 					 &= \norm{A}_2
 	\end{align*}
 	und
 	\begin{align*}
 		\norm{L^{-1}}_2^2 &= \rho(L^{-\top}L^{-1})\\
 		 				  &= \rho(A^{-1})\\ 
 		 				  &= \norm{A^{-1}}_2,
 	\end{align*}
 	also insgesamt
 	\begin{align*}
 		\cond_2(L) &= \norm{L}_2\norm{L^{-1}}_2\\
 		           &= \norm{A}_2^{1/2} \norm{A^{-1}}_2^{1/2}\\ 
 		           &= \sqrt{\cond_2(A)}
 	\end{align*}
 \end{proof}
\chapter{Eliminationsverfahren}
 \section{Gauss-Jordan-Elimination}
 	\begin{definition}\theoremname{Gauss-Jordan-Elimination}
 		Sei $A \in \bRnn$, $b \in \bR^n$.
 		\begin{enumerate}
 			\item Setze $A^{(1)} = A$, $b^{(1)} = b$, $k = 1$.
 			\item Für $A^{(k)}$ gelte für $1 \leq j \leq k -1$ und $i \geq j + 1$ $a_{ij}^k = 0$, d.h. $A^{(k)}$ habe die Form
 			\begin{align*}
 				\begin{pmatrix}
 					a_{11}&\hdots&&&\hdots&a_{1n}\\
 					&a_{22}&&&&\vdots\\
 					&&\ddots\\
 					&&&a_{kk} & \hdots & a_{kn}\\
 					&&&\vdots & &\vdots\\
 					&&&a_{nk} & \hdots & a_{nn}\\
 				\end{pmatrix}
 			\end{align*}
 			mit Nullen im unteren linken Teil.
 			\item Wir setzen $\displaystyle l_{ik} = \frac{a_{ik}}{a_{kk}^{(k)}}$ und definieren $L \in \bRnn$ als
 			\begin{align*}
 				L =
 				\begin{pmatrix}
 					1&\hdots&&&\hdots&0\\
 					&1&&&&\vdots\\
 					&&\ddots\\
 					&&&1 & \hdots & 0\\
 					&&&-l_{k+1, k} & \hdots & 0\\
 					\vdots&&&\vdots & &\vdots\\
 					0&\hdots&&-l_{n,k} & \hdots 0 \hdots& 1\\
 				\end{pmatrix}
 			\end{align*}
 			\item Setze $A^{(k+1)} = L^{(k)}A^{(k)}$, $b^{(k+1)} = L^{(k)}b^{(k)}$
 			\item Stoppe, falls $k + 1 = n$, sonst erhöhe $k$ und gehe zu Schritt 2.
 		\end{enumerate}
 	\end{definition}
 	\begin{theorem}
 		Ist $A \in \bRnn$ regulär, so ist Gauß-Jordan-Elimination genau dann durchführbar, wenn $A$ eine $LU$-Zerlegung hat. Das Verfahren liefert dann die normierte $LU$-Zerlegung $U = A^{(n)}$ und $L = (L^{(n-1)} \cdot \hdots \cdot L^{(1)})^{-1}$. Die rechte Seite $y = b^{(n)}$ löst dann $y = L^{-1}b$ und die Lösung des Linearen Gleichungssystems $Ax = b$ ist gegeben durch die Lösung von $Ux = y$.
 	\end{theorem}
\section{Pivotsuche}
	Das Gaußverfahren ist für 
	\begin{align*}
	 	A = \begin{pmatrix}
	 		\epsilon & 1\\
	 		1 & 1\\
	 	\end{pmatrix}
	\end{align*}
	zwar durchführbar, aber instabil. Wir führen deswegen eine sogenannte Pivotsuche durch - im $k$-ten Schritt bestimmen wir $p \in \{k, \hdots, n\}$, sodass
	\begin{align*}
	 	\abs{a_{pk}^{(k)}} = \max a_{ik}^{(k)}
	\end{align*}
	und vertauschen dann die Zeilen $p$ und $k$ in $A^{(k)}$ und $b^{(k)}$. Wir müssen diese Vertauschung jedoch nicht im Speicher tatsächlich durchführen, es reicht, einen Permutationsvektor $\pi \in \bN^n$ vorzuschalten. Wir initialisieren $\pi$ als $(1,2,\hdots,n)^\top$, sollen daraufhin $k$ und $p$ vertauscht werden, vertauschen wir die jeweiligen Komponenten in $\pi$. Wollen wir daraufhin im Programm auf $a_{ij}$ Zugreifen, müssen wir stattdessen auf $a_{\pi(i) j}$ zugreifen.
	\begin{theorem}
		Ist $A \in \bRnn$ regulär, $b \in \bR^n$, so ist das Gaußverfahren mit Pivotsuche durchführbar und liefert die normalisierte $LU$-Zerlegung 
		\begin{align*}
			PA = LU
		\end{align*}
		mit
		$\abs{l_{ij}} \leq 1$
		für alle $i,j$, sowie die modifizierte rechte Seite $b^{(n)} = L^{-1}Pb$, wobei 
		\begin{align*}
			P = P^{(n-1)} P^{(n-2)} \hdots P^{(1)}
		\end{align*}
	\end{theorem}




\chapter{Ausgleichsprobleme}
\begin{example}
	Zu Messdaten $t_i, y_i, i = 1, \hdots, m$ wird die Ausgleichsgerade gesucht, also die Gerade definiert durch $c,b \in \bR$, sodass der Least-Squares Abstand
	\begin{align*}
		\sum_{i = 1}^n \lr((c \cdot t_i + b) - y_i)^2
	\end{align*}
	zu den Messdaten minimiert wird.
\end{example}
\noindent Im Allgemeinen haben solche Probleme die Form
\begin{align*}
	\min (x \mapsto \norm{Ax - b}_2^2).
\end{align*}
Ist $A$ eine reguläre $n \times n$-Matrix, so wird das Problem eindeutig durch $x = A^{-1}b$ gelöst. Typischerweise ist dies aber nicht der Fall - in der Praxis sind die meisten Gleichungssysteme überbestimmt.

\section{Die Gaußsche Normalengleichung}
\begin{definition}
	Durch $A \in \bR^{m \times n}, b \in \bR^m$ wird das Ausgleichsproblem 
	\begin{align*}
		\min (x \mapsto \norm{Ax - b}_2^2).
	\end{align*}
	definiert. Für $x \in \bR^n$ heißt
	\begin{align*}
		r= b - Ax
	\end{align*}
	das \tbf{Residuum} von $x$.
\end{definition}
\begin{theorem}
	Die Lösungen des Ausgleichsproblems sind genau die Lösung der \tbf{Gaußschen Normalengleichung}
	\begin{align*}
		A^\top Ax = A^\top b.
	\end{align*}
	Insbesondere existiert immer eine Lösung $x \in \bR^n$. Ist $z \in \bR^n$ eine weiter Lösung, so gilt $Ax = Az$ und die dazugehörigen Residuen stimmen überein.
\end{theorem}
\noindent Diese Normalengleichung erhält man durch Ableitung der Residuenfunktion $Ax-b$ nach $x$.
\newpar
\begin{proof}
	Aus der linearen Algebra ist bekannt, dass
	\begin{align*}
		\bR^m = \im(A) + \ker(A^\top),
	\end{align*}
	wobei diese Zerlegung direkt und orthogonal ist. Damit existieren zu $b \in \bR^m$ eindeutig bestimmte Vektoren $y \in \im(A)$, $r \in \ker(A^\top)$, sodass $y \cdot r = 0$ und $b = r + y$. Weiter existiert $x \in \bR^n$ mit $y = Ax$.
	\newpar
	Es folgt
	\begin{align*}
		A^\top b = A^\top y + A^\top r = A^\top Ax + 0 = A^\top Ax.
	\end{align*}
	Somit löst $x$ die Gaußsche Normalengleichung. Es bleibt zu zeigen, dass $x$ auch das Ausgleichsproblem löst. Sei $z \in \bR^n$. So rechnen wir
	\begin{align*}
		\norm{b-Az}_2^2 &= \norm{(b - Ax) + A(x-z)}_2^2\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2 + 2r \cdot (Ax - z)\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2 + \underbrace{2A^\top r}_{= 0} \cdot (x - z)\\
						&= \norm{b - Ax}_2^2 + \norm{A(x-z)}_2^2\\
						&\geq \norm{b - Ax}_2^2
	\end{align*}
	somit gilt insbesondere Gleichheit genau dann, wenn $Ax = Az$.
\end{proof}
\begin{lemma}
	Die Matrix $A^\top A$ ist symmetrisch und positiv semidefinit. Weiter ist $A$ genau dann positiv definit, wenn $\ker A = \{0\}$. In diesem Fall sind die Lösungen der Gaußschen Normalengleichung eindeutig.
\end{lemma}
\begin{proof}
	Symmetrie ist offensichtlich. Positive Semidefinitheit gilt, da
	\begin{align*}
		x(A^\top A) x = (Ax) \cdot (Ax) = \norm{Ax}_2^2 \geq 0.
	\end{align*}
	Insbesondere gilt also Gleichheit genau dann, wenn $Ax = 0$, also wenn $x \in \ker A$.
	\newpar
	Die Eindeutigkeit der Lösung der Gaußschen Normalengleichung folgt aus der Regularität positiv definiter Matrizen. 
\end{proof}
\noindent Für $m = n$, $A \in \bRnn$ gilt
\begin{align*}
	\cond_2 (A^\top A) = \frac{\lambda_{\max}(A^\top A)}{\lambda_{\min}(A^\top A)} = (\cond_2(A))^2
\end{align*}
da $\cond_2 (A) \geq 1$ ist somit $A^\top A$ immer schlechter konditioniert als $A$. Die Lösung eines Ausgleichsproblems durch die Gaußsche Normalengleichung ist somit instabil.

\section{Householder-Matrizen}
Sei $Q \in O(n)$ (also eine $n \times n$-Orthogonalmatrix). So gilt $\norm{Q(Ax - b)}_2^2 = \norm{Ax-b}$. Wir versuchen, eine orthogonale Matrix $Q$ so zu konstruieren, dass $QA$ Dreiecksgestalt hat.
\newpar
\begin{lemma}
	Für alle Orthogonalen Matrizen $Q$ gilt $\cond_2(Q) = 1$
\end{lemma}
\begin{proof}
	\begin{align*}
		\norm{Q}_2 \norm{Q^\top}_2 = \norm{Q}_2 \norm{Q^{-1}}_2 = 1
	\end{align*}
\end{proof}
\begin{definition}
	Für $v \in \bR^l$ mit $\norm{v}_2 = 1$ heißt die Matrix
	\begin{align*}
		P_v = E_l - 2vv^\top
	\end{align*}
	Die Householder-Transformation zu $v$.
\end{definition}
\noindent $(vv^\top)x$ entspricht der Projektion von $x$ auf den von $v$ aufgespannten Vektorraum. Insgesamt spiegelt die Householder-Transformation also $x$ an der Ursprungsebene orthogonal zu $v$.
\begin{lemma}
	$P_v$ ist symmetrisch und orthogonal. Außerdem gilt $P_vv = -v$ und
	\begin{align*}
		\forall w \in \bR^l : wv = 0 \implies P_v w = w
	\end{align*}
\end{lemma}
\begin{lemma}
	Sei $x \in \bR^l \neq 0, x \neq \lambda e_1$ und sei
	\begin{align*}
		\sigma = \begin{cases}
			\sgn(x_1) & x_1 \neq 0\\
			1 & \textnormal{sonst}
		\end{cases}
	\end{align*}  
	Setzen wir nun
	\begin{align*}
		v = \frac{x + \sigma \norm{x}_2 e_1}{\norm{x + \sigma \norm{x}_2 e_1}_2},
	\end{align*}
	so gilt
	\begin{align*}
		P_v x = (E_l x - 2vv^\top)x = -\sigma\norm{x}_2 e_1
	\end{align*}
\end{lemma}
\begin{proof}
	Da $x \neq \lambda e_1$ ist $v$ wohldefiniert mit $\norm{v}_2 = 1$. Weiter folgt
	\begin{align*}
		\norm{x + \sigma \norm{x}_2 e_1}_2^2 &= \norm{x}^2 + 2 \sigma\norm{x}_2 x \cdot e_1 + \sigma^2 \norm{x}_2^2 \norm{e_1}_2^2\\
		&= 2(x + \sigma \norm{x}_2 e_1)^\top x
	\end{align*}
	Mit $\tilde v = x + \sigma \norm{x}_2 e_1$ gilt
	\begin{align*}
		2\tilde v ^\top x &= 2(x + \sigma \norm{x}_2 e_1)^\top x\\
		&= \norm{x + \sigma \norm{x}_2 e_1}_2^2\\
		&= \norm{\tilde v}^2_2
	\end{align*}
	Es gilt $v = \frac{\tilde v}{\norm{v}_2}$, also
	\begin{align*}
		P_vx &= (E_l - 2vv^\top)x\\
		     &= x - 2v\frac{\tilde v^\top x}{\norm{\tilde v}_2}\\
		     &= x - v\frac{\norm{\tilde v}_2^2}{\norm{\tilde v}_2}\\
		     &= x - v\norm{\tilde v}_2\\
		     &= - \sigma\norm{x}_2 e_1
	\end{align*}
\end{proof}
\noindent $\sigma$ verhindert hier sogenannte Auslöschungseffekte, also schlechte Konditionierung der Subtraktion zweier fast identischer Zahlen.
\section{$QR$-Zerlegung}
\begin{theorem}
	Sei $A \in \bR^{m \times n}$, $m \geq n$, $\rang A = n$. So existiert $Q \in O(m)$ und eine verallgemeinerte obere Dreiecksmatrix $R \in \bR^{m \times n}$, sodass
	\begin{align*}
		A = QR
	\end{align*}
	Außerdem gilt $\forall i : \abs{r}_{ii} > 0$
\end{theorem}
\begin{proof}
	Wir setzen $A_1 = A$, und es sei $x = a_1 \in \bR^m$ die erste Spalte von $A_1$. Falls $x \in \bR{e_1}$, setzen wir $Q_1 = E_m$. Ansonsten sei
	\begin{align*}
		Q_1 = P_v
	\end{align*}
	mit $v$ wie im Lemma. Es folgt
	\begin{align*}
		Q_1 a_1 = r_{11}e_1
	\end{align*}
	mit $\abs{r_{11}} = \norm{a_1}_2 \geq 0$. Somit folgt
	\begin{align*}
		Q_1 A_1 = 
		\begin{pmatrix}
			r_{11} & r_1^\top\\
			\vzero & A_2
		\end{pmatrix}
	\end{align*}
	mit $A_2 \in \bR^{(m-1) \times (n-1)}$.
	Wir setzen nun
	\begin{align*}
		\tilde Q_2 A_2 = 
		\begin{pmatrix}
			r_{12} & r_2^\top\\
			\vzero & A_3
		\end{pmatrix}
	\end{align*}
	und
	\begin{align*}
		Q_2 = \begin{pmatrix}
			1 & \vzero^\top\\
			\vzero & \tilde Q_2
		\end{pmatrix}
	\end{align*}
	Die Matrix $Q_2$ ist orthogonal, insbesondere ist sie die Householder-Matrix zu $v = \binom{0}{\tilde v}$ mit $\tilde v$, wobei $\tilde v$ der Vektor ist, der $\tilde Q_2$ als Householder-Matrix gibt.
	\newpar
	Nach $n$ solchen Schritten erhalten wir $QA := (Q_nQ_{n-1} \hdots Q_1) A = R$. Da $Q$ ein Produkt orthogonaler Matrizen ist gilt insbesonderee $Q \in O(m)$. Die Einträge erfüllen $r_{ii} = \norm{a_i}_2 > 0$, da die Matrix $A$ vollen Rang hat.
\end{proof}
\begin{anmerkung}
	Im Fall $m = n$ ist die Faktorisierung abgesehen von Vorzeichen der Diagonaleinträge von $R$ eindeutig, denn falls $A = QR = Q'R'$, so folgt mit
	\begin{align*}
		E := (Q')^{-1}Q = R'R^{-1}
	\end{align*}
	Dass $E$ eine orthogonale obere Dreiecksmatrix ist. Die orthogonalen Dreiecksmatrizen sind genau die Diagonalmatrizen mit Einträgen $\pm 1$. Damit folgt aber $Q = Q'E$ und $R = ER'$.
\end{anmerkung}
\begin{anmerkung}
	Für die Anwendung einer Householder-Transformation kann jede Matrixmultiplikation als Householder-Transformation dargestellt werden, was wesentlich schneller als allgemeine Matrixmultiplikation ist:
	\begin{align*}
		P_vA &= (E_m - 2vv^\top)A\\ 
			   &= A - 2v(v^\top A)\\
	\end{align*}
\end{anmerkung}
\begin{anmerkung}
	Die Vektoren $v$ zu den Householder-Transformationen lassen sich in den frei werdenden Einträgen von $A$ speichern. Weiter gilt
	\begin{align*}
		Q = \prod_{i = 1}^n (E_m - 2v_i v_i^\top)
	\end{align*}
\end{anmerkung}
\begin{anmerkung}
	Der Aufwand ist $\cO(n^3)$.
\end{anmerkung}
\section{Lösung des Ausgleichsproblems}
Mithilfe der $QR$-Zerlegung bekommen wir ein stabiles Verfahren zur Lösung von Ausgleichsproblemen.
\begin{theorem}
	Sei $A \in \bR^{m \times n}$, $m \geq n$, $\rang A = n$. Sei $A = QR$ und $Q^\top b = \binom{c}{d}$, $Q^\top A = R = \binom{\hat R}{0}$ mit $c \in \bR^n$, $d \in \bR^{m-n}$, $\hat R = \bR^{n \times n}$ obere Dreiecksmatrix. So ist die Lösung des Anfangswertproblems gegeben durch
	\begin{align*}
		\hat R x = c
	\end{align*}
\end{theorem}
\begin{anmerkung}
	Da $\cond_2(Q) = 1$ folgt für reguläre $A \in \bR^{n \times n}$ direkt 
	\begin{align*}
		\cond_2(R) = \cond_2(A).
	\end{align*}
	Somit ist insbesondere unser Algorithmus stabil.
\end{anmerkung}
\section{Singulärwertzerlegung}
	Wir betrachten $A^\top A \in \bRnn$, $A \in \bR^{m \times n}$.
	\newpar
	Dank Symmetrie ist $A^\top A$ diagonalisierbar, und es existiert eine Orthonormalbasis aus Eigenvektoren $v_1, \hdots, v_n$ mit Eigenwerten $\lambda_1 \geq \hdots \geq \lambda_p > \lambda_{p + 1} = \hdots = \lambda_{n} = 0$. Für $i \in 1, \hdots p$ setzten wir
	\begin{align*}
		u_i = \frac{1}{\sqrt{\lambda_i}} A v_i
	\end{align*} 
	Dann gilt für $i,j \in 1, \hdots, p$, dass
	\begin{align*}
		u_i^\top u_j &= \frac{1}{\sqrt{\lambda_i \lambda_j}}(Av_i)^\top Av_j\\
					 &= \frac{1}{\sqrt{\lambda_i \lambda_j}} v_i^\top (A^\top A)v_j\\
					 &= \frac{\lambda_j}{\sqrt{\lambda_i \lambda_j}} v_i^\top v_j\\
					 &= \delta_{ij}
	\end{align*}
	Der letzte Schritt folgt, da für $i \neq j$ alles sowieso $0$ ist und für $i = j$ auch $\lambda_i = \lambda_j$ gilt und sich dann die Lambdas kürzen.
	\newpar
	Die Vektoren $u_1, \hdots, u_p$ bilden also eine Orthonormalbasis von $\Im A$.
	Wir ergänzen zu einer Orthonormalbasis $u_1, \hdots, u_m$ des $\bR^m$. Dann gilt
	\begin{align*}
		A^\top u_i = \frac{1}{\sqrt{\lambda_i}} A^\top A v_i = \sqrt{\lambda_i} v_i
	\end{align*}
	Für $1 \leq i \leq p$. Für $i \geq p+1$ müssen die $u_i$ im Kern von $A$ liegen, also gilt die Gleichheit ebenfalls. Wir setzten $\sigma_i = \sqrt{\lambda_i}$ für $i = 1, \hdots, p$ und bekommen:
	\begin{theorem}
		\theoremname{Singulärwertzerlegung:} Sei $A \in \bR^{m \times n}$. Dann existieren Zahlen $\sigma_1 \geq \hdots \geq \sigma_p$ und Orthonormalbasen $(u_i)_{i = 1}^m$ des $\bR^m$ und $(v_i)_{i = 1}^n$ des $\bR^n$, sodass für alle $1 \leq i \leq p$:
		\begin{align*}
			Av_i = \sigma_i u_i\\ A^\top u_i = \sigma_i v_i
		\end{align*}
		und für alle $p+1 \leq j \leq n$ und $p+1 \leq k \leq m$
		\begin{align*}
			Av_j = 0\\ A^\top u_k = 0.
		\end{align*}
		Die Zahlen $\sigma_i^2$ sind genau die von Null verschiedenen Eigenwerte von $A^\top A$. Für
		\begin{align*}
			U = (u_1, \hdots, u_m) \in \bR^{m \times m},\\
			V = (u_1, \hdots, u_n) \in \bR^{n \times n}
		\end{align*}
		gilt $U \in O(m)$, $V \in O(n)$. Ist $\Sigma$ die Diagonalmatrix, die die $\sigma$ in absteigender Reihenfolge enthält, gilt
		\begin{align*}
			A = U \Sigma V^\top
		\end{align*}
	\end{theorem} 
	\begin{proofsketch}
		Folgt direkt aus der Konstruktion :)
	\end{proofsketch}
	\section{Pseudoinverse}
	\begin{definition}
		Ist $A = U \Sigma V^\top$ die Singulärwertzerlegung von $A$ und $\Sigma^+ \in \bR^{n \times m}$ gegeben durch Inversion der Einträge ungleich Null, dann heißt
		\begin{align*}
			A^+ = V \Sigma^+ U^\top = \sum_{i = 1}^p \sigma^{-1}_i v_i u_i^\top
		\end{align*}
		die \tbf{Pseudoinverse} oder \tbf{Moore-Penrose-Inverse} von $A$.
	\end{definition}
	\begin{anmerkung}
		Es gilt $\ker A^+ = \ker A^\top$ und $\Im A^+ = \Im A^\top$.
	\end{anmerkung}
	\begin{anmerkung}
		Die Pseudoinverse ist die eindeutige Lösung des Gleichungssystems
		\begin{align*}
			AXA &= A,\\
			XAX &= X,\\
			(AX)^\top &= AX\\
			(XY)^\top &= XA
		\end{align*}
	\end{anmerkung}
	\begin{theorem}
		Der Vektor $A^+ b$ löst das Ausgleichsproblem $\min \norm{Ax - b}_2^2$ und ist von allen Lösungen diejenige mit minimaler euklidischer Norm.
	\end{theorem}
	\begin{proof}
		Mit $A^+A A^+ = A^+$ und $\ker A^+ = (\Im A)^\bot$ folgt 
		\begin{align*}
			A A^+ b - d \in \ker A^+ = (\Im A)^\bot = \ker A^\top,
		\end{align*}
		also
		\begin{align*}
			A^\top A(A^+ b) = A^\top b,
		\end{align*}
		damit ist aber $A^+b$ eine Lösung der Gaußschen Normalengleichung.
		\newpar
		Falls $z \in \bR^n$ eine weitere Lösung ist, so gilt $\ker A^\top A = \ker A$, also
		\begin{align*}
			A^\top A(A^+ b) - z = 0 \Leftrightarrow A(A^+b - z) = 0
		\end{align*}
		Wir setzen nun $w = A^+ b - z \in \ker A$. Es gilt $A^+ b \in \Im A^+ = (\ker A)^\bot$, also folgt
		\begin{align*}
			(A^+ b)w = 0
		\end{align*}
		Für $z = A^+ b - w$ gilt nun
		\begin{align*}
			\norm{z}_2^2 = \norm{A^+ b}_2^2 + \norm{w}_2^2,
		\end{align*}
		da der gemischte Term $\norm{(A^+b)w}_2^2$ wegfällt. Somit ist $A^+b$ die Lösung mit minimaler Norm.
	\end{proof}
	\chapter{Eigenwertaufgaben}
	Im Prinzip gibt es die Möglichkeit, Nullstellen des charakteristischen Polynoms zu suchen, zum Beispiel durch das sog. \tit{Newton-Raphson-Verfahren}. Das ist aber typischerweise nicht praktikabel.
	\section{Abschätzungen}
	\begin{theorem}
		Sei $A \in \bR^{n \times n}$ und $\lambda \in \bC$ ein Eigenwert von $A$. Dann gilt
		\begin{align*}
			\lambda \in \bigcup_{i = 1}^n K_i,
		\end{align*}
		wobei
		\begin{align*}
			K_i = \lr{z \in \bC : \abs{z - a_{ii}} \leq \sum_{j = i, i \neq j}^n \abs{a_{ij}}}.
		\end{align*}
		Die Mengen $K_i$ heißen \tbf{Gerschgorin-Kreise}.
	\end{theorem}
	\begin{proof}
		Es sei $Ax = \lambda x$ für ein $x \in \bC^n \setminus \lr{0}$. Dann existiert ein maximaler Eintrag $i \in 1, \hdots, n$, also $\abs{x_j} \leq \abs{x_i}$ für alle $j \in 1, \hdots,n$ und $x_i \neq 0$. Dann gilt
		\begin{align*}
			\lambda x_i = (Ax)_i = \sum_{j = 1}^n a_{ij}x_j.
		\end{align*}
		Wir teilen durch $x_i \neq 0$ und erhalten
		\begin{align*}
			\lambda - a_{ii} = \sum_{j = 1, i \neq j}^n a_{ij} \frac{x_j}{x_i}.
		\end{align*}
		Durch Dreiecksungleichung und $\frac{\abs{x_j}}{x_i} \leq 1$ folgt $\lambda \in K_i$ und damit die Behauptung.
	\end{proof}
	\begin{theorem}
		Sei $a \in \bRnn$ symmetrisch, also alle Eigenwerte reell. Für den maximalen und den minimalen Eigenwert von $A$ gilt dann:
		\begin{align*}
			\lambda_{max} = \max_{x \in \bR^n \setminus \lr{0}} \frac{x^\top A x}{\norm{x}^2_2}
		\end{align*}
		\begin{align*}
			\lambda_{min} = \min_{x \in \bR^n \setminus \lr{0}} \frac{x^\top A x}{\norm{x}^2_2}
		\end{align*}
		Diese Brüche sind auch als die \tbf{Rayleigh-Quotienten} bekannt.
	\end{theorem}
	\begin{proof}
		Sei $(v_i) \subset \bR^n$ eine Orthonormalbasis des $\bR^n$ aus Eigenvektoren zu den Eigenwerten $\lambda_1 \geq \lambda_2 \geq \hdots \geq \lambda_n \in \bR$ der Matrix $A$. Wir schreiben $x \in \bR^n$ als
		\begin{align*}
			x = \sum_{i = 1}^n \alpha_i v_i
		\end{align*}
		mit $\alpha_i \in \bR$. So gilt
		\begin{align*}
			Ax = \sum_{i = 1}^n \alpha_i \lambda_i v_i
		\end{align*}
		Dank Orthonormalität der $v_i$ folgt
		\begin{align*}
			x^\top x = \sum_{i,j = 1}^n \alpha_i \alpha_j \scalar{v_i}{v_j} = \sum_{i = 1}^n \alpha_i^2
		\end{align*}
		und insbesondere
		\begin{align*}
			x^\top Ax  = \sum_{i = 1}^n \lambda_i \alpha_i^2 
		\end{align*}
		daraus folgt
		\begin{align*}
			x^\top Ax &\geq \lambda_n \sum_{i = 1}^n \alpha_i^2\\
			 		  &= \lambda_n \norm{x}_2^2\\
			\implies \frac{x^\top A x}{\norm{x}_2^2} &\geq \lambda_n
		\end{align*}
		und
		\begin{align*}
			x^\top Ax &\leq \lambda_1 \sum_{i = 1}^n \alpha_i^2\\
			 		  &= \lambda_1 \norm{x}_2^2\\
			\implies \frac{x^\top A x}{\norm{x}_2^2} &\leq \lambda_1
		\end{align*}
	\end{proof}
	\section{Konditionierung des Eigenwertproblems}
	\begin{theorem}
		Sei $A \in \bRnn$ komplex diagonalisierbar mit $A = VDV^{-1}$. Sei $E \in \bRnn$ eine beliebige "Störungsmatrix" und sei $\bar \lambda \in \bC$ ein Eigenwert von $A + E$. Dann existiert ein komplexer Eigenwert $\lambda$ von $A$, sodass
		\begin{align*}
			\abs{\bar \lambda - \lambda} \leq \cond_2(V)\norm{E}_2
		\end{align*}
	\end{theorem}
	\begin{definition}
		Die Abschätzung lässt sich auch durch den sog. \emph{Hausdorff-Abstand}
		schreiben. Für einen metrischen Raum $M$ mit Metrik $d$ und Teilmengen $A,B$ ist dieser definiert als
		\begin{align*}
			d_H(A,B) := \max \lr{\sup_{x \in A} \inf_{y \in B} d(x,y), \sup_{y \in A} \inf_{x \in B} d(x,y)}
		\end{align*}
		Damit ist
		\begin{align*}
			d_H(\Sigma, \bar \Sigma) \leq \cond_2(V) \norm{E}_2
		\end{align*}
		wobei $\Sigma$ die Menge der Eigenwerte (das \tit{Spektrum}) von $A$ ist und $\bar \Sigma$ das Spektrum von $A - E$.
	\end{definition}
	\begin{anmerkung}
		Nicht jede Matrix ist komplex diagonalisierbar.
	\end{anmerkung}
	\begin{corollary}
		Jede Matrix $A$, sodass $A^\top A = A A^\top$ (also jede sog. \emph{Normale Matrix}) ist komplex diagonalisierbar. In diesem Fall ist $V$ unitär, also insbesondere $\cond(V)_2 = 1$. Somit gilt sogar
		\begin{align*}
			\abs{\lambda - \bar\lambda} \leq \norm{E}_2
		\end{align*}
	\end{corollary}
	\begin{proposition}
		Sei $p(t) = t^n + \sum_{i = 0}^{n-1} a_i t^i$ ein normalisiertes Polynom. So gilt
		\begin{align*}
			p(t) = (-1)^n \det(A - tE_n)
		\end{align*}
		mit der sogenannten \tbf{Frobenius-Begleitmatrix} $A$, deren Einträge überall $0$ sind, außer auf der Subdiagonalen, wo sie $1$ sind, und in der letzten Spalte, in der der $i$-te Eintrag (wobei wir bei $1$ mit dem Zählen anfangen) genau $-a_{i-1}$ ist. Die komplexen Eigenwerte von $A$ sind genau die komplexen Nullstellen von $p$.
	\end{proposition}
	\begin{corollary}
		Da das Finden von Nullstellen eines Polynoms im Allgemeinen schlecht konditioniert ist, ist auch das Finden von Eigenwerten von Matrizen schlecht konditioniert.
	\end{corollary}
	\begin{example}
		Das Polynom $p_{\epsilon}(t) = (t-a)^n - \epsilon$ besitzt die Nullstellen 
		\begin{align*}
			\chi_k = a - \epsilon ^ \frac{1}{n} e^{i2\pi \frac{k}{n}}
		\end{align*}
		Die Polynome $p_0$, $p_\epsilon$ Unterscheiden sich nur im konstanten Koeffizienten, und für die dazugehörigen Begleitmatrizen $A_0$ und $A_\epsilon$ gilt
		\begin{align*}
			\norm{A_0 - A_\epsilon} = \epsilon
		\end{align*}
		die Nullstellen unterscheiden sich jedoch durch
		\begin{align*}
			\abs{\lambda - \bar \lambda} = \epsilon ^{\frac{1}{n}},
		\end{align*}
		der relative Fehler ist letzendlich
		\begin{align*}
			\frac{\abs{\lambda - \lambda_k}}{\abs{\lambda}} \sim \frac{\epsilon^{\frac{1}{n}}}{\epsilon}
		\end{align*}
		und dieser Faktor wächst für $n > 1$ und $\epsilon \to 0$ unbeschränkt.
	\end{example}
	\section{Potenzmethode}
	\begin{proposition}
		Sei $A \in \bRnn$ reell diagonal mit Eigenwerten $\lambda_1, \hdots, \lambda_n \in \bR$ und linear unabhngigen Eigenvektoren $v_1, v_2, \hdots, v_n \in \bR^n$ mit $\norm{v_j}_2 = 1$. Sei
		\begin{align*}
			x = \sum_{j = 1}^n \alpha_j v_j.
		\end{align*}
		ein beliebiger Vektor.
		Dann gilt
		\begin{align*}
			A^k = A^{k-1}(\sum_{j = 1}^n \lambda_j \alpha_j v_j) = \hdots = \sum_{j = 1}^n \lambda_j^k \alpha_j v_j
		\end{align*}
		Ist $\lambda_1$ der betragsmäßig größte Eigenwert, so folgt für $k$ hinreichend groß
		\begin{align*}
			A^k x \approx \alpha_1 \lambda^k v_1
		\end{align*}
		Da $\norm{v_1}_2 = 1$ folgt
		\begin{align*}
			\frac{\norm{A^{k+1}x}_2}{\norm{A^kx}_2} \approx \abs{\lambda_1}
		\end{align*}
	\end{proposition}
	\begin{proposition}
		\theoremname{(von Mises-Potenzmethode)}
		Sei $A \in \bRnn$, $x \in \bR^n \setminus \lr{0}$, $\epsilon \in \bR_{>0}$. Wir setzen $x_0 = \frac{x}{\norm{x}_2}$, $\mu_0 = 0$, $k = 0$.
		\begin{enumerate}
			\item Berechne $x_{k+1} = Ax_k$, $\mu_{k+1} = \norm{x_{k+1}}_2$, $x_{k+1} = \frac{x_{k+1}}{\mu_{k+1}}$.
			\item Falls $\abs{\mu_{k+1} - \mu_k} < \epsilon$ wird abgebrochen. Ansonsten setzte $k \gets k + 1$ und gehe zu Schritt 1.
		\end{enumerate} 
	\end{proposition}
	\begin{theorem}
		Sei $\abs{\lambda_1} \geq \hdots \geq \abs{\lambda_n} \geq 0$ und $x = \sum_{i = 1}^n \alpha_i v_i$ mit normierten Eigenvektoren $v_i$ von $A$.
		\newpar
		Falls $\alpha_1 \neq 0$, so folgt  mit $q = \abs{\frac{\lambda_2}{\lambda_1}} < 1$ und einem hinreichend großem $k$
		\begin{align*}
			\abs{\norm{Ax_k}_2 - \abs{\lambda_1}} \leq 4\norm{A}_2 \cdot c \cdot q^k
		\end{align*}
	mit $c$ unabhängig von $k$.
	\end{theorem}
	\begin{anmerkung}
		In jedem Schritt verringert sich der Fehler um einen Faktor $q < 1$. Das Verfahren konvergiert somit \tit{in Ordnung $1$} (Siehe später in Numerik 2) - das ist vergleichsweise sehr langsam.
	\end{anmerkung}
	\begin{anmerkung}
		Wir sehen, dass $\lambda_1 < 0$ genau dann, wenn für hinreichend großes $k$ $x_k \approx -x_{k+1}$ gilt, und dass die Folge $x_k$ abgesehen vom Vorzeichen gegen den Eigenvektor $v_1$ konvergiert.
	\end{anmerkung}
	\begin{theorem}
		Für $A \in \bRnn$ symmetrisch gilt unter den gleichen Vorraussetzungen sogar
		\begin{align*}
			\abs{\lambda_1 - x_k^\top Ax_k} \leq 2 \norm{A}_2 c^2 q^{2k}
		\end{align*}
	\end{theorem}
	\begin{anmerkung}
		Falls $0 < \abs{\lambda_n} < \hdots \leq \abs{\lambda_1}$, so liefert die Potenzmethode mit $A^{-1}$ statt $A$ eine Approximation von $\frac{1}{\abs{\lambda_n}}$
	\end{anmerkung}
	\begin{anmerkung}
		Wendet man die Methode auf $A - \mu E_n)^{-1}$ an, so kovergiert die Methode unter geeigneten Vorraussetzungen gegen den Eigenwert, der am nächsten an $\mu$ liegt.
	\end{anmerkung}
	\begin{anmerkung}
		Die Methode funktioniert auch bei wiederholtem größten Eigenwert $\lambda_1 = \lambda_2$.
	\end{anmerkung}
	\section{Das $QR$-Verfahren}
	\begin{proposition}
		\theoremname{($QR$-Verfahren)}
		Sei $A \in \bRnn$, $A_0 = A$, $k = 0$, $\epsilon \in \bR_{>0}$.
		\begin{enumerate}
			\item Bestimmt die $QR$-Zerlegung $A_k = Q_k R_k$
			\item Setze $A_{k+1} = R_k Q_k$
			\item Stoppe, falls $\norm{A_{k+1} - A_k} \leq \epsilon$. Ansonsten setze $k \gets k+1$ und gehe zu Schritt $1$.
		\end{enumerate}
	\end{proposition}
	\begin{lemma}
		Es gilt 
		\begin{align*}
			A_{k+1} = Q_k^\top A_k Q_k = (Q_0 \cdot \hdots \cdot Q_k)^\top A (Q_0 \cdot \hdots \cdot Q_k)
		\end{align*}
		\begin{align*}
			A^{k+1} = (Q_0 \cdot \hdots \cdot Q_k)(R_k \cdot \hdots \cdot R_0)
		\end{align*}
	\end{lemma}
	Zur Motivation des $QR$-Verfahrens betrachten wir die erste Spalte von $A^{k+1} = (Q_0\hdots Q_k)(R_k \hdots R_0) := \ol Q_k \ol R_k$:
	\begin{align*}
		A^{k+1}e_1 &= \ol{Q}_k \ol R_k e_1 \\
				   &= \ol Q_k \ol r_{11}^{(k)}e_1\\
				   &= \ol r_{11}^{(k)} \ol Q_k e_1\\
				   &= \ol r_{11}^{(k)} \ol q_1^{(k)}\\
	\end{align*}
	Mit den Ideen der von-Mises-Potenzmethode ist anzunehmen, dass $\ol q_1^{(k)}$ für große $k$ eine gute Näherung an den Eigenvektor zum betragsmäßig größten Eigenwert $\lambda_1$ ist. $\ol q_1^{(k)}$ hat außerdem als Spalte einer Orthogonalmatrix die Länge $1$. Wir rechnen
	\begin{align*}
		A_{k+1} e_1 &= \ol Q_k^\top A \ol Q_k e_1\\
					&= \ol Q_k^\top A \ol q_1^{(k)}\\
					&\approx \ol Q_k^\top \lambda_1 q_1^{(k)}\\
					&= \lambda_1 \ol Q_k^\top \ol q_1^{(k)}\\
					&= \lambda_1 e_1
	\end{align*}
	Angenommen, $A$ sei invertierbar. Wir erhalten
	\begin{align*}
		\ol Q_k^\top = \ol R_k A^{-(k+1)}
	\end{align*}
	Multiplikation mit $e_n^\top$ von Links liefert:
	\begin{align*}
		q_n^{(k)^\top} &= e_n^\top \ol Q_k^\top \\
				       &= e_n^\top \ol R_k A^{-(k+1)}\\
				       &= \ol r_{nn}^{(k)} \cdot e_n^\top A^{-(k+1)}\\
				       &= \ol r_{nn}^{(k)} \cdot \lr(\lr(A^{-(k+1)})^\top e_n)^\top\\
				       &= \ol r_{nn}^{(k)} \cdot v_n^\top
	\end{align*}
	Wobei $v_n^\top$ eine Approximation des Eigenvektors zum betragsmäßig kleinsten Eigenwert von $A^\top$ ist, was auch der kleinste Eigenwert von $A$ ist. Analog zum vorherigen Fall erhalten wir, dass die letzte Spalte von $A$ der Vektor $\lambda_n e_n$ ist.
	\begin{theorem}
		Sei $A \in \bRnn$ reell diagonalisierbar mit paarweise verschiedenen Eigenwerten $\abs{\lambda_1} > \abs{\lambda_2} > \hdots > \abs{\lambda_n} > 0$. Sei $\Lambda$ die zugehörige Diagonalmatrix und $X$ die Eigenvektormatrix, also $A = X \Lambda X^{-1}$. Angenommen, $X^{-1}$ besitze eine $LU$-Zerlegung. Dann konvergiert $A_k$ im $QR$-Verfahren gegen eine obere Dreiecksmatrix, deren Diagonale die Matrix $\Lambda$ ist.
	\end{theorem}
\end{document}