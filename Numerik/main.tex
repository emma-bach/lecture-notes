\documentclass{report}

% custom margins
\usepackage[a4paper,margin=1.5in]{geometry}
\renewcommand{\baselinestretch}{1.2}

% emma's long list of custom macros and universally used packages
\include{../macros-and-packages.tex}

% fancy bar next to proofs
\tcolorboxenvironment{proof}{
	colback=white,
	boxrule=0pt,
	leftrule=0.5mm,
	before skip=0.75cm,
	after skip=0.75cm,
	sharp corners,
	breakable,
	enhanced,
}

\usepackage{algorithm}
\usepackage{algpseudocode}

\renewcommand*\contentsname{Inhalt}
\renewcommand*\proofname{Beweis}

\pagestyle{fancy} %allows headers

\lhead{Emma Bach}
\rhead{\today}


\begin{document}
	\include{title}
	\tableofcontents
	\thispagestyle{fancy}
	\chapter{Aufgabenstellung}
	In der Numerik beschäftigt man sich mit der praktischen Berechnung von Lösungen mathematischer Probleme.
	\begin{example}
		Berechne $\displaystyle \int_0^1 e^{-x^2} dx$!
	\end{example}
	\begin{example}
		Berechne $\sin(20)$!
	\end{example}
	\begin{example}
		Berechne $\sqrt{753}$!
	\end{example}
	\begin{example}
		Berechne $\displaystyle \min_{x \in [0,1]} F(x)$, für eine geeignete Funktion $F$!
	\end{example}
	\begin{example}
		Berechne $x$, sodass $f(x) = 0$!
	\end{example}
	\begin{example}
		Berechne $x \in \bR^n$, sodass $Ax = b$!
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe in der Numerik besteht im Finden einer Lösung von
		\begin{align*}
			F(x,d) = 0
		\end{align*}
		für gegebenes Datum $d$ und gegebene Funktion $F$.
	\end{definition}
	\noindent
	Typischerweise können in akzeptabler Zeit keine exakten Lösungen gefunden werden, sondern nur Approximationen. Insbesondere stehen statt den vollen Mengen $\bQ$, $\bR$, $\bC$ etc. auch nur endlich viele \tbf{Maschinenzahlen} zur Verfügung - arbiträre reelle Zahlen benötigen unendlich viel Speicher! Rechenoperationen sind dementsprechend Fehlerbehaftet, es gibt Rundungsfehler. Außerdem gibt es in reellen Anwendungen oft \tbf{Modellfehler} und \tbf{Datenfehler}.
	\newpar
	Eine Grundlegende Idee in der Numerik ist es deshalb, eine gute Balance zwischen Exaktheit und Aufwand der Berechnung zu finden.
	\begin{example}
		Die Berechnung der Determinante einer Matrix mittels Laplaceschem Entwicklungssatz benötigt $O(n!)$ Rechenoperationen. Die Determinante mit diesem Verfahren zu berechnen, dauert sehr viel länger, als das Universum alt ist.
		\newpar
		Besser: Matrix (approximativ) auf Dreiecksgestalt bringen und die Diagonalelemente multiplizieren.
	\end{example}
	\begin{definition}
		Eine Mathematische Aufgabe heißt \tbf{wohlgestellt}, wenn zu geeigneten Daten $d$ eindeutige Lösungen $x$ existieren, und diese stetig von $d$ abhängt. Andernfalls ergibt die Suche nach einer numerischen Lösung wenig Sinn. Für wohlgestellte Probleme existiert eine Lösungsfunktion $\phi$, sodass $x = \phi(d)$ das Problem löst, d.h. $f(\phi(d),d) = 0$.
	\end{definition}
	\begin{definition}
		Ein numerischer Algorithmus zur näherungsweisen Lösung einer wohlgestellten Aufgabe $\phi$ ist eine Abbildung $\tilde{\phi}$, die durch Hintereinanderausführung möglicherweise fehlerbehafteter elementarer Rechenoperationen definiert ist, also 
		\begin{align*}
			\tilde{\phi} = f_j \circ f_{j-1} \circ \hdots \circ f_1
		\end{align*}
	\end{definition}
	\begin{definition}
		Der \tbf{Aufwand} eines Verfahrens $\tilde \phi$ ist die Anzahl der benötigten elementaren Rechenschritte. Typischerweise interessiert uns nicht die exakte Anzahl an Schritten, sondern nur die Größenordnung.
	\end{definition}
	\begin{proposition}
		Das Gaußverfahren hat Aufwand $\cO(n^3)$.
	\end{proposition}
\chapter{Numerische Lineare Algebra}
\section{Matrixfaktorisierung}
\subsection{Dreiecksmatrizen}
\begin{definition}
	Eine Matrix $L \in \bR^{n \times n}$ heißt \tbf{untere Dreiecksmatrix}, falls $\forall i < j : l_{ij} = 0$.
\end{definition}
\begin{definition}
	Eine Matrix $U \in \bR^{n \times n}$ heißt \tbf{obere Dreiecksmatrix}, falls $U^\top$ eine untere Dreiecksmatrix ist.
\end{definition}
\begin{definition}
	Eine Dreiecksmatrix heißt \tbf{normalisiert}, falls alle ihre Diagonaleinträge $1$ sind.
\end{definition}
\begin{definition}
	Eine Matrix heißt \tbf{regulär}, wenn sie invertierbar ist.
\end{definition}
\begin{lemma}
	Die quadratischen oberen (bzw. unteren) Dreiecksmatrizen bilden unter Matrixmultiplikation eine Gruppe.
\end{lemma}
\noindent
Lineare Gleichungssysteme mit regulärer Dreiecksmatrix lassen sich leicht lösen. Sei $U \in \bR^{n \times n}$ eine reguläre obere Dreiecksmatrix und $b \in \bR^n$. Wir berechnen $x \in \bR^n$ folgendermaßen:
\begin{enumerate}
	\item for i = n : -1 : 1:
	\begin{enumerate}
		\item $\displaystyle x_i = \left(b_i - \sum_{j = i+1}^n u_{ij}x_j\right) \cdot \frac{1}{u_{ii}}$
	\end{enumerate}
	\item end.
\end{enumerate}
Der Aufwand dieses Verfahrens ist $\cO(n^2)$. Ein analoger Algorithmus existiert für untere Dreiecksmatrizen.
\subsection{$LU$-Zerlegung}
Falls für eine reguläre Matrix $A \in \bR^{n \times n}$ eine Zerlegung $A = LU$ in eine untere Dreiecksmatrix $U$ und eine obere Dreiecksmatrix $L$ gegeben ist, so lässt sich das lineare Gleichungssystem $Ax = b$ in zwei Schritten lösen:
\begin{enumerate}
	\item Löse $Ly = b$.
	\item Löse $Ux = y$.
\end{enumerate}
\begin{definition}
	Eine Faktorisierung $A = LU$ mit unterer Dreiecksmatrix $L \in \bR^{n \times n}$ und oberer Dreiecksmatrix $U \in \bR^{n \times n}$ heißt \tbf{$LU$-Zerlegung} von $A$. Die Zerlegung heißt \tbf{normalisiert}, falls $L$ normalisiert ist.
\end{definition}
\begin{theorem}
	Für jede reguläre Matrix $A \in \bR^{n \times n}$ sind äquivalent:
	\begin{enumerate}
		\item Es existiert eine eindeutige normalisierte $LU$-Zerlegung.
		\item Alle Untermatrizen $A_k = (a_{ij})_{(i,j) \in (1, \hdots, k)^2}$ sind regulär. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	\phantom{}
	\begin{itemize}
		\item[$\rightarrow$] Ist $A$ regulär, so sind auch $L$ und $U$ regulär. Damit sind von $L$ und $U$ alle Diagonaleinträge nicht null. Somit sind auch die Untermatrizen $L_k$ und $U_k$ regulär, somit auch die Untermatrizen $A_k = L_k U_k$.
		\item[$\leftarrow$] Für $n = 1$ ist die Aussage klar. Sei nun angenommen, die Aussage gelte für Matrizen der Größe $(n - 1) \times (n - 1)$. Damit existieren Matrizen $L_{n-1}, U_{n-1}$, sodass $A_{n-1} = L_{n-1}U_{n-1}$ eine normalisierte $LU$-Zerlegung ist. Seien nun $\binom{b}{a_{nn}}$ die letzte Spalte und $(c^\top, a_{nn})$ die letzte Zeile von $A$. Die Aussage ist bewiesen, wenn geeignete $l,u \in \bR^{n-1}$ und $r \in \bR$ existieren, sodass
		\begin{align*}
			\begin{pmatrix}
				A_{n-1} & b\\
				c^\top & a_{nn}
			\end{pmatrix}
			&=
			\begin{pmatrix}
				L_{n-1} & 0\\
				l^\top & 1
			\end{pmatrix}
			\begin{pmatrix}
				U_{n-1} & u\\
				0 & r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				L_{n-1}U_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
				A_{n-1} & L_{n-1}u\\
				(U^\top_{n-1} l)^\top & l^\top u + r
			\end{pmatrix}
		\end{align*}
		Wir brauchen also $L_{n-1}u = b$, $U_{n-1}^\top l = c$, und $a_{nn} = l^\top u + r$. Durch Regularität von $L_{n-1}$ und $U_{n-1}$ existieren eindeutige Lösungen $u, l$, der ersten beiden Gleichungen, somit ist auch $r$ festgelegt.
	\end{itemize}
\end{proof}
\begin{corollary}
	\phantom{}
	\begin{itemize}
		\item Jede positiv definite Matrix besitzt eine eindeutige $LU$-Zerlegung.
		\item Jede strikt diagonaldominante Matrix, also jede Matrix $A$ mit $\sum_{j \in 1, \hdots, n, i \neq j} \abs{a_ij} < \abs{a_{ii}}$ besitzt eine eindeutige $LU$-Zerlegung.
		\item Die Matrix $A = \begin{pmatrix}
			0 & 1\\
			1 & 0
		\end{pmatrix}$
		besitzt keine $LU$-Zerlegung.
		\item Die Nullmatrix besitzt zwar $LU$-Zerlegungen, diese sind aber nicht eindeutig.
	\end{itemize}
\end{corollary}
\begin{lemma}
	Falls $A = LU$ eine normalisierte $LU$-Zerlegung von $A$ ist, so gilt
	\begin{align*}
		a_{ik} = u_{ik} + \sum_{j = 1}^{i-1} l_{ij} u_{jk}
	\end{align*}
	und
	\begin{align*}
		a_{ki} = l_{ki} u_{ii} + \sum_{j = 1}^{i-1} l_{kj} u_{ji}
	\end{align*}
\end{lemma}
\begin{proof}
	Es gilt $l_{ij} = 0$ für $j > i$ und $l_{ii} = 1$. Es gilt außerdem $u_{ij} = 0$ für $j < i$. Die Formeln folgen direkt aus der Definition des Matrixprodukts.
\end{proof}
Diese Formeln lassen sich für $i \leq k$ nach $u_{ik}$ auflössen und für $k > i$ nach $l_{ki}$ auflösen. Wir erhalten folgenden Algorithmus:
\begin{algorithm}
\begin{algorithmic}[1]
	\For{$i = 1, i \leq n, i++$}
		\For{$k = i, k \leq n, k++$}
			\State $\displaystyle u_{ik} \gets a_{ik} - \sum_{j = 1}^{i-1}l_{ij}u_{jk}$
		\EndFor
		\For{$k = i+1, k \leq n, k++$}
		\State $\displaystyle l_{ki} \gets \frac{1}{u_{ii}} \cdot \left(a_{ki} - \sum_{j = 1}^{i-1}l_{kj}u_{ji}\right)$
		\EndFor
	\EndFor
\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Rechenauftrag dieses Algorithmus beträgt $O(n^3)$.
\end{proposition}
\begin{proposition}
	Es ist nicht mehr Speicher nötig, als sowieso für $A$ benötigt wird. Die Einträge von $A$ können im Speicher einfach sukzessiv durch die jeweiligen Einträge von $L$ bzw. $U$ ersetzt werden.
\end{proposition}
\begin{definition}
	Ein numerisches Problem $\phi$ heißt \tbf{schlecht Konditioniert}, wenn kleine Unterschiede in der Eingabe zu großen Unterschieden in der korrekten Lösung führen, also wenn
	\begin{align*}
		\frac{\abs{\phi(\tilde{x}) - \phi(x)}}{\abs{\phi(x)}} >> \frac{\abs{\tilde{x} - x}}{\abs{x}}
	\end{align*}
	Ansonsten heißt die Aufgabe \tbf{gut konditioniert}.
\end{definition}
\begin{definition}
	Ein Verfahren $\tilde{\phi}$ heißt \tbf{numerisch instabil}, wenn eine Störung $\tilde{x}$ existiert, sodass der durch Rundungsfehler verursachte relative Fehler erheblich größer ist als der rein durch die Störung verursachte Fehler.
\end{definition}
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\ 
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Für $\epsilon \in \bR^+$. Es gilt
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			1 + \frac{1}{\epsilon} & -\frac{1}{\epsilon}\\ 
			- \frac{1}{\epsilon} & \frac{1}{\epsilon}
		\end{pmatrix}
	\end{align*}
	\begin{align*}
		A^{-1} \binom{1}{1} = \binom{1}{0}. \qquad \qquad A^{-1} \binom{1}{1 + \epsilon} = \binom{0}{1}.
	\end{align*}
\end{example}
\noindent Eine kleine Störung in den Daten $b$ des linearen Gleichungssystems $Ax = b$ führt zu Problemen der Größenordnung $1$!!! So können wir keine Numerik machen!!! Dieses Problem ist \tbf{schlecht konditioniert}.
\begin{example}
	Sei 
	\begin{align*}
		A = \begin{pmatrix}
			\epsilon & 1\\ 
			1 & 0
		\end{pmatrix},
	\end{align*}
	also
	\begin{align*}
		A^{-1} = \begin{pmatrix}
			0 & 1\\ 
			1 & -\epsilon
		\end{pmatrix}.
	\end{align*}
	So haben wir kein Problem bei der Berechnung von $A^{-1}b$ - die Aufgabe ist gut konditioniert. Sagen wir nun, wir versuchen, das Gleichungssystem effizient durch $LU$-Zerlegung zu lösen. Wir sehen, $LU$-Zerlegung von $A$ ist jedoch gegeben durch
	\begin{align*}
		A = 
		\begin{pmatrix}
			1 & 0\\ 
			\frac{1}{\epsilon} & 1
		\end{pmatrix}
		\begin{pmatrix}
			\epsilon & 1\\ 
			0 & \frac{1}{\epsilon}
		\end{pmatrix},
	\end{align*}
	und die Berechnung von $L^{-1}b$ und  $U^{-1}b$ führt nun wieder zu großen Rundungsfehlern. Aus unserer Idee entsteht also ein \tbf{instabiler Algorithmus.}
\end{example}
\begin{theorem}
	\theoremname{Cholesky-Zerlegung:} Sei $A \in \bR^{n \times n}$ symmetrisch und positiv definit. So existiert eine eindeutige untere Dreiecksmatrix $L$, sodass
	\begin{align*}
		A = LL^\top.
	\end{align*} 
	und $l_{ii} > 0$
\end{theorem}
\begin{proof}
	Für $n = 1$ ist die Suche durch $l_{11} = \sqrt{a_{11}}$ erledigt.
	\newpar
	Die Untermatrix $A_{n-1}$ ist immer ebenfalls positiv definit und symmetrisch. Sei also $A_{n-1} = L_{n-1}L_{n-1}^\top$. Wir setzen $\binom{b}{a_{nn}}^\top$ als die letzte Zeile von $A$. Dann müssen wir zum Beweis des Satzes einen Vektor $c \in \bR^{n-1}$ und ein $\alpha \geq 0$ finden, sodass
	\begin{align*}
		\begin{pmatrix}
			A_{n-1} & b\\
			b^\top & a_{nn}
		\end{pmatrix}
		=
		\begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_{n-1} & L_{n-1}c\\
			(L_{n-1} - c)^\top & \alpha^2 + c^\top c
		\end{pmatrix}
	\end{align*}
	Dies ist nach Annahme äquivalent zu $L_{n-1}c = b$ und $c^\top c + \alpha^2 = a_{nn}$
	\newpar
	Da $L$ regulär ist existiert ein eindeutiges $c$, welches die erste Gleichung erfüllt.
	Es gilt: 
	\begin{align*}
		\det A = \det \begin{pmatrix}
			L_{n-1} & 0\\
			c^\top & \alpha
		\end{pmatrix}
		\cdot
		\det
		\begin{pmatrix}
			L_{n-1}^\top & c\\
			0 & \alpha
		\end{pmatrix}
		=
		\alpha^2(\det L_{n-1})^2
	\end{align*}
	Da $\det A > 0$ und $\det L_{n-1} \geq 0$ bekommen wir $\alpha > 0$, sodass $c^\top c + \alpha^2 = a_{nn}$ ebenfalls eine eindeutige positive Lösung hat.
\end{proof}
\begin{lemma}
	Für $A = LL^\top$ gilt:
	\begin{align*}
		a_{ik} = 
		\begin{cases}
			l_{ik}l_{kk} + \sum_{j = i}^{k-1} l_{ij}l_{ki} & i > k\\
			l_{kk}^2 + \sum_{j = 1}^{k-1} l_{kj}^2 & i = k
		\end{cases} 
	\end{align*}
\end{lemma}
\begin{proof}
	Matrixmultiplikation ohne triviale Summanden.
\end{proof}
\begin{algorithm}
	\tbf{Cholesky-Zerlegung:}
	\begin{algorithmic}[1]
		\For{$k = 1, i \leq n, i++$}
			\State $l_{kk} = \sqrt{a_{kk} - \sum_{j = 1}^{k-1}l_{kj}^2}$
			\For{$i = k+1, i \leq n, i++$}
				\State $l_{ik} = (a_{kk} - \sum_{j = 1}^{k-1} l_{ij}l_{kj}) \frac{1}{l_{kk}}$
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}
\begin{proposition}
	Der Aufwand ist wieder $\cO(n^3)$, allerdings mit kleineren Konstanten.
\end{proposition}
\begin{proposition}
	Lösung von $Ax = b$ für $A = LL^\top$ wie gehabt durch $Ly = b$ und $L^\top x = y$.
\end{proposition}
\subsection{Matrixnormen}
Bekannt sind die üblichen Vektornormen auf $\bR^n$, insbesondere 
\begin{align*}
	\norm{\vv}_p = \left(\sum_{j = 1}^n \abs{v_j}^p\right)^{\frac{1}{p}}
\end{align*}
und
\begin{align*}
	\norm{\vv}_\infty = \max v_j
\end{align*} 
Für $1 \leq p,q \leq \infty$ existiert eine Konstante $c_{pqn}$, sodass
\begin{align*}
	\forall \vv \in \bR^n : \frac{1}{c_{pqn}}\norm{\vv}_p \leq \norm{\vv}_q \leq c_{pqn}\norm{\vv}_p 
\end{align*}
\begin{definition}
	Für Normen $\norm{-}_{\bR^n}$ und $\norm{-}_{\bR^m}$ auf $\bR^n$ und $\bR^m$ definieren wir die Operatornorm auf $\hom(\bR^n, \bR^m) = \bR^{m \times n}$ als
	\begin{align*}
		\norm{A}_{op} = \sup_{x \in \bR^n : \norm{x}_{\bR^n} = 1} \norm{Ax}_{\bR^m}
	\end{align*}
\end{definition}
\begin{lemma}
	Die Operatornorm ist eine Norm.
\end{lemma}
\begin{proof}
	\begin{enumerate}
		\item Skalare können aus der inneren Norm und dem Supremum wie nötig herausgezogen werden.
		\item Das Supremum ist über einer Menge positiver Zahlen, falls $x \neq \vzero$ gibt es mindestens einen Vektor größer $0$.
		\item Dreiecksungleichung folgt aus der Dreiecksungleichung für $\norm{-}_{\bR^m}$.
	\end{enumerate}
\end{proof}
\begin{lemma}
	\begin{align*}
		\norm{A}_{op} = \inf \{c > 0 : \forall x \in \bR^n \norm{Ax} \leq c\norm{x}\}
	\end{align*}
\end{lemma}
\begin{lemma}
	Für $A \neq 0$ und $x \in \bR^n$ mit $\norm{x} \leq 1$ und $\norm{Ax} = \norm{A}_{op}$ folgt $\norm{x} = 1$
\end{lemma}
\begin{corollary}
	Für alle $x \in \bR^n$ gilt $\norm{Ax} \leq \norm{A}_{op}\norm{x}$
\end{corollary}
\begin{lemma}
	Es gibt Vektoren, sodass die Matrixnorm ihr $\inf$ und ihren $\sup$ annimmt.
\end{lemma}
\begin{proof}
	Es handelt sich um eine stetige Funktion auf einem Kompaktum.
\end{proof}
\begin{example}
\begin{enumerate}
	\item Die \tbf{Spaltensummennorm} $\norm{-}_1$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{1} = \max_{j = 1, \hdots, n} \sum_{i = 1}^m \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Zeilensummennorm} $\norm{-}_\infty$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_{\infty} = \max_{i = 1, \hdots, m} \sum_{j = 1}^n \abs{a_{ij}}
	\end{align*}
	\item Die \tbf{Spektralnorm} $\norm{-}_2$ ist eine Operatornorm:
	\begin{align*}
		\norm{A}_2 = \rho(A^\top A) = (\max \{\abs{\lambda} : \lambda \textnormal{ ist Eigenwert von $A^\top A$}\})^\frac{1}{2}
	\end{align*}
\end{enumerate}
\end{example}
\begin{lemma}
	Für $A \in \bR^{l \times m}$, $B \in \bR^{m \times n}$ und eine beliebige Operatornorm $\norm{-}$ gilt $\norm{AB} \leq \norm{A}\norm{B}$
\end{lemma}
\begin{proof}
	\begin{align*}
		&\norm{ABx} \leq \norm{A}\norm{Bx} \leq \norm{A}\norm{B}\norm{x}\\
		\implies &\norm{AB} \leq \norm{A}\norm{B}
	\end{align*}
\end{proof}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt
	\begin{align*}
		\norm{E_n} = 1
	\end{align*}
\end{lemma}
\begin{lemma}
	Falls die Normen in Bild und Urbild gleich sind, gilt für $A$ symmetrisch mit Eigenwert $\lambda$
	\begin{align*}
		\norm{A} \geq \abs{\lambda}
	\end{align*}
\end{lemma}
\begin{example}
	Die Frobeniusnorm $\norm{-}_\cF$ einer Matrix $A \in \bR^{m \times n}$ ist gegeben durch
	\begin{align*}
		\norm{A}_\cF = \left(\sum_{j = 1}^m \sum_{i = 1}^n a_{ij}^2\right)^\frac{1}{2}
	\end{align*}
\end{example}
\begin{lemma}
	Für $n > 1$ ist die Frobeniusnorm keine Operatornorm!
\end{lemma}
\begin{proof}
	\begin{align*}
			\norm{E_n} = \sqrt{n}
	\end{align*}
	Normieren wir die Norm, gilt
	\begin{align*}
		\frac{1}{\sqrt{2}}\norm{\begin{pmatrix}
				1 & 0\\
				0 & 0
		\end{pmatrix}}_{\cF}
		=
		\frac{1}{\sqrt{2}}
		< 
		1
	\end{align*}
	Wobei $1$ ein Eigenwert ist, was unseren vorherigen Lemmata widerspricht.
\end{proof}
\subsection{Konditionszahl}
\begin{theorem}
	Sei $\norm{-}$ eine Operatornorm auf $\bRnn$. Sei $A \in \bRnn$ regulär und seien $x, x', b, b' \in \bR^n$, sodass $Ax = b$, $Ax' = b'$. Dann gilt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\frac{\norm{b - b'}}{\norm{b}}
	\end{align*}
\end{theorem}
\begin{theorem}
	\begin{align*}
		\norm{x - x'} = \norm{A^{-1}(b - b')} \leq \norm{A^{-1}}\norm{b - b'}
	\end{align*}
	und
	\begin{align*}
		\norm{b} = \norm{Ax} \leq \norm{A}\norm{x}
	\end{align*}
	Es folgt:
	\begin{align*}
		\frac{\norm{x - x'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{x}} \leq \frac{\norm{A^{-1}}\norm{b - b'}}{\norm{A^{-1}}\norm{b}}
	\end{align*}
\end{theorem}
\begin{definition}
	Die \tbf{Konditionszahl} einer regulären Matrix $A \in \bRnn$ bezüglich der durch $\norm{-}$ auf $\bR^n$ induzierten Operatornorm ist gegeben durch:
	\begin{align*}
		\textnormal{cond}_{\norm{-}}(A) = \norm{A}\norm{A^{-1}}
	\end{align*}
\end{definition}
Wir schreiben oft $\textnormal{cond}_p$ statt $\textnormal{cond}_{\norm{-}_p}$.
\begin{lemma}
	$\textnormal{cond}(A) \geq 1$
\end{lemma}
\begin{lemma}
	Für $A$ symmetrisch mit Eigenwerten $\lambda_i$ gilt 
	\begin{align*}
		\cond_2(A) = \frac{\max \abs{\lambda_j}}{\min \abs{\lambda_j}}
	\end{align*}
\end{lemma}
\begin{example}
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1\\
			1 & 1 + \epsilon
		\end{pmatrix}
	\end{align*}
	Besitzt die Eigenwerte
	\begin{align*}
		\lambda_{1,2} = 1 + \frac{\epsilon}{2} \pm \left(1 + \frac{\epsilon^2}{4}\right)^\frac{1}{2}
	\end{align*}
	Also $\lambda_1 \approx 2 + \frac{\epsilon}{2}$ und $\lambda_2 \approx \frac{\epsilon}{2}$. Für $\epsilon \to 0$ geht also die Konditionszahl gegen unendlich.
\end{example}
\begin{theorem}
	Für $A$ symmetrisch und positiv definit mit Cholesky-Zerlegung $A = L L^\top$ gilt
	\begin{align*}
		\cond_2(L) = \cond_2(L^\top) = \sqrt{\cond(A)}
	\end{align*}
\end{theorem}
 \noindent Also kann das Problem, welches bei der $LU$-Zerlegung auftrat, bei der Cholesky-Zerlegung nicht vorkommen.
\end{document}